{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon Aurora Labs for PostgreSQL \u00b6 Welcome to the AWS workshop and lab content portal for Amazon Aurora PostgreSQL compatible databases! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the Amazon Aurora features and capabilities. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. These resources are focused on helping you discover how advanced features of the Amazon Aurora PostgreSQL database operate. Prior expertise with AWS and PostgreSQL-based databases is beneficial, but not required to complete the labs.","title":"Home"},{"location":"#amazon-aurora-labs-for-postgresql","text":"Welcome to the AWS workshop and lab content portal for Amazon Aurora PostgreSQL compatible databases! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the Amazon Aurora features and capabilities. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. These resources are focused on helping you discover how advanced features of the Amazon Aurora PostgreSQL database operate. Prior expertise with AWS and PostgreSQL-based databases is beneficial, but not required to complete the labs.","title":"Amazon Aurora Labs for PostgreSQL"},{"location":"contribute/","text":"Contributing Guidelines \u00b6 Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests \u00b6 We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests \u00b6 Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on \u00b6 Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct \u00b6 This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications \u00b6 If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing \u00b6 See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Contributing"},{"location":"contribute/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"contribute/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"contribute/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"contribute/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"contribute/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"contribute/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"contribute/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Licensing"},{"location":"license/","text":"License \u00b6 MIT License Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"modules/","text":"Getting Started \u00b6 Create an AWS account \u00b6 In order to complete the hands-on content on this site, you'll need an AWS Account. We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. If you are setting up an AWS account for the first time, follow the instructions below to create an administrative IAM user account , we recommend not using your AWS account root credentials for day to day usage. If you have received credits to complete these labs follow the instructions below on adding the credits to your AWS account. Overview of labs \u00b6 The following labs are currently available, part of this instructional website: # Lab Module Recommendation Overview 1 Prerequisites Required, start here Set up the lab environment and provision the prerequisite resources 2 Create a New Aurora Cluster Optional Create a new Amazon Aurora MySQL DB cluster manually 3 Connecting, Loading Data and Auto Scaling Recommended, for provisioned clusters Connect to the DB cluster for the first time, load an initial data set and test read replica auto scaling. The initial data set may be used in subsequent labs. 4 Cloning Clusters Recommended, for provisioned clusters Cloning an Aurora DB cluster and observing the divergence of the data set. 5 Backtracking a Cluster Recommended, for provisioned clusters Backtracking an Aurora DB cluster to fix an accidental DDL operation. 6 Using Performance Insights Recommended, for provisioned clusters Examining the performance of your DB instances using RDS Performance Insights 7 Creating a Serverless Aurora Cluster Recommended, for serverless clusters Create a new Amazon Aurora Serverless MySQL DB cluster manually. You may skip the provisioned cluster labs if you are planning to operate a serverless workload. 8 Using Aurora Serverless with Lambda Functions Recommended, for serverless clusters Connect to your serverless cluster using the RDS Data API and Lambda functions. Requires the previous lab. You can also discover exercises, labs and workshops related to Amazon Aurora on the Related Labs and Workshops page. Lab environment at a glance \u00b6 To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. The environment deployed using CloudFormation includes several components: Amazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the cluster and workstation Amazon EC2 instance configured with the software components needed for the lab IAM roles with access permissions for the workstation and cluster permissions for enhanced monitoring , S3 access and logging Custom cluster and DB instance parameter groups for the Amazon Aurora cluster, enabling logging and performance schema Optionally, Amazon Aurora DB cluster with 2 nodes: a writer and read replica If the cluster is created for you, the master database credentials will be generated automatically and stored in an AWS Secrets Manager secret. Optionally, read replica auto scaling configuration Optionally, AWS Systems Manager command document to execute a load test Create an IAM user (with admin permissions) \u00b6 If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess policy, and click Next:Review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created. Add credits (optional) \u00b6 If you are doing these workshop as part of an AWS sponsored event that doesn't provide AWS accounts, you will receive credits to cover the costs. Below are the instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem. Additional software needed for labs \u00b6 The templates and scripts setting up the lab environment install the following software in the lab environment for the purposes of deploying and running the labs: mysql-client package. MySQL open source software is provided under the GPL License. sysbench available using the GPL License. test_db available using the Creative Commons Attribution-Share Alike 3.0 Unported License. Percona's sysbench-tpcc available using the Apache License 2.0.","title":"Getting Started"},{"location":"modules/#getting-started","text":"","title":"Getting Started"},{"location":"modules/#create-an-aws-account","text":"In order to complete the hands-on content on this site, you'll need an AWS Account. We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. If you are setting up an AWS account for the first time, follow the instructions below to create an administrative IAM user account , we recommend not using your AWS account root credentials for day to day usage. If you have received credits to complete these labs follow the instructions below on adding the credits to your AWS account.","title":"Create an AWS account"},{"location":"modules/#overview-of-labs","text":"The following labs are currently available, part of this instructional website: # Lab Module Recommendation Overview 1 Prerequisites Required, start here Set up the lab environment and provision the prerequisite resources 2 Create a New Aurora Cluster Optional Create a new Amazon Aurora MySQL DB cluster manually 3 Connecting, Loading Data and Auto Scaling Recommended, for provisioned clusters Connect to the DB cluster for the first time, load an initial data set and test read replica auto scaling. The initial data set may be used in subsequent labs. 4 Cloning Clusters Recommended, for provisioned clusters Cloning an Aurora DB cluster and observing the divergence of the data set. 5 Backtracking a Cluster Recommended, for provisioned clusters Backtracking an Aurora DB cluster to fix an accidental DDL operation. 6 Using Performance Insights Recommended, for provisioned clusters Examining the performance of your DB instances using RDS Performance Insights 7 Creating a Serverless Aurora Cluster Recommended, for serverless clusters Create a new Amazon Aurora Serverless MySQL DB cluster manually. You may skip the provisioned cluster labs if you are planning to operate a serverless workload. 8 Using Aurora Serverless with Lambda Functions Recommended, for serverless clusters Connect to your serverless cluster using the RDS Data API and Lambda functions. Requires the previous lab. You can also discover exercises, labs and workshops related to Amazon Aurora on the Related Labs and Workshops page.","title":"Overview of labs"},{"location":"modules/#lab-environment-at-a-glance","text":"To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. The environment deployed using CloudFormation includes several components: Amazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the cluster and workstation Amazon EC2 instance configured with the software components needed for the lab IAM roles with access permissions for the workstation and cluster permissions for enhanced monitoring , S3 access and logging Custom cluster and DB instance parameter groups for the Amazon Aurora cluster, enabling logging and performance schema Optionally, Amazon Aurora DB cluster with 2 nodes: a writer and read replica If the cluster is created for you, the master database credentials will be generated automatically and stored in an AWS Secrets Manager secret. Optionally, read replica auto scaling configuration Optionally, AWS Systems Manager command document to execute a load test","title":"Lab environment at a glance"},{"location":"modules/#create-an-iam-user-with-admin-permissions","text":"If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess policy, and click Next:Review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.","title":"Create an IAM user (with admin permissions)"},{"location":"modules/#add-credits-optional","text":"If you are doing these workshop as part of an AWS sponsored event that doesn't provide AWS accounts, you will receive credits to cover the costs. Below are the instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem.","title":"Add credits (optional)"},{"location":"modules/#additional-software-needed-for-labs","text":"The templates and scripts setting up the lab environment install the following software in the lab environment for the purposes of deploying and running the labs: mysql-client package. MySQL open source software is provided under the GPL License. sysbench available using the GPL License. test_db available using the Creative Commons Attribution-Share Alike 3.0 Unported License. Percona's sysbench-tpcc available using the Apache License 2.0.","title":"Additional software needed for labs"},{"location":"modules/create/","text":"Creating a New Aurora Cluster \u00b6 Note If you are familiar with the basic concepts of Amazon Aurora MySQL, and have created a cluster in the past, you may skip this module, by using provisioning the lab environment using the lab-with-cluster.yml CloudFormation template, so the DB cluster is provisioned for you. Skip to Connecting, Loading Data and Auto Scaling . This lab will walk you through the steps of creating an Amazon Aurora database cluster manually, and configuring app the parameters required for the cluster components. At the end of this lab you will have a database cluster ready to be used in subsequent labs. This lab contains the following tasks: Creating the DB cluster Retrieving the DB cluster endpoints Assigning an IAM role to the DB cluster Creating a replica auto scaling policy This lab requires the following lab modules to be completed first: Prerequisites (using lab-no-cluster.yml template is sufficient) 1. Creating the DB cluster \u00b6 Open the Amazon RDS service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create database to start the configuration process Note The RDS console database creation workflow has been simplified recently. Depending on your previous usage of the RDS console UI, you may see the old workflow or the new one, you may also be presented with a prompt to toggle between them. In this lab we are using the new workflow for reference, although the steps will work similarly in the old console workflow as well, if you are more familiar with it. In the first configuration section of the Create database page, called Database settings ensure the Easy create toggle button is turned OFF (grey color). Next, in the Engine options section, choose the Amazon Aurora engine type, the Amazon Aurora with MySQL compatibility edition, the Aurora (MySQL 5.6) 1.19.2 version and the Regional database location. In the Database features section, select One writer and multiple readers , and in the Templates section, select Production . The selections so far will instruct AWS to create an Aurora MySQL database cluster with the most recent version of the MySQL 5.6 compatible engine in a highly available configuration with one writer and one reader database instance in the cluster. In the Settings section give your database cluster a recognizable identifier, such as labstack-cluster . Configure the name and password of the master database user, with the most elevated permissions in the database. We recommend to use the user name masteruser for consistency with subsequent labs and a password of your choosing. For simplicity ensure the check box Auto generate a password is not checked . In the Connectivity section, expand the sub-section called Additional connectivity configuration . This section allows you to specify where the database cluster will be deployed within your defined network configuration. To simplify the labs, the CloudFormation stack you deployed in the preceding Prerequisites module, has configured a VPC that includes all resources needed for an Aurora database cluster. This includes the VPC itself, subnets, DB subnet groups, security groups and several other networking constructs. All you need to do is select the appropriate existing connectivity controls in this section. Pick the Virtual Private Cloud (VPC) named after the CloudFormation stack name, such as labstack-vpc . Similarly make sure the selected Subnet Group also matches the stack name (e.g. labstack-dbsubnets-[hash] ). Make sure the cluster Publicly accessible option is set to No . The lab environment also configured a VPC security group that allows your lab workspace EC2 instance to connect to the database. Make sure the Choose existing security group option is selected and from the dropdown pick the security group with a name ending in -mysql-internal (eg. labstack-mysql-internal ). Please remove any other security groups, such as default from the selection. Next, expand the Advanced configuration section. Set the Initial database name to mylab . For the DB cluster parameter group and DB parameter group selectors, choose the groups with the stack name in their name (e.g. labstack-[...] ). Choose a 7 days Backup retention period . Check the box to Enable encryption and select the [default] aws/rds for the Master key . Enable the backtrack capability by checking the Enable Backtrack box and set a Target backtrack window of 24 hours. Check the box to Enable Performance Insights with a Retention period of Default (7 days) and use the [default] aws/rds Master key for monitoring data encryption. Next, check the Enable Enhanced Monitoring box, and select a Granularity of 1 second . For Log exports check the Error log and Slow query log** boxes. Also in the Advanced configuration section, de-select the check box Enable delete protection . In a production use case, you will want to leave that option checked, but for testing purposes, un-checking this option will make it easier to clean up the resources once you have completed the labs. Before continuing, let's summarize the configuration options selected. You will create a database cluster with the following characteristics: Aurora MySQL 5.6 compatible (latest stable engine version) Regional cluster composed of a writer and a reader DB instance in different availability zones (highly available) Deployed in the VPC and using the network configuration of the lab environment Using custom database engine parameters that enable the slow query log, S3 access and tune a few other configurations Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption Retaining 24 hours worth of change data for backtrack purposes With Enhanced Monitoring and Performance Insights enabled Click Create database to provision the DB cluster. 2. Retrieving the DB cluster endpoints \u00b6 The database cluster may take several minutes to provision, including the DB instances making up the cluster. In order to connect to the DB cluster and start using it in subsequent labs, you need to retrieve the DB cluster endpoints. There are two endpoints created by default. The Cluster Endpoint will always point to the writer DB instance of the cluster, and should be used for both writes and reads. The Reader Endpoint will always resolve to one of the reader DB instances and should be used to offload read operations to read replicas. In the RDS console, go to the DB cluster detail view by clicking on the cluster DB identifier. The Endpoints section in the Connectivity and security tab of the details page displays the endpoints. Note these values down, as you will use them later. 3. Assigning an IAM role to the DB cluster \u00b6 Once created, you should assign an IAM role to the DB cluster, in order to allow the cluster access to Amazon S3 for importing and exporting data. The IAM role has already been created using CloudFormation when you created the lab environment. On the same DB cluster detail page as before, in the Manage IAM roles section, choose the IAM role named after the stack name, ending in -integrate-[region] (e.g. labstack-integrate-[region] ). Then click Add role . Once the operation completes the Status of the role will change from Pending to Active . 4. Creating a replica auto scaling policy \u00b6 Finally, you will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load. In the top right corner of the details page, click on Actions and then on Add replica auto scaling . Provide a Policy name based on the stack name, such as labstack-autoscale-readers . For the Target metric choose Average CPU utilization of Aurora Replicas . Enter a Target value of 20 percent. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes. Next, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs. In the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2 . In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost of associated with the labs we limit the number of readers to two. Next click Add policy .","title":"Creating a New Aurora Cluster"},{"location":"modules/create/#creating-a-new-aurora-cluster","text":"Note If you are familiar with the basic concepts of Amazon Aurora MySQL, and have created a cluster in the past, you may skip this module, by using provisioning the lab environment using the lab-with-cluster.yml CloudFormation template, so the DB cluster is provisioned for you. Skip to Connecting, Loading Data and Auto Scaling . This lab will walk you through the steps of creating an Amazon Aurora database cluster manually, and configuring app the parameters required for the cluster components. At the end of this lab you will have a database cluster ready to be used in subsequent labs. This lab contains the following tasks: Creating the DB cluster Retrieving the DB cluster endpoints Assigning an IAM role to the DB cluster Creating a replica auto scaling policy This lab requires the following lab modules to be completed first: Prerequisites (using lab-no-cluster.yml template is sufficient)","title":"Creating a New Aurora Cluster"},{"location":"modules/create/#1-creating-the-db-cluster","text":"Open the Amazon RDS service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create database to start the configuration process Note The RDS console database creation workflow has been simplified recently. Depending on your previous usage of the RDS console UI, you may see the old workflow or the new one, you may also be presented with a prompt to toggle between them. In this lab we are using the new workflow for reference, although the steps will work similarly in the old console workflow as well, if you are more familiar with it. In the first configuration section of the Create database page, called Database settings ensure the Easy create toggle button is turned OFF (grey color). Next, in the Engine options section, choose the Amazon Aurora engine type, the Amazon Aurora with MySQL compatibility edition, the Aurora (MySQL 5.6) 1.19.2 version and the Regional database location. In the Database features section, select One writer and multiple readers , and in the Templates section, select Production . The selections so far will instruct AWS to create an Aurora MySQL database cluster with the most recent version of the MySQL 5.6 compatible engine in a highly available configuration with one writer and one reader database instance in the cluster. In the Settings section give your database cluster a recognizable identifier, such as labstack-cluster . Configure the name and password of the master database user, with the most elevated permissions in the database. We recommend to use the user name masteruser for consistency with subsequent labs and a password of your choosing. For simplicity ensure the check box Auto generate a password is not checked . In the Connectivity section, expand the sub-section called Additional connectivity configuration . This section allows you to specify where the database cluster will be deployed within your defined network configuration. To simplify the labs, the CloudFormation stack you deployed in the preceding Prerequisites module, has configured a VPC that includes all resources needed for an Aurora database cluster. This includes the VPC itself, subnets, DB subnet groups, security groups and several other networking constructs. All you need to do is select the appropriate existing connectivity controls in this section. Pick the Virtual Private Cloud (VPC) named after the CloudFormation stack name, such as labstack-vpc . Similarly make sure the selected Subnet Group also matches the stack name (e.g. labstack-dbsubnets-[hash] ). Make sure the cluster Publicly accessible option is set to No . The lab environment also configured a VPC security group that allows your lab workspace EC2 instance to connect to the database. Make sure the Choose existing security group option is selected and from the dropdown pick the security group with a name ending in -mysql-internal (eg. labstack-mysql-internal ). Please remove any other security groups, such as default from the selection. Next, expand the Advanced configuration section. Set the Initial database name to mylab . For the DB cluster parameter group and DB parameter group selectors, choose the groups with the stack name in their name (e.g. labstack-[...] ). Choose a 7 days Backup retention period . Check the box to Enable encryption and select the [default] aws/rds for the Master key . Enable the backtrack capability by checking the Enable Backtrack box and set a Target backtrack window of 24 hours. Check the box to Enable Performance Insights with a Retention period of Default (7 days) and use the [default] aws/rds Master key for monitoring data encryption. Next, check the Enable Enhanced Monitoring box, and select a Granularity of 1 second . For Log exports check the Error log and Slow query log** boxes. Also in the Advanced configuration section, de-select the check box Enable delete protection . In a production use case, you will want to leave that option checked, but for testing purposes, un-checking this option will make it easier to clean up the resources once you have completed the labs. Before continuing, let's summarize the configuration options selected. You will create a database cluster with the following characteristics: Aurora MySQL 5.6 compatible (latest stable engine version) Regional cluster composed of a writer and a reader DB instance in different availability zones (highly available) Deployed in the VPC and using the network configuration of the lab environment Using custom database engine parameters that enable the slow query log, S3 access and tune a few other configurations Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption Retaining 24 hours worth of change data for backtrack purposes With Enhanced Monitoring and Performance Insights enabled Click Create database to provision the DB cluster.","title":"1. Creating the DB cluster"},{"location":"modules/create/#2-retrieving-the-db-cluster-endpoints","text":"The database cluster may take several minutes to provision, including the DB instances making up the cluster. In order to connect to the DB cluster and start using it in subsequent labs, you need to retrieve the DB cluster endpoints. There are two endpoints created by default. The Cluster Endpoint will always point to the writer DB instance of the cluster, and should be used for both writes and reads. The Reader Endpoint will always resolve to one of the reader DB instances and should be used to offload read operations to read replicas. In the RDS console, go to the DB cluster detail view by clicking on the cluster DB identifier. The Endpoints section in the Connectivity and security tab of the details page displays the endpoints. Note these values down, as you will use them later.","title":"2. Retrieving the DB cluster endpoints"},{"location":"modules/create/#3-assigning-an-iam-role-to-the-db-cluster","text":"Once created, you should assign an IAM role to the DB cluster, in order to allow the cluster access to Amazon S3 for importing and exporting data. The IAM role has already been created using CloudFormation when you created the lab environment. On the same DB cluster detail page as before, in the Manage IAM roles section, choose the IAM role named after the stack name, ending in -integrate-[region] (e.g. labstack-integrate-[region] ). Then click Add role . Once the operation completes the Status of the role will change from Pending to Active .","title":"3. Assigning an IAM role to the DB cluster"},{"location":"modules/create/#4-creating-a-replica-auto-scaling-policy","text":"Finally, you will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load. In the top right corner of the details page, click on Actions and then on Add replica auto scaling . Provide a Policy name based on the stack name, such as labstack-autoscale-readers . For the Target metric choose Average CPU utilization of Aurora Replicas . Enter a Target value of 20 percent. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes. Next, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs. In the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2 . In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost of associated with the labs we limit the number of readers to two. Next click Add policy .","title":"4. Creating a replica auto scaling policy"},{"location":"modules/perf-insights/","text":"Using Performance Insights \u00b6 This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance. This lab contains the following tasks: Generating load on your DB cluster Understanding the Performance Insights interface Examining the performance of your DB instance This lab requires the following lab modules to be completed first: Prerequisites Creating a New Aurora Cluster (conditional, if creating a cluster manually) Connecting, Loading Data and Auto Scaling (connectivity section only) 1. Generating load on your DB cluster \u00b6 You will use Percona's TPCC-like benchmark script based on sysbench to generate load. For simplicity we have packaged the correct set of commands in an AWS Systems Manager Command Document . You will use AWS Systems Manager Run Command to execute the test. On the Session Manager workstation command line see the Connecting, Loading Data and Auto Scaling lab , enter one of the following commands. If you have completed the Creating a New Aurora Cluster lab, and created the Aurora DB cluster manually execute this command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] \\ --parameters \\ clusterEndpoint=[clusterEndpoint],\\ dbUser=$DBUSER,\\ dbPassword=\"$DBPASS\" If AWS CloudFormation has provisioned the DB cluster on your behalf, and you skipped the Creating a New Aurora Cluster lab, you can run this simplified command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] Command parameter values at a glance: Parameter Parameter Placeholder Value DB cluster provisioned by CloudFormation Value DB cluster configured manually Description --document-name [loadTestRunDoc] See CloudFormation stack output See CloudFormation stack output The name of the command document to run on your behalf. --instance-ids [bastionInstance] See CloudFormation stack output See CloudFormation stack output The EC2 instance to execute this command on. --parameters clusterEndpoint=[clusterEndpoint],dbUser=$DBUSER,dbPassword=\"$DBPASS\" N/A Substitute the DB cluster endpoint with the values configured manually Additional command parameters. The command will be sent to the workstation EC2 instance which will prepare the test data set and run the load test. It may take up to a minute for CloudWatch to reflect the additional load in the metrics. You will see a confirmation that the command has been initiated. 2. Understanding the Performance Insights interface \u00b6 While the command is running, open the Amazon RDS service console in a new tab, if not already open. Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. In the menu on the left hand side, click on the Performance Insights menu option. Next, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. As the different Db instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab we are generating load on the Writer (master) DB instance only. Select the DB instance where the name either ends in -node-01 or -instance-1 Once a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into 3 sections, allowing you to drill down from high level performance indicator metrics down to individual queries, waits, users and hosts generating the load. The performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the buttons across the top right of the interface ( 5m , 1h , 5h , 24h , 1w , all ). You can also zoom into a specific period of time by dragging across the graphs. Note All dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom. Section Filters Description Counter Metrics Click cog icon in top right corner to select additional counters This section plots internal database counter metrics over time, such as number of rows read or written, buffer pool hit ratio, etc. These counters are useful to correlate with other metrics, including the database load metrics, to identify causes of abnormal behavior. Database load Load can be sliced by waits (default), SQL commands, users and hosts This metric is design to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits , SQL (default), Users and Hosts Drill down capability that allows you to get detailed performance data down to the individual commands. 3. Examining the performance of your DB instance \u00b6 After running the load generator workload above, you will see a performance profile similar to the example below in the Performance Insights dashboard. The load generator command will first create an initial data set using sysbench prepare . And then will run an OLTP workload for the duration of 5 minutes, consisting of concurrent transactional reads and writes using 4 parallel threads. Amazon Aurora MySQL specific wait events are documented in the Amazon Aurora MySQL Reference guide . Use the Performance Insights dashboard and the reference guide documentation to evaluate the workload profile of your load test, and answer the following questions: Is the database server overloaded at any point during the load test? Can you identify any resource bottlenecks during the load test? If so how can they be mitigated? What are the most common wait events during the load test? Why are the load patterns different between the first and second phase of the load test?","title":"PostgreSQL Performance Benchmarking"},{"location":"modules/perf-insights/#using-performance-insights","text":"This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance. This lab contains the following tasks: Generating load on your DB cluster Understanding the Performance Insights interface Examining the performance of your DB instance This lab requires the following lab modules to be completed first: Prerequisites Creating a New Aurora Cluster (conditional, if creating a cluster manually) Connecting, Loading Data and Auto Scaling (connectivity section only)","title":"Using Performance Insights"},{"location":"modules/perf-insights/#1-generating-load-on-your-db-cluster","text":"You will use Percona's TPCC-like benchmark script based on sysbench to generate load. For simplicity we have packaged the correct set of commands in an AWS Systems Manager Command Document . You will use AWS Systems Manager Run Command to execute the test. On the Session Manager workstation command line see the Connecting, Loading Data and Auto Scaling lab , enter one of the following commands. If you have completed the Creating a New Aurora Cluster lab, and created the Aurora DB cluster manually execute this command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] \\ --parameters \\ clusterEndpoint=[clusterEndpoint],\\ dbUser=$DBUSER,\\ dbPassword=\"$DBPASS\" If AWS CloudFormation has provisioned the DB cluster on your behalf, and you skipped the Creating a New Aurora Cluster lab, you can run this simplified command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] Command parameter values at a glance: Parameter Parameter Placeholder Value DB cluster provisioned by CloudFormation Value DB cluster configured manually Description --document-name [loadTestRunDoc] See CloudFormation stack output See CloudFormation stack output The name of the command document to run on your behalf. --instance-ids [bastionInstance] See CloudFormation stack output See CloudFormation stack output The EC2 instance to execute this command on. --parameters clusterEndpoint=[clusterEndpoint],dbUser=$DBUSER,dbPassword=\"$DBPASS\" N/A Substitute the DB cluster endpoint with the values configured manually Additional command parameters. The command will be sent to the workstation EC2 instance which will prepare the test data set and run the load test. It may take up to a minute for CloudWatch to reflect the additional load in the metrics. You will see a confirmation that the command has been initiated.","title":"1. Generating load on your DB cluster"},{"location":"modules/perf-insights/#2-understanding-the-performance-insights-interface","text":"While the command is running, open the Amazon RDS service console in a new tab, if not already open. Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. In the menu on the left hand side, click on the Performance Insights menu option. Next, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. As the different Db instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab we are generating load on the Writer (master) DB instance only. Select the DB instance where the name either ends in -node-01 or -instance-1 Once a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into 3 sections, allowing you to drill down from high level performance indicator metrics down to individual queries, waits, users and hosts generating the load. The performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the buttons across the top right of the interface ( 5m , 1h , 5h , 24h , 1w , all ). You can also zoom into a specific period of time by dragging across the graphs. Note All dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom. Section Filters Description Counter Metrics Click cog icon in top right corner to select additional counters This section plots internal database counter metrics over time, such as number of rows read or written, buffer pool hit ratio, etc. These counters are useful to correlate with other metrics, including the database load metrics, to identify causes of abnormal behavior. Database load Load can be sliced by waits (default), SQL commands, users and hosts This metric is design to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits , SQL (default), Users and Hosts Drill down capability that allows you to get detailed performance data down to the individual commands.","title":"2. Understanding the Performance Insights interface"},{"location":"modules/perf-insights/#3-examining-the-performance-of-your-db-instance","text":"After running the load generator workload above, you will see a performance profile similar to the example below in the Performance Insights dashboard. The load generator command will first create an initial data set using sysbench prepare . And then will run an OLTP workload for the duration of 5 minutes, consisting of concurrent transactional reads and writes using 4 parallel threads. Amazon Aurora MySQL specific wait events are documented in the Amazon Aurora MySQL Reference guide . Use the Performance Insights dashboard and the reference guide documentation to evaluate the workload profile of your load test, and answer the following questions: Is the database server overloaded at any point during the load test? Can you identify any resource bottlenecks during the load test? If so how can they be mitigated? What are the most common wait events during the load test? Why are the load patterns different between the first and second phase of the load test?","title":"3. Examining the performance of your DB instance"},{"location":"modules/prerequisites/","text":"Prerequisites \u00b6 The following steps should be completed before getting started with any of the labs in this repository. Not all steps may apply to all students or environments. This lab contains the following tasks: Signing in to the AWS Management Console Creating a lab environment using AWS CloudFormation 1. Signing in to the AWS Management Console \u00b6 If you are running these labs in a formal, instructional setting, please use the Console URL, and credentials provided to you to access and log into the AWS Management Console. Otherwise, please use your own credentials. You can access the console at: https://console.aws.amazon.com/ or through the Single Sign-On (SSO) mechanism provided by your organization. If you are running these labs in a formal, instructional setting, please use the AWS region provided. Ensure the correct AWS region is selected in the top right corner, if not use that dropdown to choose the correct region. The labs are designed to work in any of the regions where Amazon Aurora MySQL compatible is available. However, not all features and capabilities of Amazon Aurora may be available in all supported regions at this time. 2. Creating a lab environment using AWS CloudFormation \u00b6 To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. Formal Event If you are running these labs in a formal instructional event, the lab environment may have been initialized on your behalf. If unsure, please verify with the event support staff. Please download the most appropriate CloudFormation template based on the labs you want to run: Option Download Template One-Click Launch Description I will create the DB cluster manually lab-no-cluster.yml Use when you wish to provision the initial cluster manually by following Lab 1. - Creating a New Aurora Cluster Provision the DB cluster for me lab-with-cluster.yml Use when you wish to skip the initial cluster creation lab, and have the DB cluster provisioned for you, so you can continue from Lab 2. - Cluster Endpoints and Read Replica Auto Scaling If you downloaded the template, save it in a memorable location such as your desktop, you will need to reference it later. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack . For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options page, leave the defaults as they are, scroll to the bottom and click Next . On the Review labstack page, scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create . The stack will take approximatively 20 minutes to provision, you can monitor the status on the Stack detail page. You can monitor the progress of the stack creation process by refreshing the Events tab. The latest event in the list will indicate CREATE_COMPLETE for the stack resource. Once the status of the stack is CREATE_COMPLETE , click on the Outputs tab. The values here will be critical to the completion of the remainder of the lab. Please take a moment to save these values somewhere that you will have easy access to them during the remainder of the lab. The names that appear in the Key column are referenced directly in the instructions in subsequent steps, using the parameter format: [outputKey]","title":"Prerequisites"},{"location":"modules/prerequisites/#prerequisites","text":"The following steps should be completed before getting started with any of the labs in this repository. Not all steps may apply to all students or environments. This lab contains the following tasks: Signing in to the AWS Management Console Creating a lab environment using AWS CloudFormation","title":"Prerequisites"},{"location":"modules/prerequisites/#1-signing-in-to-the-aws-management-console","text":"If you are running these labs in a formal, instructional setting, please use the Console URL, and credentials provided to you to access and log into the AWS Management Console. Otherwise, please use your own credentials. You can access the console at: https://console.aws.amazon.com/ or through the Single Sign-On (SSO) mechanism provided by your organization. If you are running these labs in a formal, instructional setting, please use the AWS region provided. Ensure the correct AWS region is selected in the top right corner, if not use that dropdown to choose the correct region. The labs are designed to work in any of the regions where Amazon Aurora MySQL compatible is available. However, not all features and capabilities of Amazon Aurora may be available in all supported regions at this time.","title":"1. Signing in to the AWS Management Console"},{"location":"modules/prerequisites/#2-creating-a-lab-environment-using-aws-cloudformation","text":"To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. Formal Event If you are running these labs in a formal instructional event, the lab environment may have been initialized on your behalf. If unsure, please verify with the event support staff. Please download the most appropriate CloudFormation template based on the labs you want to run: Option Download Template One-Click Launch Description I will create the DB cluster manually lab-no-cluster.yml Use when you wish to provision the initial cluster manually by following Lab 1. - Creating a New Aurora Cluster Provision the DB cluster for me lab-with-cluster.yml Use when you wish to skip the initial cluster creation lab, and have the DB cluster provisioned for you, so you can continue from Lab 2. - Cluster Endpoints and Read Replica Auto Scaling If you downloaded the template, save it in a memorable location such as your desktop, you will need to reference it later. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack . For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options page, leave the defaults as they are, scroll to the bottom and click Next . On the Review labstack page, scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create . The stack will take approximatively 20 minutes to provision, you can monitor the status on the Stack detail page. You can monitor the progress of the stack creation process by refreshing the Events tab. The latest event in the list will indicate CREATE_COMPLETE for the stack resource. Once the status of the stack is CREATE_COMPLETE , click on the Outputs tab. The values here will be critical to the completion of the remainder of the lab. Please take a moment to save these values somewhere that you will have easy access to them during the remainder of the lab. The names that appear in the Key column are referenced directly in the instructions in subsequent steps, using the parameter format: [outputKey]","title":"2. Creating a lab environment using AWS CloudFormation"}]}