{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon Aurora Labs for PostgreSQL \u00b6 Welcome to the AWS workshop and lab content portal for Amazon Aurora PostgreSQL compatible databases! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the Amazon Aurora features and capabilities. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. These resources are focused on helping you discover how advanced features of the Amazon Aurora PostgreSQL database operate. Prior expertise with AWS and PostgreSQL-based databases is beneficial, but not required to complete the labs.","title":"Home"},{"location":"#amazon-aurora-labs-for-postgresql","text":"Welcome to the AWS workshop and lab content portal for Amazon Aurora PostgreSQL compatible databases! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the Amazon Aurora features and capabilities. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. These resources are focused on helping you discover how advanced features of the Amazon Aurora PostgreSQL database operate. Prior expertise with AWS and PostgreSQL-based databases is beneficial, but not required to complete the labs.","title":"Amazon Aurora Labs for PostgreSQL"},{"location":"contribute/","text":"Contributing Guidelines \u00b6 Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests \u00b6 We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests \u00b6 Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on \u00b6 Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct \u00b6 This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications \u00b6 If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing \u00b6 See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Contributing Guidelines"},{"location":"contribute/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"contribute/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"contribute/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"contribute/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"contribute/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"contribute/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"contribute/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Licensing"},{"location":"license/","text":"License \u00b6 MIT License Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"modules/","text":"Getting Started \u00b6 Create an AWS account \u00b6 In order to complete the hands-on content on this site, you'll need an AWS Account. We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. If you are setting up an AWS account for the first time, follow the instructions below to create an administrative IAM user account , we recommend not using your AWS account root credentials for day to day usage. If you have received credits to complete these labs follow the instructions below on adding the credits to your AWS account. Overview of labs \u00b6 The following labs are currently available, part of this instructional website: # Lab Module Recommendation Overview 1 Prerequisites Required, start here Set up the lab environment and provision the prerequisite resources 2 Create a New Aurora Cluster Optional Create a new Amazon Aurora PostgreSQL DB cluster manually 3 Connecting, Loading Data and Auto Scaling Recommended Connect to the DB cluster for the first time, load an initial data set and test read replica auto scaling. The initial data set may be used in subsequent labs. 4 Fast Cloning Recommended Cloning an Aurora DB cluster and observing the divergence of the data set. 5 Query Plan Management Recommended To control execution plans for a set of statements that you want to manage. 6 Cluster Cache Management Recommended Improves the performance of the new primary/writer instance after failover occurs. 7 Oracle to Aurora PostgreSQL migration using DMS and SCT Recommended migration of a sample database from Oracle to Amazon Aurora PostgreSQL using the AWS Database Migration Service (AWS DMS) and the AWS Schema Conversion Tool (AWS SCT) 8 Database Activity Streaming Recommended Monitoring database activity with Amazon Aurora with PostgreSQL compatibility is to use Database Activity Streams. Lab environment at a glance \u00b6 To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. The environment deployed using CloudFormation includes several components: Amazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the cluster and workstation Amazon EC2 instance configured with the software components needed for the lab IAM roles with access permissions for the workstation and cluster permissions for enhanced monitoring , S3 access and logging Custom cluster and DB instance parameter groups for the Amazon Aurora cluster, enabling logging and performance schema Optionally, Amazon Aurora DB cluster with 2 nodes: a writer and read replica If the cluster is created for you, the master database credentials will be generated automatically and stored in an AWS Secrets Manager secret. Optionally, read replica auto scaling configuration Optionally, AWS Systems Manager command document to execute a load test Create an IAM user (with admin permissions) \u00b6 If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess policy, and click Next:Review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created. Add credits (optional) \u00b6 If you are doing these workshop as part of an AWS sponsored event that doesn't provide AWS accounts, you will receive credits to cover the costs. Below are the instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem. Additional software needed for labs \u00b6 The templates and scripts setting up the lab environment install the following software in the lab environment for the purposes of deploying and running the labs: pgbench pgAdmin sysbench available using the GPL License. Percona's sysbench-tpcc available using the Apache License 2.0. Sql Plus:Oracle Client","title":"Getting Started"},{"location":"modules/#getting-started","text":"","title":"Getting Started"},{"location":"modules/#create-an-aws-account","text":"In order to complete the hands-on content on this site, you'll need an AWS Account. We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. If you are setting up an AWS account for the first time, follow the instructions below to create an administrative IAM user account , we recommend not using your AWS account root credentials for day to day usage. If you have received credits to complete these labs follow the instructions below on adding the credits to your AWS account.","title":"Create an AWS account"},{"location":"modules/#overview-of-labs","text":"The following labs are currently available, part of this instructional website: # Lab Module Recommendation Overview 1 Prerequisites Required, start here Set up the lab environment and provision the prerequisite resources 2 Create a New Aurora Cluster Optional Create a new Amazon Aurora PostgreSQL DB cluster manually 3 Connecting, Loading Data and Auto Scaling Recommended Connect to the DB cluster for the first time, load an initial data set and test read replica auto scaling. The initial data set may be used in subsequent labs. 4 Fast Cloning Recommended Cloning an Aurora DB cluster and observing the divergence of the data set. 5 Query Plan Management Recommended To control execution plans for a set of statements that you want to manage. 6 Cluster Cache Management Recommended Improves the performance of the new primary/writer instance after failover occurs. 7 Oracle to Aurora PostgreSQL migration using DMS and SCT Recommended migration of a sample database from Oracle to Amazon Aurora PostgreSQL using the AWS Database Migration Service (AWS DMS) and the AWS Schema Conversion Tool (AWS SCT) 8 Database Activity Streaming Recommended Monitoring database activity with Amazon Aurora with PostgreSQL compatibility is to use Database Activity Streams.","title":"Overview of labs"},{"location":"modules/#lab-environment-at-a-glance","text":"To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. The environment deployed using CloudFormation includes several components: Amazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the cluster and workstation Amazon EC2 instance configured with the software components needed for the lab IAM roles with access permissions for the workstation and cluster permissions for enhanced monitoring , S3 access and logging Custom cluster and DB instance parameter groups for the Amazon Aurora cluster, enabling logging and performance schema Optionally, Amazon Aurora DB cluster with 2 nodes: a writer and read replica If the cluster is created for you, the master database credentials will be generated automatically and stored in an AWS Secrets Manager secret. Optionally, read replica auto scaling configuration Optionally, AWS Systems Manager command document to execute a load test","title":"Lab environment at a glance"},{"location":"modules/#create-an-iam-user-with-admin-permissions","text":"If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess policy, and click Next:Review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.","title":"Create an IAM user (with admin permissions)"},{"location":"modules/#add-credits-optional","text":"If you are doing these workshop as part of an AWS sponsored event that doesn't provide AWS accounts, you will receive credits to cover the costs. Below are the instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem.","title":"Add credits (optional)"},{"location":"modules/#additional-software-needed-for-labs","text":"The templates and scripts setting up the lab environment install the following software in the lab environment for the purposes of deploying and running the labs: pgbench pgAdmin sysbench available using the GPL License. Percona's sysbench-tpcc available using the Apache License 2.0. Sql Plus:Oracle Client","title":"Additional software needed for labs"},{"location":"modules/ccm/","text":"Cluster Cache Management \u00b6 1. Prerequisite \u00b6 Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file 2. Creating Aurora PostgreSQL cluster with Cloudformation \u00b6 Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013 2.1 Retrieving Database credentials from Secret Manager \u00b6 Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database. 2.2 Cloudformation Resource chart \u00b6 Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d 2.3 Connecting to the EC2 bastion Instance \u00b6 We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here . 3. Quick Start Guide on using Cluster Cache Management \u00b6 Cluster cache management is supported for Aurora PostgreSQL DB clusters of versions 9.6.11 and above, and versions 10.5 and above. 3.1. Configuring Cluster Cache Management (CCM) \u00b6 Following are the steps to configure and enable the use of CCM on your Aurora PostgreSQL cluster Modify the Amazon Aurora DB Cluster Parameters related to the CCM. Screenshots of some of the steps are shown below a. Sign in to the AWS Management Console and open the Amazon RDS console b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format . In this case the stack name is \u201clabstack\u201d and hence the DB cluster parameter group file is \u201clabstack- apgcustom-clusterparamgroup-nnnn >. The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group . d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of the apg_ccm_enabled cluster parameter to 1 and click on \u201cSave changes\u201d f. Choose Save changes. For more Information, see Modifying Parameters in a DB Cluster Parameter Group . 2. Set the Promotion Tier Priority for the Writer DB Instance a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases. c. Choose the Writer DB instance of the Aurora PostgreSQL DB cluster and click on \u201cModify\u201d d. Choose Modify. The Modify DB Instance page appears. e. On the Failover panel, choose tier-0 for the Priority. f. Choose Continue and check the summary of modifications. g. To apply the changes immediately after you save them, choose Apply immediately. h. Choose Modify DB Instance to save your changes For more information about setting the promotion tier, see Modify a DB Instance in a DB Cluster and the Promotion tier setting . See also, Fault Tolerance for an Aurora DB Cluster . 3. Set the Promotion Tier Priority for the Reader DB Instance Repeat the steps from the above section 2 for the reader instance to act as a failover target. To designate a specific replica for cluster cache management, set the promotion tier priority to 0 for that Aurora replica. The promotion tier priority is a value that specifies the order in which an Aurora replica is promoted to the primary DB instance after a failure. Valid values are 0 to 15, where 0 is the highest and 15 the lowest priority. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases. c. Choose the Reader DB instance of the Aurora PostgreSQL DB cluster and click on \u201cModify\u201d d. Choose Modify. The Modify DB Instance page appears. e. On the Failover panel, choose tier-0 for the Priority. f. Choose Continue and check the summary of modifications. g. To apply the changes immediately after you save them, choose Apply immediately. h. Choose Modify DB Instance to save your changes 3.2. Verifying if CCM is enabled \u00b6 To verify if the CCM is enabled, query the function aurora_ccm_status() with the following code using psql. Please refer the documentation on how to connect to PostgreSQL using psql here. (please replace the endpoint, port, username and the database name with your specific setup). a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer DB instance of the Aurora PostgreSQL DB ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> select aurora_version(),version(); aurora_version | version ----------------+----------------------------------------------------------------------------- 2.3.5 | PostgreSQL 10.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.9.3, 64-bit (1 row) mylab=> \\x Expanded display is on. ccmdb=> select * from aurora_ccm_status(); - [ RECORD 1 ]--------------+--------- buffers_sent_last_minute | 2242000 buffers_found_last_minute | 2242003 buffers_sent_last_scan | 17920442 buffers_found_last_scan | 17923410 buffers_sent_current_scan | 14098000 buffers_found_current_scan | 14100964 current_scan_progress | 15877443 If the Cluster Cache management is not enabled. Querying aurora_ccm_status() will display the below output mylab=> \\x Expanded display is on. noccmdb=> select * from aurora_ccm_status(); ERROR: Cluster Cache Manager is disabled noccmdb=> 4. Benchmarking with CCM enabled (before Failover) \u00b6 Please refer above table for the resource\u2019s appropriate values for cluster endpoint, DB name etc... Please note that the scale factor 10000 is used for the data load in the sample CCMDB. For the Database created with the Cloudformation template we are using scale factor=100. Optional: - In order to reproduce the benchmarking used in the lab, we need to add more sample data to our benchmark. The below command is using scale factor of 10000. ./pgbench -i --fillfactor=100 --scale=10000 --host=labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com\u2014username= masteruser mylab Connect to the Writer node using cluster endpoint of the cluster we created. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab e. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. mylab=> SELECT pg_size_pretty( pg_database_size('mylab')); pg_size_pretty ---------------- 1643 MB (1 row) If optionally you have used the scale factor 10000 mylab=> SELECT pg_size_pretty( pg_database_size('mylab') ); pg_size_pretty ---------------- 171 GB (1 row) Running the benchmarking We are using pgbench benchmarking option tpcb-like and using \u201c@\u201d to specify the probability of running read-only workload and read-write workload. In the below example we are running tpcb-like workload with 20X read-only workload and 1x read-write workload for 600 seconds. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_before_failover . out 4.1 Using pg_buffercache (CCM enabled) \u00b6 Pg_buffercache extension provides a means to look into the contents of the buffer cache. We will be leveraging the pg_buffercache view to examine the content of the buffer cache (with the CCM enabled and CCM disabled) to illustrate the effect of the CCM. We will compare the content of the buffer cache of the Writer node with the Read-only node. With CCM enabled the content of the buffer cache of the writer node and the read-only will be similar due to the fact that in the CCM enabled cluster the Writer node will periodically sends buffer addresses of the frequently used buffers (defaults to usage count>3) to the Read-only node. The content of the pg_buffercache view is generated with different dataset (Scale =10000) as compared to (scale=100) used in this workshop so you will be getting different results. Connect to the Writer node using the cluster endpoint of your cluster. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab e. Create extension pg_buffercache on the Database. mylab=> CREATE EXTENSION pg_buffercache; CREATE EXTENSION ccmdb=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Writer node. ccmdb=> show transaction_read_only; -[ RECORD 1 ]---------+---- transaction_read_only | off SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- pgbench_accounts | 18182376 pgbench_accounts_pkey | 2741898 pgbench_history | 58807 pgbench_tellers | 7286 pgbench_branches | 5137 pgbench_tellers_pkey | 2197 pgbench_branches_pkey | 1130 pg_attribute | 30 pg_statistic | 24 pg_proc | 18 Connecting to the read replica a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Reader Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-ro-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab ccmdb=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Read-only node. mylab=> show transaction_read_only; -[ RECORD 1 ]---------+--- transaction_read_only | on SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- pgbench_accounts | 18162144 pgbench_accounts_pkey | 2741869 pgbench_history | 58804 pgbench_tellers | 7144 pgbench_branches | 5028 pgbench_tellers_pkey | 2156 pgbench_branches_pkey | 1109 pg_attribute | 26 pg_statistic | 24 pg_operator | 15 4.2 Collecting the pgbench benchmarking metrics. \u00b6 We initiated 600 seconds benchmarking on the Aurora PostgreSQL with CCM enabled above using the below command . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_before_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_enable_before_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 192341828 latency average = 1.558 ms latency stddev = 3.434 ms tps = 320517.529436 (including connections establishing) tps = 320766.435473 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 9121117 transactions (4.7% of total, tps = 15199.387034) - latency average = 15.840 ms - latency stddev = 5.073 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 179708569 transactions (93.4% of total, tps = 299465.525275) - latency average = 0.851 ms - latency stddev = 0.613 ms 4.3 CCM enabled failover benchmarking \u00b6 We are Initiating the failover from the console after 600 seconds and running the same workload once the failover is completed. For initiating the failover please go the RDS console, select your cluster and click on the writer instance and go the actions and click \u201cFailover\u201d. Once the failover is completed after about ~30 seconds. Verify the previous reader instance becomes the new writer. We will be running the same benchmarking as we did before the failover and then we will compare the pgbench metrics before and after the failover. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_after_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_enable_after_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 190891585 latency average = 1.570 ms latency stddev = 3.624 ms tps = 318119.106352 (including connections establishing) tps = 318350.153525 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 9053479 transactions (4.7% of total, tps = 15087.541176) - latency average = 16.704 ms - latency stddev = 5.387 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 178217587 transactions (93.4% of total, tps = 296998.002885) - latency average = 0.821 ms - latency stddev = 0.560 ms 4.4 Benchmarking on CCM disabled cluster \u00b6 Disabling the CCM Disable the CCM on the Aurora PostgreSQL cluster by modifying the cluster parameter group file as show as above and set the value of the apg_ccm_enabled cluster parameter to 0. Modify the Amazon Aurora DB Cluster Parameters related to the CCM. Screenshots of some of the steps are shown below a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format . In this case the stack name is \u201clabstack\u201d and hence the DB cluster parameter group file is \u201clabstack- apgcustom-clusterparamgroup-nnnn >. The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group . d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of the apg_ccm_enabled cluster parameter to 0 and click on \u201cSave changes\u201d Verify if the Cluster Cache Management is disabled, query the function aurora_ccm_status() as shown below mylab=> \\x Expanded display is on. mylab=> select * from aurora_ccm_status(); ERROR: Cluster Cache Manager is disabled mylab=> Stop and Start the Aurora PostgreSQL cluster Since we are using the same Aurora PostgreSQL cluster to the testing for the with CCM (enabled and disabled) and since earlier testing with CCM already warmed the buffer cache of the read and write Instance. Therefore, we are stopping and starting the cluster before running the benchmarking. We could also reboot both the reader and the writer Instance, but this may not guarantee that the writer and reader come up with empty buffer cache. To stop the cluster Verify the cluster status is shown as \u201cAvailable\u201d, then click on the Actions and choose \u201cstop\u201d To start the cluster Once the cluster status changes to\u201d stop\u201d, click on the Actions and choose \u201cstart\u201d Running the benchwork We are using pgbench benchmarking option tpcb-like and using \u201c@\u201d to specify the probability of running read-only workload and read-write workload. In the below example we are running tpcb-like workload with 20X read-only workload and 1x read-write workload for 600 seconds. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_disable_before_failover . out 4.5. Using pg_buffercache (CCM disabled) \u00b6 Connect to the Writer node using the cluster endpoint of your cluster. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Writer node. ccmdb=> show transaction_read_only; -[ RECORD 1 ]---------+---- transaction_read_only | off SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- relname | buffers -----------------------+----------+----------+---------- pgbench_accounts | 15401635 pgbench_accounts_pkey | 2741889 pgbench_history | 17024 pgbench_tellers | 4992 pgbench_branches | 3647 pgbench_tellers_pkey | 2200 pgbench_branches_pkey | 930 pg_attribute | 31 pg_statistic | 20 pg_proc | 20 (10 rows) Connect to the read replica a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Reader Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-ro-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab ccmdb=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Read-only node. mylab=> show transaction_read_only; -[ RECORD 1 ]---------+--- transaction_read_only | on SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- pgbench_history | 11905 pg_attribute | 26 pg_class | 11 pg_proc | 10 pg_proc_oid_index | 8 pg_attribute_relid_attnum_index | 7 pg_proc_proname_args_nsp_index | 5 pg_index | 4 pg_amproc | 4 pg_class_relname_nsp_index | 4 (10 rows) Collecting the 600 seconds pgbench benchmarking metrics. We initiated 600 seconds benchmarking on the Aurora PostgreSQL with CCM enabled above using the below command . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_disable_before_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_disable_before_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 192341828 latency average = 1.558 ms latency stddev = 3.434 ms tps = 320517.529436 (including connections establishing) tps = 320766.435473 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 9121117 transactions (4.7% of total, tps = 15199.387034) - latency average = 15.840 ms - latency stddev = 5.073 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 179708569 transactions (93.4% of total, tps = 299465.525275) - latency average = 0.851 ms - latency stddev = 0.613 ms We are Initiating the failover from the console after 600 seconds and resuming the same workload once the failover is completed. For initiating the failover please go the After failover benchmarking We are Initiating the failover from the console after 600 seconds and running the same workload once the failover is completed. For initiating the failover please go the RDS console, select your cluster and click on the writer instance and go the actions and click \u201cFailover\u201d. Once the failover is completed after about ~30 seconds. Verify the previous reader instance becomes the new writer. We will be running the same benchmarking as we did before the failover and then we will compare the pgbench metrics before and after the failover. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_after_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_enable_after_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 85237916 latency average = 3.518 ms latency stddev = 7.063 ms tps = 142047.852230 (including connections establishing) tps = 142129.718565 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 4048031 transactions (4.7% of total, tps = 6745.989769) - latency average = 19.971 ms - latency stddev = 9.161 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 80121228 transactions (94.0% of total, tps = 133520.959797) - latency average = 2.728 ms - latency stddev = 5.870 ms 4.6. CCM vs no CCM benchmarking results (for illustration only) \u00b6 This benchmarking and the graph are only for illustrative purpose, this benchmarking is done on the r4.16xl instance size and with scale factor 10000 for data load. This workshop creates Aurora PostgreSQL Database on R5.16xlarge instance class and data is loaded using and scale factor 100 so you are expected to get different results. In the graph below, we are comparing the results from the benchmarking done on the CCM enabled and CCM disabled Aurora PostgreSQL clusters. The CCM enabled cluster was able to scale up to the average 90th Percentile of the Transaction per Second (TPS), whereas the CCM disabled cluster took about 357 seconds (990-633) to scale up to the 90th Percentile of the Transaction per Second","title":"Cluster Cache Management"},{"location":"modules/ccm/#cluster-cache-management","text":"","title":"Cluster Cache Management"},{"location":"modules/ccm/#1-prerequisite","text":"Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file","title":"1. Prerequisite"},{"location":"modules/ccm/#2-creating-aurora-postgresql-cluster-with-cloudformation","text":"Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013","title":"2. Creating Aurora PostgreSQL cluster with Cloudformation"},{"location":"modules/ccm/#21-retrieving-database-credentials-from-secret-manager","text":"Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database.","title":"2.1 Retrieving Database credentials from Secret Manager"},{"location":"modules/ccm/#22-cloudformation-resource-chart","text":"Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d","title":"2.2 Cloudformation Resource chart"},{"location":"modules/ccm/#23-connecting-to-the-ec2-bastion-instance","text":"We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here .","title":"2.3 Connecting to the EC2 bastion Instance"},{"location":"modules/ccm/#3-quick-start-guide-on-using-cluster-cache-management","text":"Cluster cache management is supported for Aurora PostgreSQL DB clusters of versions 9.6.11 and above, and versions 10.5 and above.","title":"3. Quick Start Guide on using Cluster Cache Management"},{"location":"modules/ccm/#31-configuring-cluster-cache-management-ccm","text":"Following are the steps to configure and enable the use of CCM on your Aurora PostgreSQL cluster Modify the Amazon Aurora DB Cluster Parameters related to the CCM. Screenshots of some of the steps are shown below a. Sign in to the AWS Management Console and open the Amazon RDS console b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format . In this case the stack name is \u201clabstack\u201d and hence the DB cluster parameter group file is \u201clabstack- apgcustom-clusterparamgroup-nnnn >. The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group . d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of the apg_ccm_enabled cluster parameter to 1 and click on \u201cSave changes\u201d f. Choose Save changes. For more Information, see Modifying Parameters in a DB Cluster Parameter Group . 2. Set the Promotion Tier Priority for the Writer DB Instance a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases. c. Choose the Writer DB instance of the Aurora PostgreSQL DB cluster and click on \u201cModify\u201d d. Choose Modify. The Modify DB Instance page appears. e. On the Failover panel, choose tier-0 for the Priority. f. Choose Continue and check the summary of modifications. g. To apply the changes immediately after you save them, choose Apply immediately. h. Choose Modify DB Instance to save your changes For more information about setting the promotion tier, see Modify a DB Instance in a DB Cluster and the Promotion tier setting . See also, Fault Tolerance for an Aurora DB Cluster . 3. Set the Promotion Tier Priority for the Reader DB Instance Repeat the steps from the above section 2 for the reader instance to act as a failover target. To designate a specific replica for cluster cache management, set the promotion tier priority to 0 for that Aurora replica. The promotion tier priority is a value that specifies the order in which an Aurora replica is promoted to the primary DB instance after a failure. Valid values are 0 to 15, where 0 is the highest and 15 the lowest priority. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases. c. Choose the Reader DB instance of the Aurora PostgreSQL DB cluster and click on \u201cModify\u201d d. Choose Modify. The Modify DB Instance page appears. e. On the Failover panel, choose tier-0 for the Priority. f. Choose Continue and check the summary of modifications. g. To apply the changes immediately after you save them, choose Apply immediately. h. Choose Modify DB Instance to save your changes","title":"3.1. Configuring Cluster Cache Management (CCM)"},{"location":"modules/ccm/#32-verifying-if-ccm-is-enabled","text":"To verify if the CCM is enabled, query the function aurora_ccm_status() with the following code using psql. Please refer the documentation on how to connect to PostgreSQL using psql here. (please replace the endpoint, port, username and the database name with your specific setup). a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer DB instance of the Aurora PostgreSQL DB ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> select aurora_version(),version(); aurora_version | version ----------------+----------------------------------------------------------------------------- 2.3.5 | PostgreSQL 10.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.9.3, 64-bit (1 row) mylab=> \\x Expanded display is on. ccmdb=> select * from aurora_ccm_status(); - [ RECORD 1 ]--------------+--------- buffers_sent_last_minute | 2242000 buffers_found_last_minute | 2242003 buffers_sent_last_scan | 17920442 buffers_found_last_scan | 17923410 buffers_sent_current_scan | 14098000 buffers_found_current_scan | 14100964 current_scan_progress | 15877443 If the Cluster Cache management is not enabled. Querying aurora_ccm_status() will display the below output mylab=> \\x Expanded display is on. noccmdb=> select * from aurora_ccm_status(); ERROR: Cluster Cache Manager is disabled noccmdb=>","title":"3.2. Verifying if CCM is enabled"},{"location":"modules/ccm/#4-benchmarking-with-ccm-enabled-before-failover","text":"Please refer above table for the resource\u2019s appropriate values for cluster endpoint, DB name etc... Please note that the scale factor 10000 is used for the data load in the sample CCMDB. For the Database created with the Cloudformation template we are using scale factor=100. Optional: - In order to reproduce the benchmarking used in the lab, we need to add more sample data to our benchmark. The below command is using scale factor of 10000. ./pgbench -i --fillfactor=100 --scale=10000 --host=labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com\u2014username= masteruser mylab Connect to the Writer node using cluster endpoint of the cluster we created. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab e. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. mylab=> SELECT pg_size_pretty( pg_database_size('mylab')); pg_size_pretty ---------------- 1643 MB (1 row) If optionally you have used the scale factor 10000 mylab=> SELECT pg_size_pretty( pg_database_size('mylab') ); pg_size_pretty ---------------- 171 GB (1 row) Running the benchmarking We are using pgbench benchmarking option tpcb-like and using \u201c@\u201d to specify the probability of running read-only workload and read-write workload. In the below example we are running tpcb-like workload with 20X read-only workload and 1x read-write workload for 600 seconds. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_before_failover . out","title":"4. Benchmarking with CCM enabled (before Failover)"},{"location":"modules/ccm/#41-using-pg_buffercache-ccm-enabled","text":"Pg_buffercache extension provides a means to look into the contents of the buffer cache. We will be leveraging the pg_buffercache view to examine the content of the buffer cache (with the CCM enabled and CCM disabled) to illustrate the effect of the CCM. We will compare the content of the buffer cache of the Writer node with the Read-only node. With CCM enabled the content of the buffer cache of the writer node and the read-only will be similar due to the fact that in the CCM enabled cluster the Writer node will periodically sends buffer addresses of the frequently used buffers (defaults to usage count>3) to the Read-only node. The content of the pg_buffercache view is generated with different dataset (Scale =10000) as compared to (scale=100) used in this workshop so you will be getting different results. Connect to the Writer node using the cluster endpoint of your cluster. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab e. Create extension pg_buffercache on the Database. mylab=> CREATE EXTENSION pg_buffercache; CREATE EXTENSION ccmdb=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Writer node. ccmdb=> show transaction_read_only; -[ RECORD 1 ]---------+---- transaction_read_only | off SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- pgbench_accounts | 18182376 pgbench_accounts_pkey | 2741898 pgbench_history | 58807 pgbench_tellers | 7286 pgbench_branches | 5137 pgbench_tellers_pkey | 2197 pgbench_branches_pkey | 1130 pg_attribute | 30 pg_statistic | 24 pg_proc | 18 Connecting to the read replica a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Reader Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-ro-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab ccmdb=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Read-only node. mylab=> show transaction_read_only; -[ RECORD 1 ]---------+--- transaction_read_only | on SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- pgbench_accounts | 18162144 pgbench_accounts_pkey | 2741869 pgbench_history | 58804 pgbench_tellers | 7144 pgbench_branches | 5028 pgbench_tellers_pkey | 2156 pgbench_branches_pkey | 1109 pg_attribute | 26 pg_statistic | 24 pg_operator | 15","title":"4.1 Using pg_buffercache (CCM enabled)"},{"location":"modules/ccm/#42-collecting-the-pgbench-benchmarking-metrics","text":"We initiated 600 seconds benchmarking on the Aurora PostgreSQL with CCM enabled above using the below command . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_before_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_enable_before_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 192341828 latency average = 1.558 ms latency stddev = 3.434 ms tps = 320517.529436 (including connections establishing) tps = 320766.435473 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 9121117 transactions (4.7% of total, tps = 15199.387034) - latency average = 15.840 ms - latency stddev = 5.073 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 179708569 transactions (93.4% of total, tps = 299465.525275) - latency average = 0.851 ms - latency stddev = 0.613 ms","title":"4.2 Collecting the pgbench benchmarking metrics."},{"location":"modules/ccm/#43-ccm-enabled-failover-benchmarking","text":"We are Initiating the failover from the console after 600 seconds and running the same workload once the failover is completed. For initiating the failover please go the RDS console, select your cluster and click on the writer instance and go the actions and click \u201cFailover\u201d. Once the failover is completed after about ~30 seconds. Verify the previous reader instance becomes the new writer. We will be running the same benchmarking as we did before the failover and then we will compare the pgbench metrics before and after the failover. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_after_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_enable_after_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 190891585 latency average = 1.570 ms latency stddev = 3.624 ms tps = 318119.106352 (including connections establishing) tps = 318350.153525 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 9053479 transactions (4.7% of total, tps = 15087.541176) - latency average = 16.704 ms - latency stddev = 5.387 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 178217587 transactions (93.4% of total, tps = 296998.002885) - latency average = 0.821 ms - latency stddev = 0.560 ms","title":"4.3 CCM enabled failover benchmarking"},{"location":"modules/ccm/#44-benchmarking-on-ccm-disabled-cluster","text":"Disabling the CCM Disable the CCM on the Aurora PostgreSQL cluster by modifying the cluster parameter group file as show as above and set the value of the apg_ccm_enabled cluster parameter to 0. Modify the Amazon Aurora DB Cluster Parameters related to the CCM. Screenshots of some of the steps are shown below a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format . In this case the stack name is \u201clabstack\u201d and hence the DB cluster parameter group file is \u201clabstack- apgcustom-clusterparamgroup-nnnn >. The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group . d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of the apg_ccm_enabled cluster parameter to 0 and click on \u201cSave changes\u201d Verify if the Cluster Cache Management is disabled, query the function aurora_ccm_status() as shown below mylab=> \\x Expanded display is on. mylab=> select * from aurora_ccm_status(); ERROR: Cluster Cache Manager is disabled mylab=> Stop and Start the Aurora PostgreSQL cluster Since we are using the same Aurora PostgreSQL cluster to the testing for the with CCM (enabled and disabled) and since earlier testing with CCM already warmed the buffer cache of the read and write Instance. Therefore, we are stopping and starting the cluster before running the benchmarking. We could also reboot both the reader and the writer Instance, but this may not guarantee that the writer and reader come up with empty buffer cache. To stop the cluster Verify the cluster status is shown as \u201cAvailable\u201d, then click on the Actions and choose \u201cstop\u201d To start the cluster Once the cluster status changes to\u201d stop\u201d, click on the Actions and choose \u201cstart\u201d Running the benchwork We are using pgbench benchmarking option tpcb-like and using \u201c@\u201d to specify the probability of running read-only workload and read-write workload. In the below example we are running tpcb-like workload with 20X read-only workload and 1x read-write workload for 600 seconds. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_disable_before_failover . out","title":"4.4 Benchmarking on CCM disabled cluster"},{"location":"modules/ccm/#45-using-pg_buffercache-ccm-disabled","text":"Connect to the Writer node using the cluster endpoint of your cluster. a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Writer Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Writer node. ccmdb=> show transaction_read_only; -[ RECORD 1 ]---------+---- transaction_read_only | off SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- relname | buffers -----------------------+----------+----------+---------- pgbench_accounts | 15401635 pgbench_accounts_pkey | 2741889 pgbench_history | 17024 pgbench_tellers | 4992 pgbench_branches | 3647 pgbench_tellers_pkey | 2200 pgbench_branches_pkey | 930 pg_attribute | 31 pg_statistic | 20 pg_proc | 20 (10 rows) Connect to the read replica a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. c. Click on the selected cluster above and look under the section \u201cConnectivity and Security\u201d. You will see 2 different endpoints under the \u201cType\u201d column. The one on the top will be the cluster endpoint (for read-write) and the other one will be reader endpoint (for read-only). d. Choose the Reader Endpoint of the Aurora PostgreSQL Cluster. ./psql -h labstack-cluster.cluster-ro-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab ccmdb=> \\dx pg_buffercache List of installed extensions Name | Version | Schema | Description ----------------+---------+--------+--------------------------------- pg_buffercache | 1.3 | public | examine the shared buffer cache (1 row) -- Verify if we are connected to the Read-only node. mylab=> show transaction_read_only; -[ RECORD 1 ]---------+--- transaction_read_only | on SELECT c.relname, count(*) AS buffers FROM pg_buffercache b INNER JOIN pg_class c ON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -----------------------+----------+----------+---------- pgbench_history | 11905 pg_attribute | 26 pg_class | 11 pg_proc | 10 pg_proc_oid_index | 8 pg_attribute_relid_attnum_index | 7 pg_proc_proname_args_nsp_index | 5 pg_index | 4 pg_amproc | 4 pg_class_relname_nsp_index | 4 (10 rows) Collecting the 600 seconds pgbench benchmarking metrics. We initiated 600 seconds benchmarking on the Aurora PostgreSQL with CCM enabled above using the below command . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_disable_before_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_disable_before_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 192341828 latency average = 1.558 ms latency stddev = 3.434 ms tps = 320517.529436 (including connections establishing) tps = 320766.435473 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 9121117 transactions (4.7% of total, tps = 15199.387034) - latency average = 15.840 ms - latency stddev = 5.073 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 179708569 transactions (93.4% of total, tps = 299465.525275) - latency average = 0.851 ms - latency stddev = 0.613 ms We are Initiating the failover from the console after 600 seconds and resuming the same workload once the failover is completed. For initiating the failover please go the After failover benchmarking We are Initiating the failover from the console after 600 seconds and running the same workload once the failover is completed. For initiating the failover please go the RDS console, select your cluster and click on the writer instance and go the actions and click \u201cFailover\u201d. Once the failover is completed after about ~30 seconds. Verify the previous reader instance becomes the new writer. We will be running the same benchmarking as we did before the failover and then we will compare the pgbench metrics before and after the failover. . / pgbench -- progress - timestamp - M prepared - n - T 600 - P 5 - c 50 - j 50 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > ccm_enable_after_failover . out After 600 seconds once, the benchmark is completed you can verify the pgbench output on the screen (on the psql console) or can refer the output file \u201cccm_enable_after_failover.out\u201d. The summary output at the bottom will look like below. Please note that the output may be little bit different in your case. transaction type: multiple scripts scaling factor: 10000 query mode: prepared number of clients: 500 number of threads: 500 duration: 600 s number of transactions actually processed: 85237916 latency average = 3.518 ms latency stddev = 7.063 ms tps = 142047.852230 (including connections establishing) tps = 142129.718565 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 4048031 transactions (4.7% of total, tps = 6745.989769) - latency average = 19.971 ms - latency stddev = 9.161 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 80121228 transactions (94.0% of total, tps = 133520.959797) - latency average = 2.728 ms - latency stddev = 5.870 ms","title":"4.5. Using pg_buffercache (CCM disabled)"},{"location":"modules/ccm/#46-ccm-vs-no-ccm-benchmarking-results-for-illustration-only","text":"This benchmarking and the graph are only for illustrative purpose, this benchmarking is done on the r4.16xl instance size and with scale factor 10000 for data load. This workshop creates Aurora PostgreSQL Database on R5.16xlarge instance class and data is loaded using and scale factor 100 so you are expected to get different results. In the graph below, we are comparing the results from the benchmarking done on the CCM enabled and CCM disabled Aurora PostgreSQL clusters. The CCM enabled cluster was able to scale up to the average 90th Percentile of the Transaction per Second (TPS), whereas the CCM disabled cluster took about 357 seconds (990-633) to scale up to the 90th Percentile of the Transaction per Second","title":"4.6. CCM vs no CCM benchmarking results (for illustration only)"},{"location":"modules/create/","text":"Creating a New Aurora Cluster \u00b6 Note If you are familiar with the basic concepts of Amazon Aurora MySQL, and have created a cluster in the past, you may skip this module, by using provisioning the lab environment using the lab-with-cluster.yml CloudFormation template, so the DB cluster is provisioned for you. Skip to Connecting, Loading Data and Auto Scaling . This lab will walk you through the steps of creating an Amazon Aurora database cluster manually, and configuring app the parameters required for the cluster components. At the end of this lab you will have a database cluster ready to be used in subsequent labs. This lab contains the following tasks: Creating the DB cluster Retrieving the DB cluster endpoints Assigning an IAM role to the DB cluster Creating a replica auto scaling policy This lab requires the following lab modules to be completed first: Prerequisites (using lab-no-cluster-pgsql.yml template is sufficient) 1. Creating the DB cluster \u00b6 Open the Amazon RDS service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create database to start the configuration process Note The RDS console database creation workflow has been simplified recently. Depending on your previous usage of the RDS console UI, you may see the old workflow or the new one, you may also be presented with a prompt to toggle between them. In this lab we are using the new workflow for reference, although the steps will work similarly in the old console workflow as well, if you are more familiar with it. In the first configuration section of the Create database page, called Database settings ensure the Easy create toggle button is turned OFF (grey color). Next, in the Engine options section, choose the Amazon Aurora engine type, the Amazon Aurora with PostgreSQL compatibility edition, the Aurora (PostgreSQL 10.7) version. In the Database features section, select One writer and multiple readers , and in the Templates section, select Production . The selections so far will instruct AWS to create an Aurora PostgreSQL database cluster with the most recent version of the PostgreSQL 10.7 compatible engine in a highly available configuration with one writer and one reader database instance in the cluster. In the Settings section give your database cluster a recognizable identifier, such as labstack-cluster . Configure the name and password of the master database user, with the most elevated permissions in the database. We recommend to use the user name masteruser for consistency with subsequent labs and a password of your choosing. For simplicity ensure the check box Auto generate a password is not checked . For the DB Instance size choose the default DB Instance class \u201cdb.r5.large\u201d. For the Availability and durability choose the default option to deploy Multi-AZ deployment by choosing \u201cCreate an Aurora Replica/Reader node in a different AZ (recommended for scaled availability)\u201d In the Connectivity section, expand the sub-section called Additional connectivity configuration . This section allows you to specify where the database cluster will be deployed within your defined network configuration. To simplify the labs, the CloudFormation stack you deployed in the preceding Prerequisites module, has configured a VPC that includes all resources needed for an Aurora database cluster. This includes the VPC itself, subnets, DB subnet groups, security groups and several other networking constructs. All you need to do is select the appropriate existing connectivity controls in this section. Pick the Virtual Private Cloud (VPC) named after the CloudFormation stack name, such as labstack-vpc . Similarly make sure the selected Subnet Group also matches the stack name (e.g. labstack-dbsubnets-[hash] ). Make sure the cluster Publicly accessible option is set to No . The lab environment also configured a VPC security group that allows your lab workspace EC2 instance to connect to the database. Make sure the Choose existing security group option is selected and from the dropdown pick the security group with a name ending in -pgsql-internal (eg. labstack-pgsql-internal ). Please remove any other security groups, such as default from the selection. For Database authentication choose the default option to use \u201cPassword authentication\u201d Next, expand the Additional configuration section. Set the Initial database name to mylab . For the DB cluster parameter group and DB parameter group selectors, choose the groups with the stack name in their name (e.g. labstack-[...] ). Choose a 7 days Backup retention period . Check the box to Enable encryption and select the [default] aws/rds for the Master key . Check the box to Enable Performance Insights with a Retention period of Default (7 days) and use the [default] aws/rds Master key for monitoring data encryption. Next, check the Enable Enhanced Monitoring box, and select a Granularity of 1 second . For Log exports check the Postgresql log** boxes. Also in the Advanced configuration section, de-select the check box Enable delete protection . In a production use case, you will want to leave that option checked, but for testing purposes, un-checking this option will make it easier to clean up the resources once you have completed the labs. Before continuing, let's summarize the configuration options selected. You will create a database cluster with the following characteristics: Aurora PostgreSQL 10.7 compatible (latest stable engine version) Cluster composed of a writer and a reader DB instance in different availability zones (highly available) Deployed in the VPC and using the network configuration of the lab environment Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption With Enhanced Monitoring and Performance Insights enabled Click Create database to provision the DB cluster. 2. Retrieving the DB cluster endpoints \u00b6 The database cluster may take several minutes to provision, including the DB instances making up the cluster. In order to connect to the DB cluster and start using it in subsequent labs, you need to retrieve the DB cluster endpoints. There are two endpoints created by default. The Cluster Endpoint will always point to the writer DB instance of the cluster, and should be used for both writes and reads. The Reader Endpoint will always resolve to one of the reader DB instances and should be used to offload read operations to read replicas. In the RDS console, go to the DB cluster detail view by clicking on the cluster DB identifier. The Endpoints section in the Connectivity and security tab of the details page displays the endpoints. Note these values down, as you will use them later. 3. Assigning an IAM role to the DB cluster \u00b6 Once created, you should assign an IAM role to the DB cluster, in order to allow the cluster access to Amazon S3 for importing and exporting data. The IAM role has already been created using CloudFormation when you created the lab environment. On the same DB cluster detail page as before, in the Manage IAM roles section, choose the IAM role named after the stack name, ending in -integrate-[region] (e.g. labstack-integrate-[region] ). Then click Add role . Once the operation completes the Status of the role will change from Pending to Active . 4. Creating a replica auto scaling policy \u00b6 Finally, you will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load. In the top right corner of the details page, click on Actions and then on Add replica auto scaling . Provide a Policy name based on the stack name, such as labstack-autoscale-readers . For the Target metric choose Average CPU utilization of Aurora Replicas . Enter a Target value of 20 percent. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes. Next, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs. In the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2 . In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost of associated with the labs we limit the number of readers to two. Next click Add policy .","title":"Creating a New Aurora Cluster"},{"location":"modules/create/#creating-a-new-aurora-cluster","text":"Note If you are familiar with the basic concepts of Amazon Aurora MySQL, and have created a cluster in the past, you may skip this module, by using provisioning the lab environment using the lab-with-cluster.yml CloudFormation template, so the DB cluster is provisioned for you. Skip to Connecting, Loading Data and Auto Scaling . This lab will walk you through the steps of creating an Amazon Aurora database cluster manually, and configuring app the parameters required for the cluster components. At the end of this lab you will have a database cluster ready to be used in subsequent labs. This lab contains the following tasks: Creating the DB cluster Retrieving the DB cluster endpoints Assigning an IAM role to the DB cluster Creating a replica auto scaling policy This lab requires the following lab modules to be completed first: Prerequisites (using lab-no-cluster-pgsql.yml template is sufficient)","title":"Creating a New Aurora Cluster"},{"location":"modules/create/#1-creating-the-db-cluster","text":"Open the Amazon RDS service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create database to start the configuration process Note The RDS console database creation workflow has been simplified recently. Depending on your previous usage of the RDS console UI, you may see the old workflow or the new one, you may also be presented with a prompt to toggle between them. In this lab we are using the new workflow for reference, although the steps will work similarly in the old console workflow as well, if you are more familiar with it. In the first configuration section of the Create database page, called Database settings ensure the Easy create toggle button is turned OFF (grey color). Next, in the Engine options section, choose the Amazon Aurora engine type, the Amazon Aurora with PostgreSQL compatibility edition, the Aurora (PostgreSQL 10.7) version. In the Database features section, select One writer and multiple readers , and in the Templates section, select Production . The selections so far will instruct AWS to create an Aurora PostgreSQL database cluster with the most recent version of the PostgreSQL 10.7 compatible engine in a highly available configuration with one writer and one reader database instance in the cluster. In the Settings section give your database cluster a recognizable identifier, such as labstack-cluster . Configure the name and password of the master database user, with the most elevated permissions in the database. We recommend to use the user name masteruser for consistency with subsequent labs and a password of your choosing. For simplicity ensure the check box Auto generate a password is not checked . For the DB Instance size choose the default DB Instance class \u201cdb.r5.large\u201d. For the Availability and durability choose the default option to deploy Multi-AZ deployment by choosing \u201cCreate an Aurora Replica/Reader node in a different AZ (recommended for scaled availability)\u201d In the Connectivity section, expand the sub-section called Additional connectivity configuration . This section allows you to specify where the database cluster will be deployed within your defined network configuration. To simplify the labs, the CloudFormation stack you deployed in the preceding Prerequisites module, has configured a VPC that includes all resources needed for an Aurora database cluster. This includes the VPC itself, subnets, DB subnet groups, security groups and several other networking constructs. All you need to do is select the appropriate existing connectivity controls in this section. Pick the Virtual Private Cloud (VPC) named after the CloudFormation stack name, such as labstack-vpc . Similarly make sure the selected Subnet Group also matches the stack name (e.g. labstack-dbsubnets-[hash] ). Make sure the cluster Publicly accessible option is set to No . The lab environment also configured a VPC security group that allows your lab workspace EC2 instance to connect to the database. Make sure the Choose existing security group option is selected and from the dropdown pick the security group with a name ending in -pgsql-internal (eg. labstack-pgsql-internal ). Please remove any other security groups, such as default from the selection. For Database authentication choose the default option to use \u201cPassword authentication\u201d Next, expand the Additional configuration section. Set the Initial database name to mylab . For the DB cluster parameter group and DB parameter group selectors, choose the groups with the stack name in their name (e.g. labstack-[...] ). Choose a 7 days Backup retention period . Check the box to Enable encryption and select the [default] aws/rds for the Master key . Check the box to Enable Performance Insights with a Retention period of Default (7 days) and use the [default] aws/rds Master key for monitoring data encryption. Next, check the Enable Enhanced Monitoring box, and select a Granularity of 1 second . For Log exports check the Postgresql log** boxes. Also in the Advanced configuration section, de-select the check box Enable delete protection . In a production use case, you will want to leave that option checked, but for testing purposes, un-checking this option will make it easier to clean up the resources once you have completed the labs. Before continuing, let's summarize the configuration options selected. You will create a database cluster with the following characteristics: Aurora PostgreSQL 10.7 compatible (latest stable engine version) Cluster composed of a writer and a reader DB instance in different availability zones (highly available) Deployed in the VPC and using the network configuration of the lab environment Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption With Enhanced Monitoring and Performance Insights enabled Click Create database to provision the DB cluster.","title":"1. Creating the DB cluster"},{"location":"modules/create/#2-retrieving-the-db-cluster-endpoints","text":"The database cluster may take several minutes to provision, including the DB instances making up the cluster. In order to connect to the DB cluster and start using it in subsequent labs, you need to retrieve the DB cluster endpoints. There are two endpoints created by default. The Cluster Endpoint will always point to the writer DB instance of the cluster, and should be used for both writes and reads. The Reader Endpoint will always resolve to one of the reader DB instances and should be used to offload read operations to read replicas. In the RDS console, go to the DB cluster detail view by clicking on the cluster DB identifier. The Endpoints section in the Connectivity and security tab of the details page displays the endpoints. Note these values down, as you will use them later.","title":"2. Retrieving the DB cluster endpoints"},{"location":"modules/create/#3-assigning-an-iam-role-to-the-db-cluster","text":"Once created, you should assign an IAM role to the DB cluster, in order to allow the cluster access to Amazon S3 for importing and exporting data. The IAM role has already been created using CloudFormation when you created the lab environment. On the same DB cluster detail page as before, in the Manage IAM roles section, choose the IAM role named after the stack name, ending in -integrate-[region] (e.g. labstack-integrate-[region] ). Then click Add role . Once the operation completes the Status of the role will change from Pending to Active .","title":"3. Assigning an IAM role to the DB cluster"},{"location":"modules/create/#4-creating-a-replica-auto-scaling-policy","text":"Finally, you will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load. In the top right corner of the details page, click on Actions and then on Add replica auto scaling . Provide a Policy name based on the stack name, such as labstack-autoscale-readers . For the Target metric choose Average CPU utilization of Aurora Replicas . Enter a Target value of 20 percent. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes. Next, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs. In the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2 . In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost of associated with the labs we limit the number of readers to two. Next click Add policy .","title":"4. Creating a replica auto scaling policy"},{"location":"modules/das/","text":"Database Activity Streaming \u00b6 1. Prerequisite \u00b6 Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. 1.1 Create an EC2 Key Pair : \u00b6 You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file 1.2 Create KMS Key : \u00b6 Database Activity Streaming requires the Master Key to encrypt the key that in turn encrypts the database activity logged. The Default AWS RDS KMS key can\u2019t be used as the Master key. Therefore, we need to create a new customer managed KMS key to configure the Database Activity Streaming. On the AWS console search for KMS and click on Key Management Service. Select the Customer Managed Keys on the left-hand side and click on Create Key On the next screen under Configure key choose Symmetric key type and click Next On the next screen under Add Labels give the name for the key under the field Alias such as cmk-apg-lab Under Description field type a description for the key such as Customer managed Key for Aurora PostgreSQL Database Activity Streaming (DAS) lab and click Next On the next screen under the Key Administrators search for the account name (IAM user) you are using to connect to the AWS console. The IAM user name can be seen on the top right-hand side of the console. In this case the IAM user name is TeamRole. Check the box with the IAM user and click Next. On the next screen under the Define Key usage permissions search for the account name (IAM user) you are using to connect to the AWS console. The IAM user name can be seen on the top right-hand side of the console. In this case the IAM user name is TeamRole. Check the box with the IAM user and click Next. On the next screen review the policy and click Finish Verify the newly created KMS key on the KMS dashboard 2. Creating Aurora PostgreSQL cluster with Cloudformation \u00b6 Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013 2.1 Retrieving Database credentials from Secret Manager \u00b6 Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database. 2.2 Cloudformation Resource chart \u00b6 Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d 2.3 Connecting to the EC2 bastion Instance \u00b6 We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here . 3 Database Activity Streams \u00b6 Database Activity Streams provide a near real-time data stream of the database activity in your relational database. When you integrate Database Activity Streams with third-party monitoring tools, you can monitor and audit database activity. Database Activity Streams have the following limits and requirements: Currently, these streams are supported only with Aurora with PostgreSQL compatibility version 2.3, which is compatible with PostgreSQL version 10.7. They require use of AWS Key Management Service (AWS KMS) because the activity streams are always encrypted. 3.1 Configuring Database Activity Streams \u00b6 You start an activity stream at the DB cluster level to monitor database activity for all DB instances of the cluster. Any DB instances added to the cluster are also automatically monitored. You can choose to have the database session handle database activity events either synchronously or asynchronously: Synchronous mode \u2013 In synchronous mode, when a database session generates an activity stream event, the session blocks until the event is made durable. If the event can't be made durable for some reason, the database session returns to normal activities. However, an RDS event is sent indicating that activity stream records might be lost for some time. A second RDS event is sent after the system is back to a healthy state. The synchronous mode favors the accuracy of the activity stream over database performance. Asynchronous mode \u2013 In asynchronous mode, when a database session generates an activity stream event, the session returns to normal activities immediately. In the background, the activity stream event is made a durable record. If an error occurs in the background task, an RDS event is sent. This event indicates the beginning and end of any time windows where activity stream event records might have been lost. Asynchronous mode favors database performance over the accuracy of the activity stream. 3.2 To start an activity stream \u00b6 Open the Amazon RDS service console . In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. For Actions, choose Start activity stream. The Database Activity Stream window appears. Enter the following settings in the Database Activity Stream window: a. For Master key, choose a key from the list of AWS KMS keys. b. For Database activity stream mode, choose Asynchronous or Synchronous. c. Choose Apply immediately. When you're done entering settings, choose Continue. The status column on the RDS-> Database page for the cluster will start showing configuring-activity-stream Verify the activity streaming by clicking on the cluster name (with role =regional) and click on configuration. 3.3 Accessing Database Activity Streams \u00b6 We are generating load on the Database via pgbench and will access the database activity in real time. We will connect to the EC2 Instance to generate the load on the Database and will access the Database activity streaming information from another session by execution the python script (das_qpm.py) . We will be formatting the output of the 1 streaming record using JSON formatter. export PATH=/home/ec2-user/postgresql-10.7/src/bin/pgbench:$PATH cd /home/ec2-user/postgresql-10.7/src/bin/pgbench ./pgbench -i --fillfactor=90 --scale=100 --host=labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com --username=masteruser mylab ./pgbench --host=labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com --username=masteruser --protocol=prepared -P 60 --time=300 --client=16 --jobs=96 mylab> results1.log 3.4 Sample code Database Activity Streams \u00b6 The python file with the sample code is already created by the CloudFormation script at this location /home/ec2-user/das-script.py . You will be required to replace the value for RESOURCE_ID with the Resourceid value from your cluster configuration as shown below and also replace the value for STREAM_NAME with the Kinesis Stream . You can also copy and paste the script text as shown below to create a new python file and replace the value for RESOURCE_ID with the Resourceid value from your cluster configuration as shown below and also replace the value for STREAM_NAME with the Kinesis Stream . vi das_script.py import zlib import boto3 import base64 import json import aws_encryption_sdk from Crypto.Cipher import AES from aws_encryption_sdk import DefaultCryptoMaterialsManager from aws_encryption_sdk.internal.crypto import WrappingKey from aws_encryption_sdk.key_providers.raw import RawMasterKeyProvider from aws_encryption_sdk.identifiers import WrappingAlgorithm , EncryptionKeyType REGION_NAME = 'us-west-2' RESOURCE_ID = 'cluster-XXXXXXXXXXX' # cluster-ABCD123456 STREAM_NAME = 'aws-rds-das-cluster-XXXXXXXXXXX' # aws-rds-das-cluster-ABCD123456 class MyRawMasterKeyProvider ( RawMasterKeyProvider ): provider_id = \"BC\" def __new__ ( cls , * args , ** kwargs ): obj = super ( RawMasterKeyProvider , cls ) . __new__ ( cls ) return obj def __init__ ( self , plain_key ): RawMasterKeyProvider . __init__ ( self ) self . wrapping_key = WrappingKey ( wrapping_algorithm = WrappingAlgorithm . AES_256_GCM_IV12_TAG16_NO_PADDING , wrapping_key = plain_key , wrapping_key_type = EncryptionKeyType . SYMMETRIC ) def _get_raw_key ( self , key_id ): return self . wrapping_key def decrypt_payload ( payload , data_key ): my_key_provider = MyRawMasterKeyProvider ( data_key ) my_key_provider . add_master_key ( \"DataKey\" ) decrypted_plaintext , header = aws_encryption_sdk . decrypt ( source = payload , materials_manager = aws_encryption_sdk . DefaultCryptoMaterialsManager ( master_key_provider = my_key_provider )) return decrypted_plaintext def decrypt_decompress ( payload , key ): decrypted = decrypt_payload ( payload , key ) return zlib . decompress ( decrypted , zlib . MAX_WBITS + 1 ) def main (): session = boto3 . session . Session () kms = session . client ( 'kms' , region_name = REGION_NAME ) kinesis = session . client ( 'kinesis' , region_name = REGION_NAME ) response = kinesis . describe_stream ( StreamName = STREAM_NAME ) shard_iters = [] for shard in response [ 'StreamDescription' ][ 'Shards' ]: shard_iter_response = kinesis . get_shard_iterator ( StreamName = STREAM_NAME , ShardId = shard [ 'ShardId' ], ShardIteratorType = 'LATEST' ) # TRIM_HORIZON will shard_iters . append ( shard_iter_response [ 'ShardIterator' ]) while len ( shard_iters ) > 0 : next_shard_iters = [] for shard_iter in shard_iters : response = kinesis . get_records ( ShardIterator = shard_iter , Limit = 10000 ) for record in response [ 'Records' ]: record_data = record [ 'Data' ] record_data = json . loads ( record_data ) payload_decoded = base64 . b64decode ( record_data [ 'databaseActivityEvents' ]) data_key_decoded = base64 . b64decode ( record_data [ 'key' ]) data_key_decrypt_result = kms . decrypt ( CiphertextBlob = data_key_decoded , EncryptionContext = { 'aws:rds:dbc-id' : RESOURCE_ID }) print decrypt_decompress ( payload_decoded , data_key_decrypt_result [ 'Plaintext' ]) if 'NextShardIterator' in response : next_shard_iters . append ( response [ 'NextShardIterator' ]) shard_iters = next_shard_iters if __name__ == '__main__' : main () 3.5 Sample Output Activity Streaming \u00b6 jsonformatter.org { \"type\": \"DatabaseActivityMonitoringRecord\", \"clusterId\": \"cluster-XXXXXXXXXXXXXXXXXXXXXXXXXX\", \"instanceId\": \"db-XXXXXXXXXXXXXXXXXXXXXXX\", \"databaseActivityEventList\": [ { \"logTime\": \"2019-12-26 06:56:09.090054+00\", \"statementId\": 3731, \"substatementId\": 1, \"objectType\": \"TABLE\", \"command\": \"INSERT\", \"objectName\": \"public.pgbench_history\", \"databaseName\": \"mylab\", \"dbUserName\": \"masteruser\", \"remoteHost\": \"10.0.0.204\", \"remotePort\": \"33948\", \"sessionId\": \"5e04596c.2383\", \"rowCount\": 1, \"commandText\": \"INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP);\", \"paramList\": [ \"424\", \"62\", \"1788678\", \"-2770\" ], \"pid\": 9091, \"clientApplication\": \"pgbench\", \"exitCode\": null, \"class\": \"WRITE\", \"serverVersion\": \"2.3.5\", \"serverType\": \"PostgreSQL\", \"serviceName\": \"Amazon Aurora PostgreSQL-Compatible edition\", \"serverHost\": \"10.0.12.39\", \"netProtocol\": \"TCP\", \"dbProtocol\": \"Postgres 3.0\", \"type\": \"record\" } ] } 3.6 Stop the Database Activity Streaming \u00b6 In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. Click on \u201cActions\u201d and select \u201cstop activity stream\u201d and click \u201ccontinue to stop Database activity streaming on the cluster. The status column on the RDS Database home page for the cluster will start showing \u201cconfiguring-activity-stream\u201d","title":"Database Activity Streaming"},{"location":"modules/das/#database-activity-streaming","text":"","title":"Database Activity Streaming"},{"location":"modules/das/#1-prerequisite","text":"Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab.","title":"1. Prerequisite"},{"location":"modules/das/#11-create-an-ec2-key-pair","text":"You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file","title":"1.1 Create an EC2 Key Pair: "},{"location":"modules/das/#12-create-kms-key","text":"Database Activity Streaming requires the Master Key to encrypt the key that in turn encrypts the database activity logged. The Default AWS RDS KMS key can\u2019t be used as the Master key. Therefore, we need to create a new customer managed KMS key to configure the Database Activity Streaming. On the AWS console search for KMS and click on Key Management Service. Select the Customer Managed Keys on the left-hand side and click on Create Key On the next screen under Configure key choose Symmetric key type and click Next On the next screen under Add Labels give the name for the key under the field Alias such as cmk-apg-lab Under Description field type a description for the key such as Customer managed Key for Aurora PostgreSQL Database Activity Streaming (DAS) lab and click Next On the next screen under the Key Administrators search for the account name (IAM user) you are using to connect to the AWS console. The IAM user name can be seen on the top right-hand side of the console. In this case the IAM user name is TeamRole. Check the box with the IAM user and click Next. On the next screen under the Define Key usage permissions search for the account name (IAM user) you are using to connect to the AWS console. The IAM user name can be seen on the top right-hand side of the console. In this case the IAM user name is TeamRole. Check the box with the IAM user and click Next. On the next screen review the policy and click Finish Verify the newly created KMS key on the KMS dashboard","title":"1.2  Create KMS Key: "},{"location":"modules/das/#2-creating-aurora-postgresql-cluster-with-cloudformation","text":"Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013","title":"2. Creating Aurora PostgreSQL cluster with Cloudformation"},{"location":"modules/das/#21-retrieving-database-credentials-from-secret-manager","text":"Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database.","title":"2.1 Retrieving Database credentials from Secret Manager"},{"location":"modules/das/#22-cloudformation-resource-chart","text":"Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d","title":"2.2 Cloudformation Resource chart"},{"location":"modules/das/#23-connecting-to-the-ec2-bastion-instance","text":"We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here .","title":"2.3 Connecting to the EC2 bastion Instance"},{"location":"modules/das/#3-database-activity-streams","text":"Database Activity Streams provide a near real-time data stream of the database activity in your relational database. When you integrate Database Activity Streams with third-party monitoring tools, you can monitor and audit database activity. Database Activity Streams have the following limits and requirements: Currently, these streams are supported only with Aurora with PostgreSQL compatibility version 2.3, which is compatible with PostgreSQL version 10.7. They require use of AWS Key Management Service (AWS KMS) because the activity streams are always encrypted.","title":"3 Database Activity Streams"},{"location":"modules/das/#31-configuring-database-activity-streams","text":"You start an activity stream at the DB cluster level to monitor database activity for all DB instances of the cluster. Any DB instances added to the cluster are also automatically monitored. You can choose to have the database session handle database activity events either synchronously or asynchronously: Synchronous mode \u2013 In synchronous mode, when a database session generates an activity stream event, the session blocks until the event is made durable. If the event can't be made durable for some reason, the database session returns to normal activities. However, an RDS event is sent indicating that activity stream records might be lost for some time. A second RDS event is sent after the system is back to a healthy state. The synchronous mode favors the accuracy of the activity stream over database performance. Asynchronous mode \u2013 In asynchronous mode, when a database session generates an activity stream event, the session returns to normal activities immediately. In the background, the activity stream event is made a durable record. If an error occurs in the background task, an RDS event is sent. This event indicates the beginning and end of any time windows where activity stream event records might have been lost. Asynchronous mode favors database performance over the accuracy of the activity stream.","title":"3.1 Configuring Database Activity Streams"},{"location":"modules/das/#32-to-start-an-activity-stream","text":"Open the Amazon RDS service console . In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. For Actions, choose Start activity stream. The Database Activity Stream window appears. Enter the following settings in the Database Activity Stream window: a. For Master key, choose a key from the list of AWS KMS keys. b. For Database activity stream mode, choose Asynchronous or Synchronous. c. Choose Apply immediately. When you're done entering settings, choose Continue. The status column on the RDS-> Database page for the cluster will start showing configuring-activity-stream Verify the activity streaming by clicking on the cluster name (with role =regional) and click on configuration.","title":"3.2 To start an activity stream"},{"location":"modules/das/#33-accessing-database-activity-streams","text":"We are generating load on the Database via pgbench and will access the database activity in real time. We will connect to the EC2 Instance to generate the load on the Database and will access the Database activity streaming information from another session by execution the python script (das_qpm.py) . We will be formatting the output of the 1 streaming record using JSON formatter. export PATH=/home/ec2-user/postgresql-10.7/src/bin/pgbench:$PATH cd /home/ec2-user/postgresql-10.7/src/bin/pgbench ./pgbench -i --fillfactor=90 --scale=100 --host=labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com --username=masteruser mylab ./pgbench --host=labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com --username=masteruser --protocol=prepared -P 60 --time=300 --client=16 --jobs=96 mylab> results1.log","title":"3.3 Accessing Database Activity Streams"},{"location":"modules/das/#34-sample-code-database-activity-streams","text":"The python file with the sample code is already created by the CloudFormation script at this location /home/ec2-user/das-script.py . You will be required to replace the value for RESOURCE_ID with the Resourceid value from your cluster configuration as shown below and also replace the value for STREAM_NAME with the Kinesis Stream . You can also copy and paste the script text as shown below to create a new python file and replace the value for RESOURCE_ID with the Resourceid value from your cluster configuration as shown below and also replace the value for STREAM_NAME with the Kinesis Stream . vi das_script.py import zlib import boto3 import base64 import json import aws_encryption_sdk from Crypto.Cipher import AES from aws_encryption_sdk import DefaultCryptoMaterialsManager from aws_encryption_sdk.internal.crypto import WrappingKey from aws_encryption_sdk.key_providers.raw import RawMasterKeyProvider from aws_encryption_sdk.identifiers import WrappingAlgorithm , EncryptionKeyType REGION_NAME = 'us-west-2' RESOURCE_ID = 'cluster-XXXXXXXXXXX' # cluster-ABCD123456 STREAM_NAME = 'aws-rds-das-cluster-XXXXXXXXXXX' # aws-rds-das-cluster-ABCD123456 class MyRawMasterKeyProvider ( RawMasterKeyProvider ): provider_id = \"BC\" def __new__ ( cls , * args , ** kwargs ): obj = super ( RawMasterKeyProvider , cls ) . __new__ ( cls ) return obj def __init__ ( self , plain_key ): RawMasterKeyProvider . __init__ ( self ) self . wrapping_key = WrappingKey ( wrapping_algorithm = WrappingAlgorithm . AES_256_GCM_IV12_TAG16_NO_PADDING , wrapping_key = plain_key , wrapping_key_type = EncryptionKeyType . SYMMETRIC ) def _get_raw_key ( self , key_id ): return self . wrapping_key def decrypt_payload ( payload , data_key ): my_key_provider = MyRawMasterKeyProvider ( data_key ) my_key_provider . add_master_key ( \"DataKey\" ) decrypted_plaintext , header = aws_encryption_sdk . decrypt ( source = payload , materials_manager = aws_encryption_sdk . DefaultCryptoMaterialsManager ( master_key_provider = my_key_provider )) return decrypted_plaintext def decrypt_decompress ( payload , key ): decrypted = decrypt_payload ( payload , key ) return zlib . decompress ( decrypted , zlib . MAX_WBITS + 1 ) def main (): session = boto3 . session . Session () kms = session . client ( 'kms' , region_name = REGION_NAME ) kinesis = session . client ( 'kinesis' , region_name = REGION_NAME ) response = kinesis . describe_stream ( StreamName = STREAM_NAME ) shard_iters = [] for shard in response [ 'StreamDescription' ][ 'Shards' ]: shard_iter_response = kinesis . get_shard_iterator ( StreamName = STREAM_NAME , ShardId = shard [ 'ShardId' ], ShardIteratorType = 'LATEST' ) # TRIM_HORIZON will shard_iters . append ( shard_iter_response [ 'ShardIterator' ]) while len ( shard_iters ) > 0 : next_shard_iters = [] for shard_iter in shard_iters : response = kinesis . get_records ( ShardIterator = shard_iter , Limit = 10000 ) for record in response [ 'Records' ]: record_data = record [ 'Data' ] record_data = json . loads ( record_data ) payload_decoded = base64 . b64decode ( record_data [ 'databaseActivityEvents' ]) data_key_decoded = base64 . b64decode ( record_data [ 'key' ]) data_key_decrypt_result = kms . decrypt ( CiphertextBlob = data_key_decoded , EncryptionContext = { 'aws:rds:dbc-id' : RESOURCE_ID }) print decrypt_decompress ( payload_decoded , data_key_decrypt_result [ 'Plaintext' ]) if 'NextShardIterator' in response : next_shard_iters . append ( response [ 'NextShardIterator' ]) shard_iters = next_shard_iters if __name__ == '__main__' : main ()","title":"3.4 Sample code Database Activity Streams"},{"location":"modules/das/#35-sample-output-activity-streaming","text":"jsonformatter.org { \"type\": \"DatabaseActivityMonitoringRecord\", \"clusterId\": \"cluster-XXXXXXXXXXXXXXXXXXXXXXXXXX\", \"instanceId\": \"db-XXXXXXXXXXXXXXXXXXXXXXX\", \"databaseActivityEventList\": [ { \"logTime\": \"2019-12-26 06:56:09.090054+00\", \"statementId\": 3731, \"substatementId\": 1, \"objectType\": \"TABLE\", \"command\": \"INSERT\", \"objectName\": \"public.pgbench_history\", \"databaseName\": \"mylab\", \"dbUserName\": \"masteruser\", \"remoteHost\": \"10.0.0.204\", \"remotePort\": \"33948\", \"sessionId\": \"5e04596c.2383\", \"rowCount\": 1, \"commandText\": \"INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP);\", \"paramList\": [ \"424\", \"62\", \"1788678\", \"-2770\" ], \"pid\": 9091, \"clientApplication\": \"pgbench\", \"exitCode\": null, \"class\": \"WRITE\", \"serverVersion\": \"2.3.5\", \"serverType\": \"PostgreSQL\", \"serviceName\": \"Amazon Aurora PostgreSQL-Compatible edition\", \"serverHost\": \"10.0.12.39\", \"netProtocol\": \"TCP\", \"dbProtocol\": \"Postgres 3.0\", \"type\": \"record\" } ] }","title":"3.5 Sample Output Activity Streaming"},{"location":"modules/das/#36-stop-the-database-activity-streaming","text":"In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. Click on \u201cActions\u201d and select \u201cstop activity stream\u201d and click \u201ccontinue to stop Database activity streaming on the cluster. The status column on the RDS Database home page for the cluster will start showing \u201cconfiguring-activity-stream\u201d","title":"3.6 Stop the Database Activity Streaming"},{"location":"modules/fastclone/","text":"Fast Cloning \u00b6 1. Prerequisite \u00b6 Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file 2. Creating Aurora PostgreSQL cluster with Cloudformation \u00b6 Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013 2.1 Retrieving Database credentials from Secret Manager \u00b6 Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database. 2.2 Cloudformation Resource chart \u00b6 Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d 2.3 Connecting to the EC2 bastion Instance \u00b6 We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here . 3. Setting up the Fast Clone divergence test \u00b6 The CloudFormation script used to spin up the stack automatically creates following objects in the Database \"mylab\" Resource name Value cloneeventtest Table to store the counter and the timestamp statusflag Table to store the status flag which controls the start/stop counter eventerrormsg Table to store error messages cloneeventproc Function to add data to the cloneeventtest table based on the start counter flag In case you need to create these objects manually then you can connect to the Database using psql and run the below commands ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> CREATE TABLE cloneeventtest( mylab(> counter smallint, mylab(> seconds_elapsed int, mylab(> data TIMESTAMPTZ mylab(> ) mylab-> WITH ( mylab(> OIDS=FALSE mylab(> ); CREATE TABLE mylab=> CREATE TABLE IF NOT EXISTS statusflag( mylab(> delflag CHARACTER VARYING(2) mylab(> ) mylab-> WITH ( mylab(> OIDS=FALSE mylab(> ); CREATE TABLE mylab=> CREATE TABLE IF NOT EXISTS eventerrormsg( mylab(> err_msg CHARACTER VARYING(2000), mylab(> time TIMESTAMPTZ mylab(> ) mylab-> WITH ( mylab(> OIDS=FALSE mylab(> ); CREATE TABLE mylab=> INSERT INTO statusflag VALUES('N'); INSERT 0 1 mylab=> CREATE OR REPLACE FUNCTION cloneeventproc() mylab-> RETURNS void mylab-> AS mylab-> $BODY$ mylab$> DECLARE mylab$> l_flag CHARACTER VARYING(2); mylab$> l_cntr smallint := 1; mylab$> l_sec int := 60; mylab$> l_current_time TIMESTAMPTZ :=now(); mylab$> l_errmsg CHARACTER VARYING(350); mylab$> BEGIN mylab$> WHILE TRUE LOOP mylab$> SELECT mylab$> delflag mylab$> INTO STRICT l_flag mylab$> FROM statusflag; mylab$> mylab$> IF l_flag = 'Y' THEN mylab$> EXIT; mylab$> END IF; mylab$> INSERT INTO cloneeventtest mylab$> VALUES (l_cntr, l_sec, l_current_time); mylab$> l_cntr := l_cntr + 1; mylab$> l_sec := l_sec + 60; mylab$> l_current_time=l_current_time + (1 ||' minutes')::interval; mylab$> PERFORM pg_sleep(60); mylab$> mylab$> END LOOP; mylab$> EXCEPTION mylab$> WHEN others THEN mylab$> INSERT INTO eventerrormsg mylab$> VALUES (l_errmsg, now()); mylab$> commit; mylab$> END; mylab$> $BODY$ mylab-> LANGUAGE plpgsql; CREATE FUNCTION 4. Creating and verifying performance impact of the Fast Clone \u00b6 To verify the performance impact of the clone cluster on the primary source cluster we will be performing follow steps a. Start the pgbench workload (a PostgreSQL benchmarking tool) to generate a synthetic workload and run benchmarking on the primary cluster for 30 minutes. b. Execute the function \u201ccloneeventproc\u201d to start adding sample data on the source cluster. c. After 5 minutes or so kick off fast clone cluster creation. The pgbench workload will continue to execute. d. The clone cluster should be ready after about 15 minutes or so. Run the same pgbench workload on the clone cluster as the one running on the primary cluster (step #a) e. Verify the pgbench Transaction per seconds (TPS) metrics on the primary and the clone cluster. f. Verify the output from the sample table \u201ccloneeventtest\u201d on both the primary and the clone cluster to see the data divergence. 4.1. Running the pgbench workload on the primary cluster \u00b6 Before creating a Fast Clone of the primary cluster, we are going to start pgbench test to measure the Transaction per seconds (TPS) metrics on the primary cluster before the clone creation. pgbench -- progress - timestamp - M prepared - n - T 1800 - P 60 - c 8 - j 8 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > Primary_results . log 4.2 Verify the environment and run the sample divergence test \u00b6 In order to verify the data divergence on the primary and the clone cluster we will be adding sample data using sample tables and the function created above in step#3. Verify the status flag is set to \u2018N\u2019 and there is no data in the table \u201ccloneeventtest\u201d. Execute the function cloneeventproc() to start adding sample data. This function will add a row to the table \u201ccloneeventtest\u201d every 60 seconds. psql -h labstack-cluster.cluster-cwkjlhbdms9s.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> select * from statusflag; delflag --------- N (1 row) mylab=> select * from cloneeventtest; counter | seconds_elapsed | data ---------+-----------------+------ (0 rows) mylab=> select cloneeventproc(); cloneeventproc ---------------- At this time (we call as time \u201cT1\u201d) the pgbench workload is running on the source DB cluster and also, we are adding sample data to the table on the primary cluster every 60 seconds. 4.3. Stop the sample data generation \u00b6 At time (After 5 minutes or so kick off fast clone cluster creation. T1+5 minutes we will stop the function execution by manually resetting the \u201cdelflag\u201d column on the table \u201cstatusflag\u201d to \u201cY\u201d from another session (different than the session where we executed the function). The pgbench workload will continue to execute on the primary source cluster From session #2 [ec2-user@ip-10-0-0-204 ~]$ psql -h labstack-cluster.cluster-xxxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> update statusflag set delflag='Y'; UPDATE 1 Back to session #1 mylab=> select cloneeventproc(); cloneeventproc ---------------- (1 row) mylab=> select count(*) from cloneeventtest; count ------- 5 (1 row) mylab=> show timezone; TimeZone ---------- UTC (1 row) mylab=> SET timezone = 'America/Los_Angeles'; SET mylab=> select * from cloneeventtest; counter | seconds_elapsed | data ---------+-----------------+------------------------------- 1 | 60 | 2020-01-20 13:57:13.473963-08 2 | 120 | 2020-01-20 13:58:13.473963-08 3 | 180 | 2020-01-20 13:59:13.473963-08 4 | 240 | 2020-01-20 14:00:13.473963-08 5 | 300 | 2020-01-20 14:01:13.473963-08 (5 rows) 4.4. Create Fast Clone Cluster \u00b6 Once the function execution is stopped (after time T1+5 minutes) we will start creating the Fast Clone of the primary cluster. The pgbench workload on the primary will continue on the primary cluster. will walk you through the process of cloning a DB cluster . Cloning creates a separate, independent DB cluster, with a consistent copy of your data set as of the time you cloned it. Database cloning uses a copy-on-write protocol, in which data is copied at the time that data changes, either on the source databases or the clone databases. The two clusters are isolated, and there is no performance impact on the source DB cluster from database operations on the clone, or the other way around. Following are the steps to configure the Database Fast clone on your Aurora PostgreSQL cluster a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. Click on the selected cluster above and look under the section c. Please refer the below table on the input parameters required on next screens Resource name Value DB Engine Use default Database Instance class Use the same as the primary DB DB Instance Identifier Labstack-clone Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d VPC refer CloudFormation template output section and refer the key value \u201cvpcid\u201d Subnet Group Use default Public accessibility No Availability Zone Use default Security Group Use default Database Port Use default IAM DB authentication Disable Log Exports Leave it unchecked. Auto Minor version upgrade Yes d. Once you click on the \u201cCreate Clone\u201d the status column will show status as \u201cCreating\u201d. e. The clone cluster should be ready after about 10-15 minutes or so. The status column will show as \u201cAvailable\u201d 4.5. Start the sample data divergence process on the primary cluster \u00b6 Once the Clone cluster creation process is kicked off, we will start the sample data generation process on the primary cluster. Any sample data added from this point onwards should only be available on the primary cluster and not on the clone cluster. psql -h labstack-cluster.cluster-xxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> truncate cloneeventtest; TRUNCATE TABLE mylab=> update statusflag set delflag='N'; UPDATE 1 mylab=> mylab=> select count(*) from cloneeventtest; count ------- 0 (1 row) mylab=> select cloneeventproc(); cloneeventproc ---------------- 4.6. Verify the data divergence on the Clone Cluster \u00b6 clone cluster should be ready after about 15 minutes or so (time T1+~10-15 minutes). To verify the data on the clone cluster, as we started creating the clone cluster approximately at time ~T1+5 minutes. The table \u201ccloneeventtest\u201d on the clone cluster should have the data point in time to the time when we spin up the clone cluster. From session #2 [ec2-user@ip-10-0-0-204 ~]$ psql -h labstack-clone-cluster.cluster-xxxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> select count(*) from cloneeventtest; count ------- 5 (1 row) mylab=> show timezone; TimeZone ---------- UTC (1 row) mylab=> SET timezone = 'America/Los_Angeles'; SET mylab=> select * from cloneeventtest; counter | seconds_elapsed | data ---------+-----------------+------------------------------- 1 | 60 | 2020-01-20 13:57:13.473963-08 2 | 120 | 2020-01-20 13:58:13.473963-08 3 | 180 | 2020-01-20 13:59:13.473963-08 4 | 240 | 2020-01-20 14:00:13.473963-08 5 | 300 | 2020-01-20 14:01:13.473963-08 (5 rows) 4.7. Run the pgbench workload on the Clone Cluster \u00b6 We are going to start the same pgbench workload as we did on the primary cluster in step# 4.1. pgbench -- progress - timestamp - M prepared - n - T 1800 - P 60 - c 8 - j 8 -- host = labstack - clone - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > clone_results . log 4.8. Verify the pgbench metrics on the primary and the clone cluster. \u00b6 Once the pgbench workload completed on both the primary and the clone cluster we can verify the TPS metrics from both the cluster by looking at the output file. [ec2-user@ip-xxxxxxxx ~]$ more Primary_results.log transaction type: multiple scripts scaling factor: 100 query mode: prepared number of clients: 8 number of threads: 8 duration: 1800 s number of transactions actually processed: 19494742 latency average = 0.739 ms latency stddev = 1.361 ms tps = 10830.345301 (including connections establishing) tps = 10830.428982 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 928533 transactions (4.8% of total, tps = 515.848479) - latency average = 6.228 ms - latency stddev = 2.127 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 18561932 transactions (95.2% of total, tps = 10312.120725) - latency average = 0.464 ms - latency stddev = 0.371 ms [ec2-user@ip-10-0-0-204 ~]$ more clone_results.log transaction type: multiple scripts scaling factor: 100 query mode: prepared number of clients: 8 number of threads: 8 duration: 1800 s number of transactions actually processed: 19494742 latency average = 0.739 ms latency stddev = 1.361 ms tps = 10830.345301 (including connections establishing) tps = 10830.428982 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 928533 transactions (4.8% of total, tps = 515.848479) - latency average = 6.228 ms - latency stddev = 2.127 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 18561932 transactions (95.2% of total, tps = 10312.120725) - latency average = 0.464 ms - latency stddev = 0.371 ms 4.9. Sample graph on the fast clone performance impact \u00b6 Below is the sample comparison slide to show the performance impact of the clone","title":"Fast Cloning"},{"location":"modules/fastclone/#fast-cloning","text":"","title":"Fast Cloning"},{"location":"modules/fastclone/#1-prerequisite","text":"Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file","title":"1. Prerequisite"},{"location":"modules/fastclone/#2-creating-aurora-postgresql-cluster-with-cloudformation","text":"Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013","title":"2. Creating Aurora PostgreSQL cluster with Cloudformation"},{"location":"modules/fastclone/#21-retrieving-database-credentials-from-secret-manager","text":"Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database.","title":"2.1 Retrieving Database credentials from Secret Manager"},{"location":"modules/fastclone/#22-cloudformation-resource-chart","text":"Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d","title":"2.2 Cloudformation Resource chart"},{"location":"modules/fastclone/#23-connecting-to-the-ec2-bastion-instance","text":"We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here .","title":"2.3 Connecting to the EC2 bastion Instance"},{"location":"modules/fastclone/#3-setting-up-the-fast-clone-divergence-test","text":"The CloudFormation script used to spin up the stack automatically creates following objects in the Database \"mylab\" Resource name Value cloneeventtest Table to store the counter and the timestamp statusflag Table to store the status flag which controls the start/stop counter eventerrormsg Table to store error messages cloneeventproc Function to add data to the cloneeventtest table based on the start counter flag In case you need to create these objects manually then you can connect to the Database using psql and run the below commands ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> CREATE TABLE cloneeventtest( mylab(> counter smallint, mylab(> seconds_elapsed int, mylab(> data TIMESTAMPTZ mylab(> ) mylab-> WITH ( mylab(> OIDS=FALSE mylab(> ); CREATE TABLE mylab=> CREATE TABLE IF NOT EXISTS statusflag( mylab(> delflag CHARACTER VARYING(2) mylab(> ) mylab-> WITH ( mylab(> OIDS=FALSE mylab(> ); CREATE TABLE mylab=> CREATE TABLE IF NOT EXISTS eventerrormsg( mylab(> err_msg CHARACTER VARYING(2000), mylab(> time TIMESTAMPTZ mylab(> ) mylab-> WITH ( mylab(> OIDS=FALSE mylab(> ); CREATE TABLE mylab=> INSERT INTO statusflag VALUES('N'); INSERT 0 1 mylab=> CREATE OR REPLACE FUNCTION cloneeventproc() mylab-> RETURNS void mylab-> AS mylab-> $BODY$ mylab$> DECLARE mylab$> l_flag CHARACTER VARYING(2); mylab$> l_cntr smallint := 1; mylab$> l_sec int := 60; mylab$> l_current_time TIMESTAMPTZ :=now(); mylab$> l_errmsg CHARACTER VARYING(350); mylab$> BEGIN mylab$> WHILE TRUE LOOP mylab$> SELECT mylab$> delflag mylab$> INTO STRICT l_flag mylab$> FROM statusflag; mylab$> mylab$> IF l_flag = 'Y' THEN mylab$> EXIT; mylab$> END IF; mylab$> INSERT INTO cloneeventtest mylab$> VALUES (l_cntr, l_sec, l_current_time); mylab$> l_cntr := l_cntr + 1; mylab$> l_sec := l_sec + 60; mylab$> l_current_time=l_current_time + (1 ||' minutes')::interval; mylab$> PERFORM pg_sleep(60); mylab$> mylab$> END LOOP; mylab$> EXCEPTION mylab$> WHEN others THEN mylab$> INSERT INTO eventerrormsg mylab$> VALUES (l_errmsg, now()); mylab$> commit; mylab$> END; mylab$> $BODY$ mylab-> LANGUAGE plpgsql; CREATE FUNCTION","title":"3. Setting up the Fast Clone divergence test"},{"location":"modules/fastclone/#4-creating-and-verifying-performance-impact-of-the-fast-clone","text":"To verify the performance impact of the clone cluster on the primary source cluster we will be performing follow steps a. Start the pgbench workload (a PostgreSQL benchmarking tool) to generate a synthetic workload and run benchmarking on the primary cluster for 30 minutes. b. Execute the function \u201ccloneeventproc\u201d to start adding sample data on the source cluster. c. After 5 minutes or so kick off fast clone cluster creation. The pgbench workload will continue to execute. d. The clone cluster should be ready after about 15 minutes or so. Run the same pgbench workload on the clone cluster as the one running on the primary cluster (step #a) e. Verify the pgbench Transaction per seconds (TPS) metrics on the primary and the clone cluster. f. Verify the output from the sample table \u201ccloneeventtest\u201d on both the primary and the clone cluster to see the data divergence.","title":"4. Creating and verifying performance impact of the Fast Clone"},{"location":"modules/fastclone/#41-running-the-pgbench-workload-on-the-primary-cluster","text":"Before creating a Fast Clone of the primary cluster, we are going to start pgbench test to measure the Transaction per seconds (TPS) metrics on the primary cluster before the clone creation. pgbench -- progress - timestamp - M prepared - n - T 1800 - P 60 - c 8 - j 8 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > Primary_results . log","title":"4.1. Running the pgbench workload on the primary cluster"},{"location":"modules/fastclone/#42-verify-the-environment-and-run-the-sample-divergence-test","text":"In order to verify the data divergence on the primary and the clone cluster we will be adding sample data using sample tables and the function created above in step#3. Verify the status flag is set to \u2018N\u2019 and there is no data in the table \u201ccloneeventtest\u201d. Execute the function cloneeventproc() to start adding sample data. This function will add a row to the table \u201ccloneeventtest\u201d every 60 seconds. psql -h labstack-cluster.cluster-cwkjlhbdms9s.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> select * from statusflag; delflag --------- N (1 row) mylab=> select * from cloneeventtest; counter | seconds_elapsed | data ---------+-----------------+------ (0 rows) mylab=> select cloneeventproc(); cloneeventproc ---------------- At this time (we call as time \u201cT1\u201d) the pgbench workload is running on the source DB cluster and also, we are adding sample data to the table on the primary cluster every 60 seconds.","title":"4.2 Verify the environment and run the sample divergence test"},{"location":"modules/fastclone/#43-stop-the-sample-data-generation","text":"At time (After 5 minutes or so kick off fast clone cluster creation. T1+5 minutes we will stop the function execution by manually resetting the \u201cdelflag\u201d column on the table \u201cstatusflag\u201d to \u201cY\u201d from another session (different than the session where we executed the function). The pgbench workload will continue to execute on the primary source cluster From session #2 [ec2-user@ip-10-0-0-204 ~]$ psql -h labstack-cluster.cluster-xxxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> update statusflag set delflag='Y'; UPDATE 1 Back to session #1 mylab=> select cloneeventproc(); cloneeventproc ---------------- (1 row) mylab=> select count(*) from cloneeventtest; count ------- 5 (1 row) mylab=> show timezone; TimeZone ---------- UTC (1 row) mylab=> SET timezone = 'America/Los_Angeles'; SET mylab=> select * from cloneeventtest; counter | seconds_elapsed | data ---------+-----------------+------------------------------- 1 | 60 | 2020-01-20 13:57:13.473963-08 2 | 120 | 2020-01-20 13:58:13.473963-08 3 | 180 | 2020-01-20 13:59:13.473963-08 4 | 240 | 2020-01-20 14:00:13.473963-08 5 | 300 | 2020-01-20 14:01:13.473963-08 (5 rows)","title":"4.3. Stop the sample data generation"},{"location":"modules/fastclone/#44-create-fast-clone-cluster","text":"Once the function execution is stopped (after time T1+5 minutes) we will start creating the Fast Clone of the primary cluster. The pgbench workload on the primary will continue on the primary cluster. will walk you through the process of cloning a DB cluster . Cloning creates a separate, independent DB cluster, with a consistent copy of your data set as of the time you cloned it. Database cloning uses a copy-on-write protocol, in which data is copied at the time that data changes, either on the source databases or the clone databases. The two clusters are isolated, and there is no performance impact on the source DB cluster from database operations on the clone, or the other way around. Following are the steps to configure the Database Fast clone on your Aurora PostgreSQL cluster a. Sign in to the AWS Management Console and open the Amazon RDS console . b. In the navigation pane, choose Databases and click on the DB identifier with the cluster name you created as a part of the CloudFormation stack. Click on the selected cluster above and look under the section c. Please refer the below table on the input parameters required on next screens Resource name Value DB Engine Use default Database Instance class Use the same as the primary DB DB Instance Identifier Labstack-clone Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d VPC refer CloudFormation template output section and refer the key value \u201cvpcid\u201d Subnet Group Use default Public accessibility No Availability Zone Use default Security Group Use default Database Port Use default IAM DB authentication Disable Log Exports Leave it unchecked. Auto Minor version upgrade Yes d. Once you click on the \u201cCreate Clone\u201d the status column will show status as \u201cCreating\u201d. e. The clone cluster should be ready after about 10-15 minutes or so. The status column will show as \u201cAvailable\u201d","title":"4.4. Create Fast Clone Cluster"},{"location":"modules/fastclone/#45-start-the-sample-data-divergence-process-on-the-primary-cluster","text":"Once the Clone cluster creation process is kicked off, we will start the sample data generation process on the primary cluster. Any sample data added from this point onwards should only be available on the primary cluster and not on the clone cluster. psql -h labstack-cluster.cluster-xxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> truncate cloneeventtest; TRUNCATE TABLE mylab=> update statusflag set delflag='N'; UPDATE 1 mylab=> mylab=> select count(*) from cloneeventtest; count ------- 0 (1 row) mylab=> select cloneeventproc(); cloneeventproc ----------------","title":"4.5. Start the sample data divergence process on the primary cluster"},{"location":"modules/fastclone/#46-verify-the-data-divergence-on-the-clone-cluster","text":"clone cluster should be ready after about 15 minutes or so (time T1+~10-15 minutes). To verify the data on the clone cluster, as we started creating the clone cluster approximately at time ~T1+5 minutes. The table \u201ccloneeventtest\u201d on the clone cluster should have the data point in time to the time when we spin up the clone cluster. From session #2 [ec2-user@ip-10-0-0-204 ~]$ psql -h labstack-clone-cluster.cluster-xxxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab Password for user masteruser: psql (10.7) Type \"help\" for help. mylab=> select count(*) from cloneeventtest; count ------- 5 (1 row) mylab=> show timezone; TimeZone ---------- UTC (1 row) mylab=> SET timezone = 'America/Los_Angeles'; SET mylab=> select * from cloneeventtest; counter | seconds_elapsed | data ---------+-----------------+------------------------------- 1 | 60 | 2020-01-20 13:57:13.473963-08 2 | 120 | 2020-01-20 13:58:13.473963-08 3 | 180 | 2020-01-20 13:59:13.473963-08 4 | 240 | 2020-01-20 14:00:13.473963-08 5 | 300 | 2020-01-20 14:01:13.473963-08 (5 rows)","title":"4.6. Verify the data divergence on the Clone Cluster"},{"location":"modules/fastclone/#47-run-the-pgbench-workload-on-the-clone-cluster","text":"We are going to start the same pgbench workload as we did on the primary cluster in step# 4.1. pgbench -- progress - timestamp - M prepared - n - T 1800 - P 60 - c 8 - j 8 -- host = labstack - clone - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab > clone_results . log","title":"4.7. Run the pgbench workload on the Clone Cluster"},{"location":"modules/fastclone/#48-verify-the-pgbench-metrics-on-the-primary-and-the-clone-cluster","text":"Once the pgbench workload completed on both the primary and the clone cluster we can verify the TPS metrics from both the cluster by looking at the output file. [ec2-user@ip-xxxxxxxx ~]$ more Primary_results.log transaction type: multiple scripts scaling factor: 100 query mode: prepared number of clients: 8 number of threads: 8 duration: 1800 s number of transactions actually processed: 19494742 latency average = 0.739 ms latency stddev = 1.361 ms tps = 10830.345301 (including connections establishing) tps = 10830.428982 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 928533 transactions (4.8% of total, tps = 515.848479) - latency average = 6.228 ms - latency stddev = 2.127 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 18561932 transactions (95.2% of total, tps = 10312.120725) - latency average = 0.464 ms - latency stddev = 0.371 ms [ec2-user@ip-10-0-0-204 ~]$ more clone_results.log transaction type: multiple scripts scaling factor: 100 query mode: prepared number of clients: 8 number of threads: 8 duration: 1800 s number of transactions actually processed: 19494742 latency average = 0.739 ms latency stddev = 1.361 ms tps = 10830.345301 (including connections establishing) tps = 10830.428982 (excluding connections establishing) SQL script 1: <builtin: TPC-B (sort of)> - weight: 1 (targets 4.8% of total) - 928533 transactions (4.8% of total, tps = 515.848479) - latency average = 6.228 ms - latency stddev = 2.127 ms SQL script 2: <builtin: select only> - weight: 20 (targets 95.2% of total) - 18561932 transactions (95.2% of total, tps = 10312.120725) - latency average = 0.464 ms - latency stddev = 0.371 ms","title":"4.8. Verify the pgbench metrics on the primary and the clone cluster."},{"location":"modules/fastclone/#49-sample-graph-on-the-fast-clone-performance-impact","text":"Below is the sample comparison slide to show the performance impact of the clone","title":"4.9. Sample graph on the fast clone performance impact"},{"location":"modules/migration/","text":"Oracle to Aurora PostgreSQL migration using DMS and SCT \u00b6 Note This lab is independent from the Aurora PostgreSQL lab and it creates its own infrastructure (VPC\u2019s .Subnets, security group etc, Source Oracle Database, Target Aurora PostgreSQL Database etc,). In order to use this lab you are required to run the cloudformation from this module only. Once the clouformation stack is complete then you can proceed to the rest of this module. The following instructions help you launch an AWS CloudFormation stack, which sets up the resources required for this lab. Once the resources are launched you are guided through the migration of a sample database from Oracle to Amazon Aurora PostgreSQL using the AWS Database Migration Service (AWS DMS) and the AWS Schema Conversion Tool (AWS SCT). The resources launched by the CloudFormation stack include: An Oracle RDS source within a VPC with all networking resources configured with a sample database installed from github An Aurora target within same VPC as #1 with all networking resources configured A Windows Host with the Schema Conversion Tool pre-installed within same VPC as #1 A Database Migration Service Instance within same VPC as #1 The AWS CloudFormation script provisions the necessary security groups with access to database resources from #3 and #4 and allow RDP access to #3 Prerequisites before you start: \u00b6 1.An active AWS account to use AWS services. All the participants will be using pre-provisioned AWS accounts to run this lab. 2. The AWS account should have required privileges to be able to create all the resources created in this lab. The pre-provisioned AWS account will have all required privilege to run this lab. 3. If you are on a Mac, download the Remote Desktop Client from App Store, if you don\u2019t have one already. 4. Amazon EC2 Key pair required to log into the AWS SCT instance launched as a part of this lab. The pre-provisioned AWS accounts have a pre-created EC2 key pair \u201cee-default-keypair\u201d, this will be used in this lab. 5. Active login to Oracle OTN (https://login.oracle.com) to be able to download the ODBC drivers required for this lab. High-level steps: Log into your AWS account Launch the CloudFormation stack Log in to the Windows Host and install JDBC drivers Use AWS SCT to convert source Oracle schema objects to target Aurora PostgreSQL Create a source endpoint (the source database is launched using the CloudFormation stack) Create a target endpoint (the target database is launched using the CloudFormation stack) Create a task to migrate the data from the source database to the target database Verify that the data has been migrated to the target Create an EC2 Key Pair : Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. You will need an Amazon EC2 key pair in order to log into the AWS SCT instance launched as part of your stack. If you have access to an existing one in the Region where your assets are launched, you can skip this step and use it, (however you should check to be sure.) If you don\u2019t have access to an existing key pair, you will need to create one, (you can remove it once the lab is complete if you want to.) To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file Lab Instructions: Log in to your AWS Management Console and go to the AWS CloudFormation landing page (This is for Ireland, you can find the AWS CloudFormation service in all supported Regions in your console.) Click create stack, select \u201cUpload a template file\u201d and launch the CloudFormation stack from this url (Please download the template file to your computer drive) Here is a screenshot of the first page \u2013 Click on \u201ccreate stack\u201d and select \u201cwith new resources(standard) Click on the \u201cChoose file\u201d button and point the location of the template file which you have downloaded in step -2 and click next. Click next to bring up the stack details page Do the following for the entries on this page Parameters for the Source RDS Database Configuration Resource name Value Stack name give a friendly name, such as \u201creinvent2019-dms-sct-lab\u201d for the stack SourceDBPassword use srcadmin123 (default) SourceDBStorage use 50 (default) sourceDBInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit. SourceDBEngineType use oracle-se2 (default) LicenseModelType use \u201cbring-your-own-license\u201d (default) Parameters for the target Aurora Database Configuration Resource name Value AuroraDBUsername use auradmin (default) AuroraDBPassword use auroradmin123 AuroraInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit. AuroraEngineType use aurora-postgresql (default) Parameters for the Schema Conversion Tool Host Configuration Resource name Value SCTHostKeyName Select the existing pre-created EC2 key pairs to enable access to the AWS SCT host. The pre-created EC2 key pair is \u201cee-default-keypair\u201d SCTHostInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit Parameters for the DMS Instance Configuration Resource name Value DMSInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit. DMSInstanceStorage use 50 (default) DMSInstanceVersion use 3.1.3 (default) Other Parameters Resource name Value CreateRoleResources This should be true, unless you have run DMS before in this region and the required IAM roles have already been created. Setting this to true when the roles already exist will cause the Stack Creation to fail, and you will need to re-run the stack creation with this set to false. If you aren\u2019t sure, try it with \u201ctrue\u201d first. If it fails, try again with \u201cfalse.\u201d LaptopIPAddress the IP address you are running the workshop from, this will limit access to the SCT Host only from your laptop. If you want to limit access to your SCT host enter your laptop IP address here. Finally, click Next. 7. On the next page, simply click Next. 8. On the next page, check the check box to allow AWS CloudFormation to create IAM resources with custom names. Under the \u201cCapabilities\u201d section check the box to acknowledge that you allow Cloudformation to create IAM roles with custom names. Then click Create task ! Leave everything else default. Once the CloudFormation stack launch is complete, you should see two Amazon RDS instances in your account in the Region you created the stack. Approximately, it will take 15\u201320 minutes for creating the Stack. 9. When the stack creation completes, look at the Outputs section. It will provide you with the names, etc., of the resources needed to complete the workshop. 10. Once stack is completed, Log in to the AWS SCT host using RDP and the information provided in the stack Output section. The user name to connect as is \u201cAdministrator\u201d. You will need to open a new tab and go to the Amazon EC2 console to decrypt the password using the key pair you provided during the stack creation. a. Go to the EC2 console. b. Find and select the EC2 instance you just created. The name should contain your stack name. c. Click the \u201cconnect\u201d button at the top of the console. d. Supply the key pair when prompted to do so. e. Launch RDP and connect as Administrator using the password just obtained. 11. Once on the host, open the document \u201cJDBC Driver Install Instructions.rtf\u201d located on the desktop and follow the instructions for installing the JDBC drivers for the database engines selected during stack creation. Internet Explorer \u2022 For this Oracle -> Aurora PostgreSQL lab you will need the Oracle and PostgreSQL drivers Please note if you are not able to see the pop on the Internet explorer then do this (Internet Options -> security -> trusted sites -> add the website address) (or whatever level you're working with) -> Custom Level -> Downloads -> \"Automatic prompting for file downloads\" set to enabled. Or simply download and Install Firefox Schema Conversion \u00b6 Launch AWS SCT on your Windows Host \u2013 there\u2019s a shortcut for it on your desktop You can choose to update the AWS SCT installation or dismiss update dialog box (this is recommended for the Workshop, in all other situations you will want to download the update and install it) Accept the license agreement The \u201cCreate a new database Migration Project\u201d Wizard will launch. If it doesn\u2019t, you should select this option from the File Menu. Make sure that you have the following selected, and then click Next. a. Transactional database (OLTP) b. Source database engine: Oracle c. I want to switch engines and optimize for the cloud. Provide the connection information for your source Amazon RDS database. This information can be obtained from the Outputs section of the CloudFormation Stack. Leave \u201cUse SSL\u201d unchecked Check \u201cStore password\u201d Browse for and find the JDBC driver you just installed. For Oracle sources the SID to use is SRCDB> When you are finished press the \u201cTest Connection\u201d button. 6. After a successful connection test. Click \u201cOK\u201d, then click \u201cNext\u201d 7. Now. select the schema to convert and click next. For this workshop the schema will be \u201cDMS_SAMPLE\u201d 8. The Schema Conversion Tool will now perform an analysis of the DMS_SAMPLE, wait while it completes. 9. When the analysis is complete you will see the \u201cDatabase migration assessment report\u201d. Review it as it will provide information on various target databases and how much can automatically be converted. When done reviewing the report click \u201cNext\u201d 10. Now you will set up your connection to your target database. Select the correct target engine type (Amazon Aurora PostgreSQL Compatible). For Aurora PostgreSQL compatible use postgres for the Database field, The rest of the information for filling in the option can be obtained from the outputs section of the CloudFormation stack you created. Once you have filled out all of the fields click \u201cTest Connection\u201d 11. Once the connection test completes successfully click \u201cOK\u201d and then click \u201cFinish\u201d. 12. At this point a migration project will launch. 13. In the left-hand panel deselect all of the source schemas except for DMS_SAMPLE. 14. Now, on the source schema panel, select the DMS_SAMPLE (or dms_sample -> dbo) and right-click to bring up the menu to convert the schema. You can also select convert schema from the Actions 15. A confirmation dialog box opens. Click \u201cYes\u201d to continue as there are no objects in the target database at this point. 16. AWS SCT will convert your source schema to a target schema that will be displayed in the right-hand panel as either dms_sample or dms_sample_dbo. 17. At this point you can select objects in either the source or target panels and see a comparison of original and converted code and objects. You can also make changes to the converted objects as desired. Look through which objects could be automatically converted and which could not be. 18. Since the majority of the objects that could not be converted are secondary objects like functions or procedures, right-click on the schema that was created (in the right panel) and select \u201cApply to database\u201d. This will write the converted objects to the target database. 19. Another confirmation dialog box will pop up at this point. Click \u201cYes\u201d to continue. 20. The schema conversion is now complete. You are ready to move on to the next step and use the Database Migration Service to move the data from your Source to your Target. Data Migration \u00b6 Now we will use the Database Migration Service (DMS) to move the data from the Source to the Target. Go to the AWS Management Console and bring up the Database Migration Service page. To migrate date from source to target we will: Create a source endpoint Create a target endpoint Create a task to move data from the source endpoint to the target endpoint Create a source endpoint \u00b6 Click Endpoints on the left-hand side of the screen. Then click \u201cCreate Endpoint\u201d and the create endpoint page will appear. You will do this twice, once to create a source endpoint and once to create a target endpoint. The screenshot are taken from the old AWS console. \u2022Select Endpoints from the menu on the left and \u201cclick the \"create endpoint\" button at the top. \u2022Endpoint Type: source Note: if you check \u201cselect RDS DB Instance\u201d you can select your particular instance from drop-down menus and several entries will be populated for you. \u2022Endpoint identifier: orclsourcedb \u2022Engine: Oracle \u2022Server Name: RDS Source endpoint from your account (found in your CloudFormation stack outputs) \u2022Port: 1521 for Oracle \u2022SSL Mode: none \u2022User name: dbmaster \u2022Password: srcadmin123 ( Please note that if you changed the default settings when creating the CloudFormation stack, then you need to enter the password that you have provided) \u2022Database name: dms_sample \u2022For Oracle - SID: SRCDB \u2022VPC: VPC launched by the AWS CloudFormation template \u2022Replication Instance: choose the instance that was created as part of the stack creation \u2022Refresh schemas: Yes - check this box Test the connection and save it if the test is successful. Create a target endpoint \u00b6 \u2022Click the \"create endpoint\" button at the top \u2022Endpoint Type: Target Note: if you check \u201cselect RDS DB Instance\u201d you can select your particular instance from drop down menus and several entries will be populated for you. \u2022Endpoint identifier: apgtargetdb \u2022Engine: simply aurora-postgresql \u2022Server name: endpoint of Aurora target instance \u2022Port: 5432 \u2022SSL Mode: none \u2022Username: auradmin \u2022Password: auroradmin123 ( Please note that if you changed the default settings when creating the Cloudformation stack, then you need to enter the password that you have provided) \u2022For Aurora PostgreSQL - Database name: postgres \u2022VPC: VPC launched by the CloudFormation template \u2022Replication Instance: choose the instance that was created as part of the workshop stack Test the connection and save it if the test is successful Create a migraiton task \u00b6 Next, we need to create a migration task. Navigate to the Database Migration Service item in the console and select \u201cDatabase Migration Tasks\u201d from the menu on the left in your DMS console. Click the \u201cCreate task\u201d button in the upper left and fill out the form as follows: \u2022Name: Give your task a relevant name (dms-workshop or something like that) \u2022Replication instance: choose the one you created (see your CloudFormation stack outputs) \u2022Source endpoint: choose the source endpoint you just created \u2022Target endpoint: choose the target endpoint you just created \u2022Migration type: Migrate existing data \u2022Start task on create: check this (or don't, up to you). If you don\u2019t, you\u2019ll need to start your task separately. Target table preparation mode: Do Nothing (Important for PostgreSQL targets - you DON\u2019T want to drop tables on the target) Why a do-nothing task? Since the initial load in AWS DMS is done table by table, which means that the target tables cannot have active foreign key constraints. As we are using AWS SCT to convert source objects into target objects, all secondary objects were created as part of the process. This means that we would need to disable all foreign key constraints on the target for the initial full load to be successful. One of the ways to do this is to use the session_replication_role parameter in PostgreSQL. Triggers also have a state in PostgreSQL (Origin, replica, always or disabled). When the session_replication_role parameter is set to replica, only triggers of the state replica will be active and are fired when called. If not, the triggers remain inactive. We have already setup the parameter group on the target to set this role to replica which means all foreign key constraints (innately triggers in the origin state) will not be active. However, PostgreSQL has a failsafe mechanism of not letting a table truncate even with this role set. As we are using prepopulated tables on the target and cannot truncate the table, we are using \u201cdo_nothing\u201d for the target table prep mode. More details in this awesome blog post \u2022 Target table preparation mode: Do nothing \u2022 Include LOB columns in replication: Accept the default \u2022 Enable validation: up to you \u2013 enabling will take additional time to complete \u2022 Enable CloudWatch Logs: YES! (Check this box) Table Mappings \u2013 Selection and Transformation Rules (For Oracle to Aurora PostgreSQL Migrations): Select Guided UI and click \u201cAdd new selection rule\u201d Choose DMS_SAMPLE from the drop-down box Next, expand \u201cTransformation rule\u201d and click on \u201cadd transformation rule\u201d Choose schema for target dropdown -> Schema name is DMS_SAMPLE -> Choose \u201cmake lower case\u201d for action & click \u201cadd transformation rule\u201d. Choose table for target dropdown -> Schema name is DMS_SAMPLE and table name is % -> Choose \u201cmake lower case\u201d for action & click \u201cadd transformation rule\u201d. Choose column for target dropdown -> schema name is DMS_SAMPLE, table name is % -> column name is % -> Choose \u201cmake lower case\u201d for action Click on the \u201cJSON\u201d tab to review the details you created to make sure they look like this: Note \u2013 We are doing 2, 3 and 4 above as the schema has been pre-created on the target with lowercase (PostgreSQL convention). Your task JSON should look like this: { \"rules\": [ { \"rule-type\": \"selection\", \"rule-id\": \"1\", \"rule-name\": \"1\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\", \"table-name\": \"%\" }, \"rule-action\": \"include\" }, { \"rule-type\": \"transformation\", \"rule-id\": \"2\", \"rule-name\": \"2\", \"rule-target\": \"schema\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\" }, \"rule-action\": \"convert-lowercase\" }, { \"rule-type\": \"transformation\", \"rule-id\": \"3\", \"rule-name\": \"3\", \"rule-target\": \"table\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\", \"table-name\": \"%\" }, \"rule-action\": \"convert-lowercase\" }, { \"rule-type\": \"transformation\", \"rule-id\": \"4\", \"rule-name\": \"4\", \"rule-target\": \"column\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\", \"table-name\": \"%\", \"column-name\": \"%\" }, \"rule-action\": \"convert-lowercase\" } ] } Start Moving Data \u00b6 Click create task now and let the task run. Note: Task may fail for 3 tables while it should go well for other tables in the DMS_SAMPLE (or dbo) schema. As a bonus step you can investigate why this is happening and fix the problem before reloading failed tables. Checking your results: In the AWS DMS dashboard, view the running process In the remote desktop connection, install PGAdmin and review the database tables, etc. Add new server and run the query to verify the tables and the number of rows.","title":"Oracle to Aurora PostgreSQL migration using DMS and SCT"},{"location":"modules/migration/#oracle-to-aurora-postgresql-migration-using-dms-and-sct","text":"Note This lab is independent from the Aurora PostgreSQL lab and it creates its own infrastructure (VPC\u2019s .Subnets, security group etc, Source Oracle Database, Target Aurora PostgreSQL Database etc,). In order to use this lab you are required to run the cloudformation from this module only. Once the clouformation stack is complete then you can proceed to the rest of this module. The following instructions help you launch an AWS CloudFormation stack, which sets up the resources required for this lab. Once the resources are launched you are guided through the migration of a sample database from Oracle to Amazon Aurora PostgreSQL using the AWS Database Migration Service (AWS DMS) and the AWS Schema Conversion Tool (AWS SCT). The resources launched by the CloudFormation stack include: An Oracle RDS source within a VPC with all networking resources configured with a sample database installed from github An Aurora target within same VPC as #1 with all networking resources configured A Windows Host with the Schema Conversion Tool pre-installed within same VPC as #1 A Database Migration Service Instance within same VPC as #1 The AWS CloudFormation script provisions the necessary security groups with access to database resources from #3 and #4 and allow RDP access to #3","title":"Oracle to Aurora PostgreSQL migration using DMS and SCT"},{"location":"modules/migration/#prerequisites-before-you-start","text":"1.An active AWS account to use AWS services. All the participants will be using pre-provisioned AWS accounts to run this lab. 2. The AWS account should have required privileges to be able to create all the resources created in this lab. The pre-provisioned AWS account will have all required privilege to run this lab. 3. If you are on a Mac, download the Remote Desktop Client from App Store, if you don\u2019t have one already. 4. Amazon EC2 Key pair required to log into the AWS SCT instance launched as a part of this lab. The pre-provisioned AWS accounts have a pre-created EC2 key pair \u201cee-default-keypair\u201d, this will be used in this lab. 5. Active login to Oracle OTN (https://login.oracle.com) to be able to download the ODBC drivers required for this lab. High-level steps: Log into your AWS account Launch the CloudFormation stack Log in to the Windows Host and install JDBC drivers Use AWS SCT to convert source Oracle schema objects to target Aurora PostgreSQL Create a source endpoint (the source database is launched using the CloudFormation stack) Create a target endpoint (the target database is launched using the CloudFormation stack) Create a task to migrate the data from the source database to the target database Verify that the data has been migrated to the target Create an EC2 Key Pair : Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. You will need an Amazon EC2 key pair in order to log into the AWS SCT instance launched as part of your stack. If you have access to an existing one in the Region where your assets are launched, you can skip this step and use it, (however you should check to be sure.) If you don\u2019t have access to an existing key pair, you will need to create one, (you can remove it once the lab is complete if you want to.) To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file Lab Instructions: Log in to your AWS Management Console and go to the AWS CloudFormation landing page (This is for Ireland, you can find the AWS CloudFormation service in all supported Regions in your console.) Click create stack, select \u201cUpload a template file\u201d and launch the CloudFormation stack from this url (Please download the template file to your computer drive) Here is a screenshot of the first page \u2013 Click on \u201ccreate stack\u201d and select \u201cwith new resources(standard) Click on the \u201cChoose file\u201d button and point the location of the template file which you have downloaded in step -2 and click next. Click next to bring up the stack details page Do the following for the entries on this page Parameters for the Source RDS Database Configuration Resource name Value Stack name give a friendly name, such as \u201creinvent2019-dms-sct-lab\u201d for the stack SourceDBPassword use srcadmin123 (default) SourceDBStorage use 50 (default) sourceDBInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit. SourceDBEngineType use oracle-se2 (default) LicenseModelType use \u201cbring-your-own-license\u201d (default) Parameters for the target Aurora Database Configuration Resource name Value AuroraDBUsername use auradmin (default) AuroraDBPassword use auroradmin123 AuroraInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit. AuroraEngineType use aurora-postgresql (default) Parameters for the Schema Conversion Tool Host Configuration Resource name Value SCTHostKeyName Select the existing pre-created EC2 key pairs to enable access to the AWS SCT host. The pre-created EC2 key pair is \u201cee-default-keypair\u201d SCTHostInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit Parameters for the DMS Instance Configuration Resource name Value DMSInstanceType Pick any instance type. Maybe pick something different than your neighbor to spread things around a bit. DMSInstanceStorage use 50 (default) DMSInstanceVersion use 3.1.3 (default) Other Parameters Resource name Value CreateRoleResources This should be true, unless you have run DMS before in this region and the required IAM roles have already been created. Setting this to true when the roles already exist will cause the Stack Creation to fail, and you will need to re-run the stack creation with this set to false. If you aren\u2019t sure, try it with \u201ctrue\u201d first. If it fails, try again with \u201cfalse.\u201d LaptopIPAddress the IP address you are running the workshop from, this will limit access to the SCT Host only from your laptop. If you want to limit access to your SCT host enter your laptop IP address here. Finally, click Next. 7. On the next page, simply click Next. 8. On the next page, check the check box to allow AWS CloudFormation to create IAM resources with custom names. Under the \u201cCapabilities\u201d section check the box to acknowledge that you allow Cloudformation to create IAM roles with custom names. Then click Create task ! Leave everything else default. Once the CloudFormation stack launch is complete, you should see two Amazon RDS instances in your account in the Region you created the stack. Approximately, it will take 15\u201320 minutes for creating the Stack. 9. When the stack creation completes, look at the Outputs section. It will provide you with the names, etc., of the resources needed to complete the workshop. 10. Once stack is completed, Log in to the AWS SCT host using RDP and the information provided in the stack Output section. The user name to connect as is \u201cAdministrator\u201d. You will need to open a new tab and go to the Amazon EC2 console to decrypt the password using the key pair you provided during the stack creation. a. Go to the EC2 console. b. Find and select the EC2 instance you just created. The name should contain your stack name. c. Click the \u201cconnect\u201d button at the top of the console. d. Supply the key pair when prompted to do so. e. Launch RDP and connect as Administrator using the password just obtained. 11. Once on the host, open the document \u201cJDBC Driver Install Instructions.rtf\u201d located on the desktop and follow the instructions for installing the JDBC drivers for the database engines selected during stack creation. Internet Explorer \u2022 For this Oracle -> Aurora PostgreSQL lab you will need the Oracle and PostgreSQL drivers Please note if you are not able to see the pop on the Internet explorer then do this (Internet Options -> security -> trusted sites -> add the website address) (or whatever level you're working with) -> Custom Level -> Downloads -> \"Automatic prompting for file downloads\" set to enabled. Or simply download and Install Firefox","title":"Prerequisites before you start:"},{"location":"modules/migration/#schema-conversion","text":"Launch AWS SCT on your Windows Host \u2013 there\u2019s a shortcut for it on your desktop You can choose to update the AWS SCT installation or dismiss update dialog box (this is recommended for the Workshop, in all other situations you will want to download the update and install it) Accept the license agreement The \u201cCreate a new database Migration Project\u201d Wizard will launch. If it doesn\u2019t, you should select this option from the File Menu. Make sure that you have the following selected, and then click Next. a. Transactional database (OLTP) b. Source database engine: Oracle c. I want to switch engines and optimize for the cloud. Provide the connection information for your source Amazon RDS database. This information can be obtained from the Outputs section of the CloudFormation Stack. Leave \u201cUse SSL\u201d unchecked Check \u201cStore password\u201d Browse for and find the JDBC driver you just installed. For Oracle sources the SID to use is SRCDB> When you are finished press the \u201cTest Connection\u201d button. 6. After a successful connection test. Click \u201cOK\u201d, then click \u201cNext\u201d 7. Now. select the schema to convert and click next. For this workshop the schema will be \u201cDMS_SAMPLE\u201d 8. The Schema Conversion Tool will now perform an analysis of the DMS_SAMPLE, wait while it completes. 9. When the analysis is complete you will see the \u201cDatabase migration assessment report\u201d. Review it as it will provide information on various target databases and how much can automatically be converted. When done reviewing the report click \u201cNext\u201d 10. Now you will set up your connection to your target database. Select the correct target engine type (Amazon Aurora PostgreSQL Compatible). For Aurora PostgreSQL compatible use postgres for the Database field, The rest of the information for filling in the option can be obtained from the outputs section of the CloudFormation stack you created. Once you have filled out all of the fields click \u201cTest Connection\u201d 11. Once the connection test completes successfully click \u201cOK\u201d and then click \u201cFinish\u201d. 12. At this point a migration project will launch. 13. In the left-hand panel deselect all of the source schemas except for DMS_SAMPLE. 14. Now, on the source schema panel, select the DMS_SAMPLE (or dms_sample -> dbo) and right-click to bring up the menu to convert the schema. You can also select convert schema from the Actions 15. A confirmation dialog box opens. Click \u201cYes\u201d to continue as there are no objects in the target database at this point. 16. AWS SCT will convert your source schema to a target schema that will be displayed in the right-hand panel as either dms_sample or dms_sample_dbo. 17. At this point you can select objects in either the source or target panels and see a comparison of original and converted code and objects. You can also make changes to the converted objects as desired. Look through which objects could be automatically converted and which could not be. 18. Since the majority of the objects that could not be converted are secondary objects like functions or procedures, right-click on the schema that was created (in the right panel) and select \u201cApply to database\u201d. This will write the converted objects to the target database. 19. Another confirmation dialog box will pop up at this point. Click \u201cYes\u201d to continue. 20. The schema conversion is now complete. You are ready to move on to the next step and use the Database Migration Service to move the data from your Source to your Target.","title":"Schema Conversion"},{"location":"modules/migration/#data-migration","text":"Now we will use the Database Migration Service (DMS) to move the data from the Source to the Target. Go to the AWS Management Console and bring up the Database Migration Service page. To migrate date from source to target we will: Create a source endpoint Create a target endpoint Create a task to move data from the source endpoint to the target endpoint","title":"Data Migration"},{"location":"modules/migration/#create-a-source-endpoint","text":"Click Endpoints on the left-hand side of the screen. Then click \u201cCreate Endpoint\u201d and the create endpoint page will appear. You will do this twice, once to create a source endpoint and once to create a target endpoint. The screenshot are taken from the old AWS console. \u2022Select Endpoints from the menu on the left and \u201cclick the \"create endpoint\" button at the top. \u2022Endpoint Type: source Note: if you check \u201cselect RDS DB Instance\u201d you can select your particular instance from drop-down menus and several entries will be populated for you. \u2022Endpoint identifier: orclsourcedb \u2022Engine: Oracle \u2022Server Name: RDS Source endpoint from your account (found in your CloudFormation stack outputs) \u2022Port: 1521 for Oracle \u2022SSL Mode: none \u2022User name: dbmaster \u2022Password: srcadmin123 ( Please note that if you changed the default settings when creating the CloudFormation stack, then you need to enter the password that you have provided) \u2022Database name: dms_sample \u2022For Oracle - SID: SRCDB \u2022VPC: VPC launched by the AWS CloudFormation template \u2022Replication Instance: choose the instance that was created as part of the stack creation \u2022Refresh schemas: Yes - check this box Test the connection and save it if the test is successful.","title":"Create a source endpoint"},{"location":"modules/migration/#create-a-target-endpoint","text":"\u2022Click the \"create endpoint\" button at the top \u2022Endpoint Type: Target Note: if you check \u201cselect RDS DB Instance\u201d you can select your particular instance from drop down menus and several entries will be populated for you. \u2022Endpoint identifier: apgtargetdb \u2022Engine: simply aurora-postgresql \u2022Server name: endpoint of Aurora target instance \u2022Port: 5432 \u2022SSL Mode: none \u2022Username: auradmin \u2022Password: auroradmin123 ( Please note that if you changed the default settings when creating the Cloudformation stack, then you need to enter the password that you have provided) \u2022For Aurora PostgreSQL - Database name: postgres \u2022VPC: VPC launched by the CloudFormation template \u2022Replication Instance: choose the instance that was created as part of the workshop stack Test the connection and save it if the test is successful","title":"Create a target endpoint"},{"location":"modules/migration/#create-a-migraiton-task","text":"Next, we need to create a migration task. Navigate to the Database Migration Service item in the console and select \u201cDatabase Migration Tasks\u201d from the menu on the left in your DMS console. Click the \u201cCreate task\u201d button in the upper left and fill out the form as follows: \u2022Name: Give your task a relevant name (dms-workshop or something like that) \u2022Replication instance: choose the one you created (see your CloudFormation stack outputs) \u2022Source endpoint: choose the source endpoint you just created \u2022Target endpoint: choose the target endpoint you just created \u2022Migration type: Migrate existing data \u2022Start task on create: check this (or don't, up to you). If you don\u2019t, you\u2019ll need to start your task separately. Target table preparation mode: Do Nothing (Important for PostgreSQL targets - you DON\u2019T want to drop tables on the target) Why a do-nothing task? Since the initial load in AWS DMS is done table by table, which means that the target tables cannot have active foreign key constraints. As we are using AWS SCT to convert source objects into target objects, all secondary objects were created as part of the process. This means that we would need to disable all foreign key constraints on the target for the initial full load to be successful. One of the ways to do this is to use the session_replication_role parameter in PostgreSQL. Triggers also have a state in PostgreSQL (Origin, replica, always or disabled). When the session_replication_role parameter is set to replica, only triggers of the state replica will be active and are fired when called. If not, the triggers remain inactive. We have already setup the parameter group on the target to set this role to replica which means all foreign key constraints (innately triggers in the origin state) will not be active. However, PostgreSQL has a failsafe mechanism of not letting a table truncate even with this role set. As we are using prepopulated tables on the target and cannot truncate the table, we are using \u201cdo_nothing\u201d for the target table prep mode. More details in this awesome blog post \u2022 Target table preparation mode: Do nothing \u2022 Include LOB columns in replication: Accept the default \u2022 Enable validation: up to you \u2013 enabling will take additional time to complete \u2022 Enable CloudWatch Logs: YES! (Check this box) Table Mappings \u2013 Selection and Transformation Rules (For Oracle to Aurora PostgreSQL Migrations): Select Guided UI and click \u201cAdd new selection rule\u201d Choose DMS_SAMPLE from the drop-down box Next, expand \u201cTransformation rule\u201d and click on \u201cadd transformation rule\u201d Choose schema for target dropdown -> Schema name is DMS_SAMPLE -> Choose \u201cmake lower case\u201d for action & click \u201cadd transformation rule\u201d. Choose table for target dropdown -> Schema name is DMS_SAMPLE and table name is % -> Choose \u201cmake lower case\u201d for action & click \u201cadd transformation rule\u201d. Choose column for target dropdown -> schema name is DMS_SAMPLE, table name is % -> column name is % -> Choose \u201cmake lower case\u201d for action Click on the \u201cJSON\u201d tab to review the details you created to make sure they look like this: Note \u2013 We are doing 2, 3 and 4 above as the schema has been pre-created on the target with lowercase (PostgreSQL convention). Your task JSON should look like this: { \"rules\": [ { \"rule-type\": \"selection\", \"rule-id\": \"1\", \"rule-name\": \"1\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\", \"table-name\": \"%\" }, \"rule-action\": \"include\" }, { \"rule-type\": \"transformation\", \"rule-id\": \"2\", \"rule-name\": \"2\", \"rule-target\": \"schema\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\" }, \"rule-action\": \"convert-lowercase\" }, { \"rule-type\": \"transformation\", \"rule-id\": \"3\", \"rule-name\": \"3\", \"rule-target\": \"table\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\", \"table-name\": \"%\" }, \"rule-action\": \"convert-lowercase\" }, { \"rule-type\": \"transformation\", \"rule-id\": \"4\", \"rule-name\": \"4\", \"rule-target\": \"column\", \"object-locator\": { \"schema-name\": \"DMS_SAMPLE\", \"table-name\": \"%\", \"column-name\": \"%\" }, \"rule-action\": \"convert-lowercase\" } ] }","title":"Create a migraiton task"},{"location":"modules/migration/#start-moving-data","text":"Click create task now and let the task run. Note: Task may fail for 3 tables while it should go well for other tables in the DMS_SAMPLE (or dbo) schema. As a bonus step you can investigate why this is happening and fix the problem before reloading failed tables. Checking your results: In the AWS DMS dashboard, view the running process In the remote desktop connection, install PGAdmin and review the database tables, etc. Add new server and run the query to verify the tables and the number of rows.","title":"Start Moving Data"},{"location":"modules/perf-insights/","text":"Using Performance Insights \u00b6 This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance. This lab contains the following tasks: Generating load on your DB cluster Understanding the Performance Insights interface Examining the performance of your DB instance This lab requires the following lab modules to be completed first: Prerequisites Creating a New Aurora Cluster (conditional, if creating a cluster manually) Connecting, Loading Data and Auto Scaling (connectivity section only) 1. Generating load on your DB cluster \u00b6 You will use Percona's TPCC-like benchmark script based on sysbench to generate load. For simplicity we have packaged the correct set of commands in an AWS Systems Manager Command Document . You will use AWS Systems Manager Run Command to execute the test. On the Session Manager workstation command line see the Connecting, Loading Data and Auto Scaling lab , enter one of the following commands. If you have completed the Creating a New Aurora Cluster lab, and created the Aurora DB cluster manually execute this command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] \\ --parameters \\ clusterEndpoint=[clusterEndpoint],\\ dbUser=$DBUSER,\\ dbPassword=\"$DBPASS\" If AWS CloudFormation has provisioned the DB cluster on your behalf, and you skipped the Creating a New Aurora Cluster lab, you can run this simplified command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] Command parameter values at a glance: Parameter Parameter Placeholder Value DB cluster provisioned by CloudFormation Value DB cluster configured manually Description --document-name [loadTestRunDoc] See CloudFormation stack output See CloudFormation stack output The name of the command document to run on your behalf. --instance-ids [bastionInstance] See CloudFormation stack output See CloudFormation stack output The EC2 instance to execute this command on. --parameters clusterEndpoint=[clusterEndpoint],dbUser=$DBUSER,dbPassword=\"$DBPASS\" N/A Substitute the DB cluster endpoint with the values configured manually Additional command parameters. The command will be sent to the workstation EC2 instance which will prepare the test data set and run the load test. It may take up to a minute for CloudWatch to reflect the additional load in the metrics. You will see a confirmation that the command has been initiated. 2. Understanding the Performance Insights interface \u00b6 While the command is running, open the Amazon RDS service console in a new tab, if not already open. Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. In the menu on the left hand side, click on the Performance Insights menu option. Next, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. As the different Db instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab we are generating load on the Writer (master) DB instance only. Select the DB instance where the name either ends in -node-01 or -instance-1 Once a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into 3 sections, allowing you to drill down from high level performance indicator metrics down to individual queries, waits, users and hosts generating the load. The performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the buttons across the top right of the interface ( 5m , 1h , 5h , 24h , 1w , all ). You can also zoom into a specific period of time by dragging across the graphs. Note All dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom. Section Filters Description Counter Metrics Click cog icon in top right corner to select additional counters This section plots internal database counter metrics over time, such as number of rows read or written, buffer pool hit ratio, etc. These counters are useful to correlate with other metrics, including the database load metrics, to identify causes of abnormal behavior. Database load Load can be sliced by waits (default), SQL commands, users and hosts This metric is design to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits , SQL (default), Users and Hosts Drill down capability that allows you to get detailed performance data down to the individual commands. 3. Examining the performance of your DB instance \u00b6 After running the load generator workload above, you will see a performance profile similar to the example below in the Performance Insights dashboard. The load generator command will first create an initial data set using sysbench prepare . And then will run an OLTP workload for the duration of 5 minutes, consisting of concurrent transactional reads and writes using 4 parallel threads. Amazon Aurora MySQL specific wait events are documented in the Amazon Aurora MySQL Reference guide . Use the Performance Insights dashboard and the reference guide documentation to evaluate the workload profile of your load test, and answer the following questions: Is the database server overloaded at any point during the load test? Can you identify any resource bottlenecks during the load test? If so how can they be mitigated? What are the most common wait events during the load test? Why are the load patterns different between the first and second phase of the load test?","title":"Using Performance Insights"},{"location":"modules/perf-insights/#using-performance-insights","text":"This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance. This lab contains the following tasks: Generating load on your DB cluster Understanding the Performance Insights interface Examining the performance of your DB instance This lab requires the following lab modules to be completed first: Prerequisites Creating a New Aurora Cluster (conditional, if creating a cluster manually) Connecting, Loading Data and Auto Scaling (connectivity section only)","title":"Using Performance Insights"},{"location":"modules/perf-insights/#1-generating-load-on-your-db-cluster","text":"You will use Percona's TPCC-like benchmark script based on sysbench to generate load. For simplicity we have packaged the correct set of commands in an AWS Systems Manager Command Document . You will use AWS Systems Manager Run Command to execute the test. On the Session Manager workstation command line see the Connecting, Loading Data and Auto Scaling lab , enter one of the following commands. If you have completed the Creating a New Aurora Cluster lab, and created the Aurora DB cluster manually execute this command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] \\ --parameters \\ clusterEndpoint=[clusterEndpoint],\\ dbUser=$DBUSER,\\ dbPassword=\"$DBPASS\" If AWS CloudFormation has provisioned the DB cluster on your behalf, and you skipped the Creating a New Aurora Cluster lab, you can run this simplified command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] Command parameter values at a glance: Parameter Parameter Placeholder Value DB cluster provisioned by CloudFormation Value DB cluster configured manually Description --document-name [loadTestRunDoc] See CloudFormation stack output See CloudFormation stack output The name of the command document to run on your behalf. --instance-ids [bastionInstance] See CloudFormation stack output See CloudFormation stack output The EC2 instance to execute this command on. --parameters clusterEndpoint=[clusterEndpoint],dbUser=$DBUSER,dbPassword=\"$DBPASS\" N/A Substitute the DB cluster endpoint with the values configured manually Additional command parameters. The command will be sent to the workstation EC2 instance which will prepare the test data set and run the load test. It may take up to a minute for CloudWatch to reflect the additional load in the metrics. You will see a confirmation that the command has been initiated.","title":"1. Generating load on your DB cluster"},{"location":"modules/perf-insights/#2-understanding-the-performance-insights-interface","text":"While the command is running, open the Amazon RDS service console in a new tab, if not already open. Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. In the menu on the left hand side, click on the Performance Insights menu option. Next, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. As the different Db instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab we are generating load on the Writer (master) DB instance only. Select the DB instance where the name either ends in -node-01 or -instance-1 Once a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into 3 sections, allowing you to drill down from high level performance indicator metrics down to individual queries, waits, users and hosts generating the load. The performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the buttons across the top right of the interface ( 5m , 1h , 5h , 24h , 1w , all ). You can also zoom into a specific period of time by dragging across the graphs. Note All dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom. Section Filters Description Counter Metrics Click cog icon in top right corner to select additional counters This section plots internal database counter metrics over time, such as number of rows read or written, buffer pool hit ratio, etc. These counters are useful to correlate with other metrics, including the database load metrics, to identify causes of abnormal behavior. Database load Load can be sliced by waits (default), SQL commands, users and hosts This metric is design to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits , SQL (default), Users and Hosts Drill down capability that allows you to get detailed performance data down to the individual commands.","title":"2. Understanding the Performance Insights interface"},{"location":"modules/perf-insights/#3-examining-the-performance-of-your-db-instance","text":"After running the load generator workload above, you will see a performance profile similar to the example below in the Performance Insights dashboard. The load generator command will first create an initial data set using sysbench prepare . And then will run an OLTP workload for the duration of 5 minutes, consisting of concurrent transactional reads and writes using 4 parallel threads. Amazon Aurora MySQL specific wait events are documented in the Amazon Aurora MySQL Reference guide . Use the Performance Insights dashboard and the reference guide documentation to evaluate the workload profile of your load test, and answer the following questions: Is the database server overloaded at any point during the load test? Can you identify any resource bottlenecks during the load test? If so how can they be mitigated? What are the most common wait events during the load test? Why are the load patterns different between the first and second phase of the load test?","title":"3. Examining the performance of your DB instance"},{"location":"modules/prerequisites/","text":"Prerequisites \u00b6 The following steps should be completed before getting started with any of the labs in this repository. Not all steps may apply to all students or environments. This lab contains the following tasks: Signing in to the AWS Management Console Creating a lab environment using AWS CloudFormation 1. Signing in to the AWS Management Console \u00b6 If you are running these labs in a formal, instructional setting, please use the Console URL, and credentials provided to you to access and log into the AWS Management Console. Otherwise, please use your own credentials. You can access the console at: https://console.aws.amazon.com/ or through the Single Sign-On (SSO) mechanism provided by your organization. If you are running these labs in a formal, instructional setting, please use the AWS region provided. Ensure the correct AWS region is selected in the top right corner, if not use that dropdown to choose the correct region. The labs are designed to work in any of the regions where Amazon Aurora PostgreSQL compatible is available. However, not all features and capabilities of Amazon Aurora may be available in all supported regions at this time. 2. Creating a lab environment with no Aurora Cluster using AWS CloudFormation \u00b6 To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. Formal Event If you are running these labs in a formal instructional event, the lab environment may have been initialized on your behalf. If unsure, please verify with the event support staff. Cloudformation template for creating the DB cluster manually. Option Download Template One-Click Launch Description I will create the DB cluster manually lab-no-cluster-pgsql.yml Use when you wish to provision the initial cluster manually.Follow the below section to create the VPC and other infrastructure first. Once the Infrastructure is created then following Lab 1. - Creating a New Aurora Cluster Please download the CloudFormation template (lab-no-cluster-pgsql.yml) and save it in a memorable location such as your desktop, you will need to reference it later. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack . For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options page, leave the defaults as they are, scroll to the bottom and click Next . On the Review labstack page, scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create . The stack will take approximatively 20 minutes to provision, you can monitor the status on the Stack detail page. You can monitor the progress of the stack creation process by refreshing the Events tab. The latest event in the list will indicate CREATE_COMPLETE for the stack resource. Once the status of the stack is CREATE_COMPLETE , click on the Outputs tab. The values here will be critical to the completion of the remainder of the lab. Please take a moment to save these values somewhere that you will have easy access to them during the remainder of the lab. The names that appear in the Key column are referenced directly in the instructions in subsequent steps, using the parameter format: [outputKey] 3. Creating a lab environment with the Aurora Cluster using AWS CloudFormation \u00b6 Option Download Template One-Click Launch Description Provision the DB cluster for me lab-with-cluster-pgsql.yml Use when you wish to skip the initial cluster creation lab and wish to use Cloudformation template to have the DB cluster provisioned for you.Follow the below section to run the cloudformation template. Once the cloudformation stack is completed then continue to Lab 2. - Cluster Endpoints and Read Replica Auto Scaling or Lab 3 and so on. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack. For the ec2Keypair click on the dropdown and choose the keypair you wish to use. If using Event Engine then select ee-default-keypair . If you need to create a new EC2 key pair then refer creating a key pair on EC2 documentation. For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options leave the default and click on Next On the next screen Review the parameters and scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create Stack Once the CloudFormation stack shows CREATE_COMPLETE you can verify the all the resources created by the Stack under the Outputs tab","title":"Prerequisites"},{"location":"modules/prerequisites/#prerequisites","text":"The following steps should be completed before getting started with any of the labs in this repository. Not all steps may apply to all students or environments. This lab contains the following tasks: Signing in to the AWS Management Console Creating a lab environment using AWS CloudFormation","title":"Prerequisites"},{"location":"modules/prerequisites/#1-signing-in-to-the-aws-management-console","text":"If you are running these labs in a formal, instructional setting, please use the Console URL, and credentials provided to you to access and log into the AWS Management Console. Otherwise, please use your own credentials. You can access the console at: https://console.aws.amazon.com/ or through the Single Sign-On (SSO) mechanism provided by your organization. If you are running these labs in a formal, instructional setting, please use the AWS region provided. Ensure the correct AWS region is selected in the top right corner, if not use that dropdown to choose the correct region. The labs are designed to work in any of the regions where Amazon Aurora PostgreSQL compatible is available. However, not all features and capabilities of Amazon Aurora may be available in all supported regions at this time.","title":"1. Signing in to the AWS Management Console"},{"location":"modules/prerequisites/#2-creating-a-lab-environment-with-no-aurora-cluster-using-aws-cloudformation","text":"To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. Formal Event If you are running these labs in a formal instructional event, the lab environment may have been initialized on your behalf. If unsure, please verify with the event support staff. Cloudformation template for creating the DB cluster manually. Option Download Template One-Click Launch Description I will create the DB cluster manually lab-no-cluster-pgsql.yml Use when you wish to provision the initial cluster manually.Follow the below section to create the VPC and other infrastructure first. Once the Infrastructure is created then following Lab 1. - Creating a New Aurora Cluster Please download the CloudFormation template (lab-no-cluster-pgsql.yml) and save it in a memorable location such as your desktop, you will need to reference it later. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack . For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options page, leave the defaults as they are, scroll to the bottom and click Next . On the Review labstack page, scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create . The stack will take approximatively 20 minutes to provision, you can monitor the status on the Stack detail page. You can monitor the progress of the stack creation process by refreshing the Events tab. The latest event in the list will indicate CREATE_COMPLETE for the stack resource. Once the status of the stack is CREATE_COMPLETE , click on the Outputs tab. The values here will be critical to the completion of the remainder of the lab. Please take a moment to save these values somewhere that you will have easy access to them during the remainder of the lab. The names that appear in the Key column are referenced directly in the instructions in subsequent steps, using the parameter format: [outputKey]","title":"2. Creating a lab environment with no Aurora Cluster using AWS CloudFormation"},{"location":"modules/prerequisites/#3-creating-a-lab-environment-with-the-aurora-cluster-using-aws-cloudformation","text":"Option Download Template One-Click Launch Description Provision the DB cluster for me lab-with-cluster-pgsql.yml Use when you wish to skip the initial cluster creation lab and wish to use Cloudformation template to have the DB cluster provisioned for you.Follow the below section to run the cloudformation template. Once the cloudformation stack is completed then continue to Lab 2. - Cluster Endpoints and Read Replica Auto Scaling or Lab 3 and so on. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack. For the ec2Keypair click on the dropdown and choose the keypair you wish to use. If using Event Engine then select ee-default-keypair . If you need to create a new EC2 key pair then refer creating a key pair on EC2 documentation. For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options leave the default and click on Next On the next screen Review the parameters and scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create Stack Once the CloudFormation stack shows CREATE_COMPLETE you can verify the all the resources created by the Stack under the Outputs tab","title":"3. Creating a lab environment with the Aurora Cluster using AWS CloudFormation"},{"location":"modules/qpm/","text":"Query Plan Management \u00b6 1. Prerequisite \u00b6 Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file 2. Creating Aurora PostgreSQL cluster with Cloudformation \u00b6 Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013 2.1 Retrieving Database credentials from Secret Manager \u00b6 Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database. 2.2 Cloudformation Resource chart \u00b6 Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d 2.3 Connecting to the EC2 bastion Instance \u00b6 We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here . 3. Quick start guide on using QPM with automatic capture \u00b6 Query plan management is available with Amazon Aurora PostgreSQL version 10.5-compatible (Aurora 2.1.0) and later, or Amazon Aurora PostgreSQL version 9.6.11-compatible (Aurora 1.4.0) and later. The quickest way to enable QPM is to use the automatic plan capture, which enables the plan capture for all SQL statements that run at least two times. With query plan management, you can control execution plans for a set of statements that you want to manage. You can do the following: Improve plan stability by forcing the optimizer to choose from a small number of known, good plans. Optimize plans centrally and then distribute the best plans globally. Identify indexes that aren't used and assess the impact of creating or dropping an index. Automatically detect a new minimum-cost plan discovered by the optimizer. Try new optimizer features with less risk, because you can choose to approve only the plan changes that improve performance. For additional details on the Query Plan Management please refer official documentation Managing Query Execution Plans for Aurora PostgreSQL . Here are the steps to configure and enable the use of QPM on your Aurora PostgreSQL cluster for automatic capture and using managed plans with QPM: 1. Modify the Amazon Aurora DB Cluster Parameters related to the CCM. \u00b6 Screenshots of some of the steps are shown below a. Open the Amazon RDS service console . b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format <stack-name-apgcustom-clusterparamgroup-nnnn> . In this case the stack name is labstack and hence the DB cluster parameter group file is labstack- apgcustom-clusterparamgroup-nnnn . The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group. d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of rds.enable_plan_management parameter to 1 and click on \u201cSave changes\u201d f. Open your database level parameter group and click on \u201cEdit Parameters\u201d g. Modify the value for apg_plan_mgmt.capture_plan_baselines parameter to \u201cautomatic\u201d and apg_plan_mgmt.use_plan_baselines to \u201cautomatic\u201d. Modify the value for apg_plan_mgmt.use_plan_baselines to \u201ctrue\u201d. For more information, see Modifying Parameters in a DB Parameter Group. Please note that these parameters can be set at either the cluster level or at the database level. The default recommendation would be to set it at the Aurora cluster level. h. Click on the \u201cPreview changes\u201d to verify the changes and click save changes. i. Wait for the status of the instance to change to available . Restart your DB instance to enable this new setting. j. Connect to your DB instance with a SQL client such as psql. For more information, see Using psql to Connect to a PostgreSQL DB Instance . The cluster endpoint for the Aurora PostgreSQL can be found from the key value from the output\u2019s sections under \u201cclusterEndpoint\u201d key. The username and the password need to be extracted from the secrets Manager as shown above ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> select aurora_version(),version(); aurora_version | version ----------------+----------------------------------------------------------------------------- 2.3.5 | PostgreSQL 10.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.9.3, 64-bit (1 row) 2. Create and verify the apg_plan_mgmt extension for your DB instance. \u00b6 a. Create the apg_plan_mgmt extension for your DB instance. mylab=> CREATE EXTENSION apg_plan_mgmt; CREATE EXTENSION mylab=> select extname,extversion from pg_extension where extname='apg_plan_mgmt'; extname | extversion ---------------+------------ apg_plan_mgmt | 1.0.1 b. Query to make sure that all QPM-related parameters are modified to the appropriate value. mylab=> show rds.enable_plan_management; rds.enable_plan_management ---------------------------- 1 mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- automatic mylab=> show apg_plan_mgmt.use_plan_baselines; apg_plan_mgmt.use_plan_baselines ---------------------------------- on 3. Run synthetic workload with automatic capture. \u00b6 The Cloud Formation template used for this workshop creates an EC2 bastion host bootstrapped with PostgreSQL tools (Pgbench, psql and sysbench etc.). The CF will also initialize the Database with pgbench (scale=100) data. a. From another EC2 Instance terminal use pgbench (a PostgreSQL benchmarking tool) to generate a simulated workload, which runs same queries for a specified period. With automatic capture enabled, QPM captures plans for each query that runs at least twice. Below is the example. export PATH =/ home / ec2 - user / postgresql - 10.7 / src / bin / pgbench : $ PATH cd / home / ec2 - user / postgresql - 10.7 / src / bin / pgbench . / pgbench -- progress - timestamp - M prepared - n - T 100 - P 1 - c 500 - j 500 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab b. Query apg_plan_mgmt.dba_plans table to view the managed statements and the execution plans for the SQL statements started with the pgbench tool. cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | enabled | sql_text -1677381765 -225188843 Approved t UPDATE pgbench_branches SET bbalance = bbalance + $1 WHERE bid = $2; -60114982 300482084 Approved t INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP); 1319555216 30042398 Approved t select count(*) from pgbench_branches; -2033469270 -1987991358 Approved t UPDATE pgbench_tellers SET tbalance = tbalance + $1 WHERE tid = $2; c. Turn off automatic capture. Capturing all plans with automatic capture has little runtime overhead and can be enabled in production. We are turning off the automatic capture to make sure that we don\u2019t capture SQL statements outside the pgbench workload. This can be turned off by setting the apg_plan_mgmt.capture_plan_baselines parameter to off from the DB instance-level parameter group. mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- Off d. Verify that the execution plan of the managed statement is the plan captured by QPM. We have manually executed the explain plan on one of the managed statements (highlighted in the yellow above). The explain plan output does show the SQL hash and the plan hash matches with the QPM approved plan for that statement. mylab=> explain (hashes true) UPDATE pgbench_tellers SET tbalance = tbalance + 100 WHERE tid = 200; QUERY PLAN ---------------------------------------------------------------------- Update on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) -> Index Scan using pgbench_tellers_pkey on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) Index Cond: (tid = 200) SQL Hash: -2033469270, Plan Hash: -1987991358 In addition to automatic plan capture, QPM also offers manual capture, which offers a mechanism to capture execution plans for known problematic queries. Capturing the plans automatically is recommended generally. However, there are situations where capturing plans manually would be the best option, such as: You don't want to enable plan management at the Database level, but you do want to control a few critical SQL statements only. You want to save the plan for a specific set of literals or parameter values that are causing a performance problem. 4. QPM Plan adaptability with plan evolution mechanism \u00b6 If the optimizer's generated plan is not a stored plan, the optimizer captures and stores it as a new unapproved plan to preserve stability for the QPM-managed SQL statements. Query plan management provides techniques and functions to add, maintain, and improve execution plans and thus provides Plan adaptability. Users can on demand or periodically instruct QPM to evolve all the stored plans to see if there is a better minimum cost plan available than any of the approved plans. QPM provides apg_plan_mgmt.evolve_plan_baselines function to compare plans based on their actual performance. Depending on the outcome of your performance experiments, you can change a plan's status from unapproved to either approved or rejected. You can instead decide to use the apg_plan_mgmt.evolve_plan_baselines function to temporarily disable a plan if it does not meet your requirements. For additional details about the QPM Plan evolution, see Evaluating Plan Performance. For the first use case, we walk through an example on how QPM helps ensure plan stability, these changes can result in plan regression. In most cases, you set up QPM to use automatic plan capture so that plans are captured for all statements that run two or more times. However, you can also capture plans for a specific set of statements that you specify manually. To do this, you set apg_plan_mgmt.capture_plan_baselines = off by default. At the session level, apg_plan_mgmt.capture_plan_baselines = manual at the session level. How to do it is described later. a. Enable manual plan capture to instruct QPM to capture the execution plan of the desired SQL statements manually. mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET b. Run an explain plan for the query so that QPM can capture the plan of the query (the following output for the explain plan is truncated for brevity). mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 c. Disable manual capture of the plan after you capture the execution plan for the desired SQL statement. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET d. View the captured query plan for the query that ran previously. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline isn't shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); e. To instruct the query optimizer to use the approved or preferred captured plans for your managed statements, set the parameter apg_plan_mgmt.use_plan_baselines to true. mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET f. View the explain plan output to see that the QPM approved plan is used by the query optimizer. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 g. Create a new index on the pgbench_history table column \u201cmtime\u201d to change the planner configuration and force the query optimizer to generate a new plan. mylab=> create index pgbench_hist_mtime on pgbench_history(mtime); CREATE INDEX h. View the explain plan output to see that QPM detects a new plan but still uses the approved plan and maintains the plan stability. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. ***Note: For this example, an approved plan was used instead of the minimum cost plan. SQL Hash: 1561242727, Plan Hash: -1990695905, Minimum Cost Plan Hash: -794604077*** i. Run the following SQL query to view the new plan and status of the plan. To ensure plan stability, QPM stores all the newly generated plans for a managed query in QPM as unapproved plans. The following output shows that there are two different execution plans stored for the same managed statement, as shown by the two different plan_hash values. Although the new execution plan has the minimum cost (lower than the approved plan), QPM continues to ignore the unapproved plans to maintain plan stability. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For the sake of brevity, the plan_outline is not shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+---------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 UnApproved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); The following is an example of plan adaptability with QPM. This example evaluates the unapproved plan based on the minimum speedup factor. It approves any captured unapproved plan that is faster by at least 10 percent than the best approved plan for the statement. For additional details, see Evaluating Plan Performance in the Aurora documentation . mylab=> SELECT apg_plan_mgmt.Evolve_plan_baselines (sql_hash, plan_hash, 1.1,'approve') FROM apg_plan_mgmt.dba_plans WHERE status = 'Unapproved'; NOTICE: [Unapproved] SQL Hash: 1561242727, Plan Hash: 1944377599, SELECT sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where ... NOTICE: Baseline [Planning time 0.693 ms, Execution time 316.644 ms] NOTICE: Baseline+1 [Planning time 0.695 ms, Execution time 213.919 ms] NOTICE: Total time benefit: 102.723 ms, Execution time benefit: 102.725 ms, Avg Log Cardinality Error: 3.53418, Cost = 111.16..111.17 NOTICE: Unapproved -> Approved After QPM evaluates the plan based on the speed factor, the plan status changes to approved. At this point, the optimizer can choose that plan for that managed statement now. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 Approved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); j. View the explain plan output to see whether the query is using the newly approved minimum cost plan. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ----------------------------------------------------------------------------- Aggregate (cost=111.16..111.17 rows=1 width=16) InitPlan 2 (returns $1) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 1 (returns $0) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan using pgbench_hist_mtime on pgbench_history mn (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) InitPlan 4 (returns $3) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 3 (returns $2) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan Backward using pgbench_hist_mtime on pgbench_history mx (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) -> Hash Join (cost=14.60..107.06 rows=632 width=8) Hash Cond: (h.bid = b.bid) -> Index Scan using pgbench_hist_mtime on pgbench_history h (cost=0.42..85.01 rows=2107 width=8) Index Cond: ((mtime >= $1) AND (mtime <= $3)) -> Hash (cost=14.14..14.14 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) SQL Hash: 1561242727, Plan Hash: -794604077 5. Fixing plans with QPM using pg_hint_plan \u00b6 In some cases, the query optimizer doesn\u2019t generate the best execution plan for the query. One approach to fixing this problem is to put query hints into your application code, but this approach is widely discouraged because it makes applications more brittle and harder to maintain, and in some cases, you can\u2019t hint the SQL because it is generated by a 3rd party application. What we will show is how to use hints to control the query optimizer, but then to remove the hints and allow QPM to enforce the desired plan, without adding hints to the application code. For this purpose, PostgreSQL users can use the pg_hint_plan extension to provide directives such as \u201cscan method,\u201d \u201cjoin method,\u201d \u201cjoin order,\u201d, or \u201crow number correction,\u201d to the optimizer. The resulting plan will be saved by QPM, along with any GUC parameters you choose to override (such as work_mem). QPM remembers any GUC parameter overrides and uses them when it needs to recreate the plan. To install and learn more about how to use the pg_hint_plan extension, see the pg_hint_plan documentation. QPM steps to fix the plan generated by using hints Working with pg_hint_plan is incredibly useful for cases where the query can\u2019t be modified to add hints. In this example, use a sample query to generate the execution plan that you want by adding hints to the managed statement. Then associate this execution plan with the original unmodified statement. Here are the detailed steps: a. Check if the plan capture is disabled mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- off (1 row) b. Run the query with the hint to use. In the following example, use the \u201cHashJoin\u201d hint, which is a directive for the optimizer to choose the join method as HashJoin. The original plan of the query without hints is as follows. mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Nested Loop (cost=0.42..181906.82 rows=1000000 width=465) Join Filter: (b.bid = a.bid) -> Index Scan using pgbench_accounts_pkey on pgbench_accounts a (cost=0.42..44165.43 rows=1000000 width=97) -> Materialize (cost=0.00..14.15 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: -451962956 c. Enable pg_hint_plan and manual plan capture: Mylab=> SET pg_hint_plan.enable_hint = true; SET mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET d. EXPLAIN the query with the hints you want to use. In the following example, use the HashJoin (a, b) hint, which is a directive for the optimizer to use a hash join algorithm to join from table a to table b: The plan that you want with a hash join is as follows. mylab=> /*+ HashJoin(a b) */ EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..250138.04 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: 1139293728 e. Verify that plan 1139293728 was captured, and note the status of the plan View the captured plan and the status of the plan. mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; Where plan_hash=1139293728; sql_hash | plan_hash | status | enabled | sql_text -----------+------------+----------+---------+--------------------------- 356104612 | 1139293728 | Approved | t | SELECT + | | | | * + | | | | FROM + | | | | pgbench_branches b + | | | | JOIN + | | | | pgbench_accounts a + | | | | ON b.bid = a.bid + | | | | ORDER BY + | | | | a.aid; f. If necessary, Approve the plan In this case this is the first and only plan saved for statement 356104612, so it was saved as an Approved plan. If this statement already had a baseline of approved plans, then this plan would have been saved as an Unapproved plan. In general, to Reject all existing plans for a statement and then Approve one specific plan, you could call apg_plan_mgmt.set_plan_status twice, like this: mylab=> SELECT apg_plan_mgmt.set_plan_status (sql_hash, plan_hash, 'Rejected') from apg_plan_mgmt.dba_plans where sql_hash = 356104612; SET mylab=> SELECT apg_plan_mgmt.set_plan_status (356104612, 1139293728, 'Approved'); SET g. Remove the hint, turn off manual capture, turn on use_plan_baselines, and also verify that the desired plan is in use without the hint. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..337638.11 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) Note: An Approved plan was used instead of the minimum cost plan. SQL Hash: 356104612, Plan Hash: 1139293728, Minimum Cost Plan Hash: -451962956 6. Deploying QPM-managed plans globally using export and import (no lab) \u00b6 Large enterprise customers often have applications and databases deployed globally. They also often maintain several environments (Dev, QA, Staging, UAT, Preprod, and Prod) for each application database. However, managing the execution plans manually in each of the databases in specific AWS Regions and each of the database environments can be cumbersome and time-consuming. QPM provides an option to export and import QPM-managed plans from one database to another database. With this option, you can manage the query execution centrally and deploy databases globally. This feature is useful for the scenarios where you investigate a set of plans on a preprod database, verify that they perform well, and then load them into a production database. Here are the steps to migrate QPM-managed plans from one database to another. For additional details, see Exporting and Importing Plans in the Aurora documentation. Export the QPM-managed plan from the source system. To do this, from the source database with the preferred execution plan, an authorized DB user can copy any subset of the apg_plan_mgmt.plans table to another table. That user can then save it using the pg_dump command. For additional details on the pg_dump, see pg_dump in the PostgreSQL documentation. Import the QPM-managed plan on the target system. On the target system, copy any subset of the apg_plan_mgmt.plans table to another table, and then save it using the pg_dump command. This is an optional step to preserve any existing managed plans on the target system before importing new plans for the source system. We have assumed that you have used pg_dump tar-format archive in step 1. Use the pg_restore command to copy the .tar file into a new table (plan_copy). For additional details about the pg_restore, see pg_restore in the PostgreSQL documentation. Merge the new table with the apg_plan_mgmt.plans table. Reload the managed plans into shared memory and remove the temporary plans table. SELECT apg_plan_mgmt.reload(); -- Refresh shared memory with new plans. DROP TABLE plans_copy; -- Drop the temporary plan table. 7. Disabling QPM and deleting plans manually \u00b6 Disable the QPM Open your cluster-level parameter group and set the rds.enable_plan_management parameter to 0 Delete all the plans captured SELECT SUM(apg_plan_mgmt.delete_plan(sql_hash, plan_hash)) FROM apg_plan_mgmt.dba_plans; 3.Helpful SQL statements SELECT sql_hash,plan_hash,status,enabled,sql_text FROM apg_plan_mgmt.dba_plans; SELECT sql_hash,plan_hash,status,estimated_total_cost \"cost\",sql_text FROM apg_plan_mgmt.dba_plans;","title":"Query Plan Management"},{"location":"modules/qpm/#query-plan-management","text":"","title":"Query Plan Management"},{"location":"modules/qpm/#1-prerequisite","text":"Note If you started with module - \"Creating a New Aurora Cluster\" please skip to the next step/section as you have already created the EC2 key pair which you can use for this lab. Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file","title":"1. Prerequisite"},{"location":"modules/qpm/#2-creating-aurora-postgresql-cluster-with-cloudformation","text":"Note if you started the lab with module \"Creating a New Aurora Cluster\" please skip the section -2 and proceed to the next section as you have already created the Aurora PostgreSQL cluster. Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013","title":"2. Creating Aurora PostgreSQL cluster with Cloudformation"},{"location":"modules/qpm/#21-retrieving-database-credentials-from-secret-manager","text":"Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database.","title":"2.1 Retrieving Database credentials from Secret Manager"},{"location":"modules/qpm/#22-cloudformation-resource-chart","text":"Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d","title":"2.2 Cloudformation Resource chart"},{"location":"modules/qpm/#23-connecting-to-the-ec2-bastion-instance","text":"We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here .","title":"2.3 Connecting to the EC2 bastion Instance"},{"location":"modules/qpm/#3-quick-start-guide-on-using-qpm-with-automatic-capture","text":"Query plan management is available with Amazon Aurora PostgreSQL version 10.5-compatible (Aurora 2.1.0) and later, or Amazon Aurora PostgreSQL version 9.6.11-compatible (Aurora 1.4.0) and later. The quickest way to enable QPM is to use the automatic plan capture, which enables the plan capture for all SQL statements that run at least two times. With query plan management, you can control execution plans for a set of statements that you want to manage. You can do the following: Improve plan stability by forcing the optimizer to choose from a small number of known, good plans. Optimize plans centrally and then distribute the best plans globally. Identify indexes that aren't used and assess the impact of creating or dropping an index. Automatically detect a new minimum-cost plan discovered by the optimizer. Try new optimizer features with less risk, because you can choose to approve only the plan changes that improve performance. For additional details on the Query Plan Management please refer official documentation Managing Query Execution Plans for Aurora PostgreSQL . Here are the steps to configure and enable the use of QPM on your Aurora PostgreSQL cluster for automatic capture and using managed plans with QPM:","title":"3. Quick start guide on using QPM with automatic capture"},{"location":"modules/qpm/#1-modify-the-amazon-aurora-db-cluster-parameters-related-to-the-ccm","text":"Screenshots of some of the steps are shown below a. Open the Amazon RDS service console . b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format <stack-name-apgcustom-clusterparamgroup-nnnn> . In this case the stack name is labstack and hence the DB cluster parameter group file is labstack- apgcustom-clusterparamgroup-nnnn . The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group. d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of rds.enable_plan_management parameter to 1 and click on \u201cSave changes\u201d f. Open your database level parameter group and click on \u201cEdit Parameters\u201d g. Modify the value for apg_plan_mgmt.capture_plan_baselines parameter to \u201cautomatic\u201d and apg_plan_mgmt.use_plan_baselines to \u201cautomatic\u201d. Modify the value for apg_plan_mgmt.use_plan_baselines to \u201ctrue\u201d. For more information, see Modifying Parameters in a DB Parameter Group. Please note that these parameters can be set at either the cluster level or at the database level. The default recommendation would be to set it at the Aurora cluster level. h. Click on the \u201cPreview changes\u201d to verify the changes and click save changes. i. Wait for the status of the instance to change to available . Restart your DB instance to enable this new setting. j. Connect to your DB instance with a SQL client such as psql. For more information, see Using psql to Connect to a PostgreSQL DB Instance . The cluster endpoint for the Aurora PostgreSQL can be found from the key value from the output\u2019s sections under \u201cclusterEndpoint\u201d key. The username and the password need to be extracted from the secrets Manager as shown above ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> select aurora_version(),version(); aurora_version | version ----------------+----------------------------------------------------------------------------- 2.3.5 | PostgreSQL 10.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.9.3, 64-bit (1 row)","title":"1. Modify the Amazon Aurora DB Cluster Parameters related to the CCM."},{"location":"modules/qpm/#2-create-and-verify-the-apg_plan_mgmt-extension-for-your-db-instance","text":"a. Create the apg_plan_mgmt extension for your DB instance. mylab=> CREATE EXTENSION apg_plan_mgmt; CREATE EXTENSION mylab=> select extname,extversion from pg_extension where extname='apg_plan_mgmt'; extname | extversion ---------------+------------ apg_plan_mgmt | 1.0.1 b. Query to make sure that all QPM-related parameters are modified to the appropriate value. mylab=> show rds.enable_plan_management; rds.enable_plan_management ---------------------------- 1 mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- automatic mylab=> show apg_plan_mgmt.use_plan_baselines; apg_plan_mgmt.use_plan_baselines ---------------------------------- on","title":"2. Create and verify the apg_plan_mgmt extension for your DB instance."},{"location":"modules/qpm/#3-run-synthetic-workload-with-automatic-capture","text":"The Cloud Formation template used for this workshop creates an EC2 bastion host bootstrapped with PostgreSQL tools (Pgbench, psql and sysbench etc.). The CF will also initialize the Database with pgbench (scale=100) data. a. From another EC2 Instance terminal use pgbench (a PostgreSQL benchmarking tool) to generate a simulated workload, which runs same queries for a specified period. With automatic capture enabled, QPM captures plans for each query that runs at least twice. Below is the example. export PATH =/ home / ec2 - user / postgresql - 10.7 / src / bin / pgbench : $ PATH cd / home / ec2 - user / postgresql - 10.7 / src / bin / pgbench . / pgbench -- progress - timestamp - M prepared - n - T 100 - P 1 - c 500 - j 500 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab b. Query apg_plan_mgmt.dba_plans table to view the managed statements and the execution plans for the SQL statements started with the pgbench tool. cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | enabled | sql_text -1677381765 -225188843 Approved t UPDATE pgbench_branches SET bbalance = bbalance + $1 WHERE bid = $2; -60114982 300482084 Approved t INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP); 1319555216 30042398 Approved t select count(*) from pgbench_branches; -2033469270 -1987991358 Approved t UPDATE pgbench_tellers SET tbalance = tbalance + $1 WHERE tid = $2; c. Turn off automatic capture. Capturing all plans with automatic capture has little runtime overhead and can be enabled in production. We are turning off the automatic capture to make sure that we don\u2019t capture SQL statements outside the pgbench workload. This can be turned off by setting the apg_plan_mgmt.capture_plan_baselines parameter to off from the DB instance-level parameter group. mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- Off d. Verify that the execution plan of the managed statement is the plan captured by QPM. We have manually executed the explain plan on one of the managed statements (highlighted in the yellow above). The explain plan output does show the SQL hash and the plan hash matches with the QPM approved plan for that statement. mylab=> explain (hashes true) UPDATE pgbench_tellers SET tbalance = tbalance + 100 WHERE tid = 200; QUERY PLAN ---------------------------------------------------------------------- Update on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) -> Index Scan using pgbench_tellers_pkey on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) Index Cond: (tid = 200) SQL Hash: -2033469270, Plan Hash: -1987991358 In addition to automatic plan capture, QPM also offers manual capture, which offers a mechanism to capture execution plans for known problematic queries. Capturing the plans automatically is recommended generally. However, there are situations where capturing plans manually would be the best option, such as: You don't want to enable plan management at the Database level, but you do want to control a few critical SQL statements only. You want to save the plan for a specific set of literals or parameter values that are causing a performance problem.","title":"3. Run synthetic workload with automatic capture."},{"location":"modules/qpm/#4-qpm-plan-adaptability-with-plan-evolution-mechanism","text":"If the optimizer's generated plan is not a stored plan, the optimizer captures and stores it as a new unapproved plan to preserve stability for the QPM-managed SQL statements. Query plan management provides techniques and functions to add, maintain, and improve execution plans and thus provides Plan adaptability. Users can on demand or periodically instruct QPM to evolve all the stored plans to see if there is a better minimum cost plan available than any of the approved plans. QPM provides apg_plan_mgmt.evolve_plan_baselines function to compare plans based on their actual performance. Depending on the outcome of your performance experiments, you can change a plan's status from unapproved to either approved or rejected. You can instead decide to use the apg_plan_mgmt.evolve_plan_baselines function to temporarily disable a plan if it does not meet your requirements. For additional details about the QPM Plan evolution, see Evaluating Plan Performance. For the first use case, we walk through an example on how QPM helps ensure plan stability, these changes can result in plan regression. In most cases, you set up QPM to use automatic plan capture so that plans are captured for all statements that run two or more times. However, you can also capture plans for a specific set of statements that you specify manually. To do this, you set apg_plan_mgmt.capture_plan_baselines = off by default. At the session level, apg_plan_mgmt.capture_plan_baselines = manual at the session level. How to do it is described later. a. Enable manual plan capture to instruct QPM to capture the execution plan of the desired SQL statements manually. mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET b. Run an explain plan for the query so that QPM can capture the plan of the query (the following output for the explain plan is truncated for brevity). mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 c. Disable manual capture of the plan after you capture the execution plan for the desired SQL statement. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET d. View the captured query plan for the query that ran previously. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline isn't shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); e. To instruct the query optimizer to use the approved or preferred captured plans for your managed statements, set the parameter apg_plan_mgmt.use_plan_baselines to true. mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET f. View the explain plan output to see that the QPM approved plan is used by the query optimizer. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 g. Create a new index on the pgbench_history table column \u201cmtime\u201d to change the planner configuration and force the query optimizer to generate a new plan. mylab=> create index pgbench_hist_mtime on pgbench_history(mtime); CREATE INDEX h. View the explain plan output to see that QPM detects a new plan but still uses the approved plan and maintains the plan stability. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. ***Note: For this example, an approved plan was used instead of the minimum cost plan. SQL Hash: 1561242727, Plan Hash: -1990695905, Minimum Cost Plan Hash: -794604077*** i. Run the following SQL query to view the new plan and status of the plan. To ensure plan stability, QPM stores all the newly generated plans for a managed query in QPM as unapproved plans. The following output shows that there are two different execution plans stored for the same managed statement, as shown by the two different plan_hash values. Although the new execution plan has the minimum cost (lower than the approved plan), QPM continues to ignore the unapproved plans to maintain plan stability. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For the sake of brevity, the plan_outline is not shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+---------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 UnApproved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); The following is an example of plan adaptability with QPM. This example evaluates the unapproved plan based on the minimum speedup factor. It approves any captured unapproved plan that is faster by at least 10 percent than the best approved plan for the statement. For additional details, see Evaluating Plan Performance in the Aurora documentation . mylab=> SELECT apg_plan_mgmt.Evolve_plan_baselines (sql_hash, plan_hash, 1.1,'approve') FROM apg_plan_mgmt.dba_plans WHERE status = 'Unapproved'; NOTICE: [Unapproved] SQL Hash: 1561242727, Plan Hash: 1944377599, SELECT sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where ... NOTICE: Baseline [Planning time 0.693 ms, Execution time 316.644 ms] NOTICE: Baseline+1 [Planning time 0.695 ms, Execution time 213.919 ms] NOTICE: Total time benefit: 102.723 ms, Execution time benefit: 102.725 ms, Avg Log Cardinality Error: 3.53418, Cost = 111.16..111.17 NOTICE: Unapproved -> Approved After QPM evaluates the plan based on the speed factor, the plan status changes to approved. At this point, the optimizer can choose that plan for that managed statement now. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 Approved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); j. View the explain plan output to see whether the query is using the newly approved minimum cost plan. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ----------------------------------------------------------------------------- Aggregate (cost=111.16..111.17 rows=1 width=16) InitPlan 2 (returns $1) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 1 (returns $0) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan using pgbench_hist_mtime on pgbench_history mn (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) InitPlan 4 (returns $3) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 3 (returns $2) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan Backward using pgbench_hist_mtime on pgbench_history mx (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) -> Hash Join (cost=14.60..107.06 rows=632 width=8) Hash Cond: (h.bid = b.bid) -> Index Scan using pgbench_hist_mtime on pgbench_history h (cost=0.42..85.01 rows=2107 width=8) Index Cond: ((mtime >= $1) AND (mtime <= $3)) -> Hash (cost=14.14..14.14 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) SQL Hash: 1561242727, Plan Hash: -794604077","title":"4. QPM Plan adaptability with plan evolution mechanism"},{"location":"modules/qpm/#5-fixing-plans-with-qpm-using-pg_hint_plan","text":"In some cases, the query optimizer doesn\u2019t generate the best execution plan for the query. One approach to fixing this problem is to put query hints into your application code, but this approach is widely discouraged because it makes applications more brittle and harder to maintain, and in some cases, you can\u2019t hint the SQL because it is generated by a 3rd party application. What we will show is how to use hints to control the query optimizer, but then to remove the hints and allow QPM to enforce the desired plan, without adding hints to the application code. For this purpose, PostgreSQL users can use the pg_hint_plan extension to provide directives such as \u201cscan method,\u201d \u201cjoin method,\u201d \u201cjoin order,\u201d, or \u201crow number correction,\u201d to the optimizer. The resulting plan will be saved by QPM, along with any GUC parameters you choose to override (such as work_mem). QPM remembers any GUC parameter overrides and uses them when it needs to recreate the plan. To install and learn more about how to use the pg_hint_plan extension, see the pg_hint_plan documentation. QPM steps to fix the plan generated by using hints Working with pg_hint_plan is incredibly useful for cases where the query can\u2019t be modified to add hints. In this example, use a sample query to generate the execution plan that you want by adding hints to the managed statement. Then associate this execution plan with the original unmodified statement. Here are the detailed steps: a. Check if the plan capture is disabled mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- off (1 row) b. Run the query with the hint to use. In the following example, use the \u201cHashJoin\u201d hint, which is a directive for the optimizer to choose the join method as HashJoin. The original plan of the query without hints is as follows. mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Nested Loop (cost=0.42..181906.82 rows=1000000 width=465) Join Filter: (b.bid = a.bid) -> Index Scan using pgbench_accounts_pkey on pgbench_accounts a (cost=0.42..44165.43 rows=1000000 width=97) -> Materialize (cost=0.00..14.15 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: -451962956 c. Enable pg_hint_plan and manual plan capture: Mylab=> SET pg_hint_plan.enable_hint = true; SET mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET d. EXPLAIN the query with the hints you want to use. In the following example, use the HashJoin (a, b) hint, which is a directive for the optimizer to use a hash join algorithm to join from table a to table b: The plan that you want with a hash join is as follows. mylab=> /*+ HashJoin(a b) */ EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..250138.04 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: 1139293728 e. Verify that plan 1139293728 was captured, and note the status of the plan View the captured plan and the status of the plan. mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; Where plan_hash=1139293728; sql_hash | plan_hash | status | enabled | sql_text -----------+------------+----------+---------+--------------------------- 356104612 | 1139293728 | Approved | t | SELECT + | | | | * + | | | | FROM + | | | | pgbench_branches b + | | | | JOIN + | | | | pgbench_accounts a + | | | | ON b.bid = a.bid + | | | | ORDER BY + | | | | a.aid; f. If necessary, Approve the plan In this case this is the first and only plan saved for statement 356104612, so it was saved as an Approved plan. If this statement already had a baseline of approved plans, then this plan would have been saved as an Unapproved plan. In general, to Reject all existing plans for a statement and then Approve one specific plan, you could call apg_plan_mgmt.set_plan_status twice, like this: mylab=> SELECT apg_plan_mgmt.set_plan_status (sql_hash, plan_hash, 'Rejected') from apg_plan_mgmt.dba_plans where sql_hash = 356104612; SET mylab=> SELECT apg_plan_mgmt.set_plan_status (356104612, 1139293728, 'Approved'); SET g. Remove the hint, turn off manual capture, turn on use_plan_baselines, and also verify that the desired plan is in use without the hint. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..337638.11 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) Note: An Approved plan was used instead of the minimum cost plan. SQL Hash: 356104612, Plan Hash: 1139293728, Minimum Cost Plan Hash: -451962956","title":"5. Fixing plans with QPM using pg_hint_plan"},{"location":"modules/qpm/#6-deploying-qpm-managed-plans-globally-using-export-and-import-no-lab","text":"Large enterprise customers often have applications and databases deployed globally. They also often maintain several environments (Dev, QA, Staging, UAT, Preprod, and Prod) for each application database. However, managing the execution plans manually in each of the databases in specific AWS Regions and each of the database environments can be cumbersome and time-consuming. QPM provides an option to export and import QPM-managed plans from one database to another database. With this option, you can manage the query execution centrally and deploy databases globally. This feature is useful for the scenarios where you investigate a set of plans on a preprod database, verify that they perform well, and then load them into a production database. Here are the steps to migrate QPM-managed plans from one database to another. For additional details, see Exporting and Importing Plans in the Aurora documentation. Export the QPM-managed plan from the source system. To do this, from the source database with the preferred execution plan, an authorized DB user can copy any subset of the apg_plan_mgmt.plans table to another table. That user can then save it using the pg_dump command. For additional details on the pg_dump, see pg_dump in the PostgreSQL documentation. Import the QPM-managed plan on the target system. On the target system, copy any subset of the apg_plan_mgmt.plans table to another table, and then save it using the pg_dump command. This is an optional step to preserve any existing managed plans on the target system before importing new plans for the source system. We have assumed that you have used pg_dump tar-format archive in step 1. Use the pg_restore command to copy the .tar file into a new table (plan_copy). For additional details about the pg_restore, see pg_restore in the PostgreSQL documentation. Merge the new table with the apg_plan_mgmt.plans table. Reload the managed plans into shared memory and remove the temporary plans table. SELECT apg_plan_mgmt.reload(); -- Refresh shared memory with new plans. DROP TABLE plans_copy; -- Drop the temporary plan table.","title":"6. Deploying QPM-managed plans globally using export and import (no lab)"},{"location":"modules/qpm/#7-disabling-qpm-and-deleting-plans-manually","text":"Disable the QPM Open your cluster-level parameter group and set the rds.enable_plan_management parameter to 0 Delete all the plans captured SELECT SUM(apg_plan_mgmt.delete_plan(sql_hash, plan_hash)) FROM apg_plan_mgmt.dba_plans; 3.Helpful SQL statements SELECT sql_hash,plan_hash,status,enabled,sql_text FROM apg_plan_mgmt.dba_plans; SELECT sql_hash,plan_hash,status,estimated_total_cost \"cost\",sql_text FROM apg_plan_mgmt.dba_plans;","title":"7. Disabling QPM and deleting plans manually"}]}