{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon Aurora Labs for PostgreSQL \u00b6 Welcome to the AWS workshop and lab content portal for Amazon Aurora PostgreSQL compatible databases! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the Amazon Aurora features and capabilities. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. These resources are focused on helping you discover how advanced features of the Amazon Aurora PostgreSQL database operate. Prior expertise with AWS and PostgreSQL-based databases is beneficial, but not required to complete the labs.","title":"Home"},{"location":"#amazon-aurora-labs-for-postgresql","text":"Welcome to the AWS workshop and lab content portal for Amazon Aurora PostgreSQL compatible databases! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of the Amazon Aurora features and capabilities. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. These resources are focused on helping you discover how advanced features of the Amazon Aurora PostgreSQL database operate. Prior expertise with AWS and PostgreSQL-based databases is beneficial, but not required to complete the labs.","title":"Amazon Aurora Labs for PostgreSQL"},{"location":"contribute/","text":"Contributing Guidelines \u00b6 Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution. Reporting Bugs/Feature Requests \u00b6 We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment Contributing via Pull Requests \u00b6 Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request . Finding contributions to work on \u00b6 Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start. Code of Conduct \u00b6 This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments. Security issue notifications \u00b6 If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue. Licensing \u00b6 See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Contributing"},{"location":"contribute/#contributing-guidelines","text":"Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community. Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.","title":"Contributing Guidelines"},{"location":"contribute/#reporting-bugsfeature-requests","text":"We welcome you to use the GitHub issue tracker to report bugs or suggest features. When filing an issue, please check existing open , or recently closed , issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful: A reproducible test case or series of steps The version of our code being used Any modifications you've made relevant to the bug Anything unusual about your environment or deployment","title":"Reporting Bugs/Feature Requests"},{"location":"contribute/#contributing-via-pull-requests","text":"Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that: You are working against the latest source on the master branch. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already. You open an issue to discuss any significant work - we would hate for your time to be wasted. To send us a pull request, please: Fork the repository. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change. Ensure local tests pass. Commit to your fork using clear commit messages. Send us a pull request, answering any default questions in the pull request interface. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation. GitHub provides additional document on forking a repository and creating a pull request .","title":"Contributing via Pull Requests"},{"location":"contribute/#finding-contributions-to-work-on","text":"Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.","title":"Finding contributions to work on"},{"location":"contribute/#code-of-conduct","text":"This project has adopted the Amazon Open Source Code of Conduct . For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.","title":"Code of Conduct"},{"location":"contribute/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page . Please do not create a public github issue.","title":"Security issue notifications"},{"location":"contribute/#licensing","text":"See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution. We may ask you to sign a Contributor License Agreement (CLA) for larger changes.","title":"Licensing"},{"location":"license/","text":"License \u00b6 MIT License Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#license","text":"MIT License Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"modules/","text":"Getting Started \u00b6 Create an AWS account \u00b6 In order to complete the hands-on content on this site, you'll need an AWS Account. We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. If you are setting up an AWS account for the first time, follow the instructions below to create an administrative IAM user account , we recommend not using your AWS account root credentials for day to day usage. If you have received credits to complete these labs follow the instructions below on adding the credits to your AWS account. Overview of labs \u00b6 The following labs are currently available, part of this instructional website: # Lab Module Recommendation Overview 1 Prerequisites Required, start here Set up the lab environment and provision the prerequisite resources 2 Create a New Aurora Cluster Optional Create a new Amazon Aurora PostgreSQL DB cluster manually 3 Connecting, Loading Data and Auto Scaling Recommended, for provisioned clusters Connect to the DB cluster for the first time, load an initial data set and test read replica auto scaling. The initial data set may be used in subsequent labs. 4 Cloning Clusters Recommended, for provisioned clusters Cloning an Aurora DB cluster and observing the divergence of the data set. 5 Backtracking a Cluster Recommended, for provisioned clusters Backtracking an Aurora DB cluster to fix an accidental DDL operation. 6 Using Performance Insights Recommended, for provisioned clusters Examining the performance of your DB instances using RDS Performance Insights 7 Creating a Serverless Aurora Cluster Recommended, for serverless clusters Create a new Amazon Aurora Serverless PostgreSQL DB cluster manually. You may skip the provisioned cluster labs if you are planning to operate a serverless workload. 8 Using Aurora Serverless with Lambda Functions Recommended, for serverless clusters Connect to your serverless cluster using the RDS Data API and Lambda functions. Requires the previous lab. Lab environment at a glance \u00b6 To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. The environment deployed using CloudFormation includes several components: Amazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the cluster and workstation Amazon EC2 instance configured with the software components needed for the lab IAM roles with access permissions for the workstation and cluster permissions for enhanced monitoring , S3 access and logging Custom cluster and DB instance parameter groups for the Amazon Aurora cluster, enabling logging and performance schema Optionally, Amazon Aurora DB cluster with 2 nodes: a writer and read replica If the cluster is created for you, the master database credentials will be generated automatically and stored in an AWS Secrets Manager secret. Optionally, read replica auto scaling configuration Optionally, AWS Systems Manager command document to execute a load test Create an IAM user (with admin permissions) \u00b6 If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess policy, and click Next:Review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created. Add credits (optional) \u00b6 If you are doing these workshop as part of an AWS sponsored event that doesn't provide AWS accounts, you will receive credits to cover the costs. Below are the instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem. Additional software needed for labs \u00b6 The templates and scripts setting up the lab environment install the following software in the lab environment for the purposes of deploying and running the labs: mysql-client package. PostgreSQL open source software is provided under the GPL License. sysbench available using the GPL License. test_db available using the Creative Commons Attribution-Share Alike 3.0 Unported License. Percona's sysbench-tpcc available using the Apache License 2.0.","title":"Getting Started"},{"location":"modules/#getting-started","text":"","title":"Getting Started"},{"location":"modules/#create-an-aws-account","text":"In order to complete the hands-on content on this site, you'll need an AWS Account. We strongly recommend that you use a personal account or create a new AWS account to ensure you have the necessary access and that you do not accidentally modify corporate resources. Do not use an AWS account from the company you work for unless they provide sandbox accounts just for this purpose. If you are setting up an AWS account for the first time, follow the instructions below to create an administrative IAM user account , we recommend not using your AWS account root credentials for day to day usage. If you have received credits to complete these labs follow the instructions below on adding the credits to your AWS account.","title":"Create an AWS account"},{"location":"modules/#overview-of-labs","text":"The following labs are currently available, part of this instructional website: # Lab Module Recommendation Overview 1 Prerequisites Required, start here Set up the lab environment and provision the prerequisite resources 2 Create a New Aurora Cluster Optional Create a new Amazon Aurora PostgreSQL DB cluster manually 3 Connecting, Loading Data and Auto Scaling Recommended, for provisioned clusters Connect to the DB cluster for the first time, load an initial data set and test read replica auto scaling. The initial data set may be used in subsequent labs. 4 Cloning Clusters Recommended, for provisioned clusters Cloning an Aurora DB cluster and observing the divergence of the data set. 5 Backtracking a Cluster Recommended, for provisioned clusters Backtracking an Aurora DB cluster to fix an accidental DDL operation. 6 Using Performance Insights Recommended, for provisioned clusters Examining the performance of your DB instances using RDS Performance Insights 7 Creating a Serverless Aurora Cluster Recommended, for serverless clusters Create a new Amazon Aurora Serverless PostgreSQL DB cluster manually. You may skip the provisioned cluster labs if you are planning to operate a serverless workload. 8 Using Aurora Serverless with Lambda Functions Recommended, for serverless clusters Connect to your serverless cluster using the RDS Data API and Lambda functions. Requires the previous lab.","title":"Overview of labs"},{"location":"modules/#lab-environment-at-a-glance","text":"To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. The environment deployed using CloudFormation includes several components: Amazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the cluster and workstation Amazon EC2 instance configured with the software components needed for the lab IAM roles with access permissions for the workstation and cluster permissions for enhanced monitoring , S3 access and logging Custom cluster and DB instance parameter groups for the Amazon Aurora cluster, enabling logging and performance schema Optionally, Amazon Aurora DB cluster with 2 nodes: a writer and read replica If the cluster is created for you, the master database credentials will be generated automatically and stored in an AWS Secrets Manager secret. Optionally, read replica auto scaling configuration Optionally, AWS Systems Manager command document to execute a load test","title":"Lab environment at a glance"},{"location":"modules/#create-an-iam-user-with-admin-permissions","text":"If you don't already have an AWS IAM user with admin permissions, please use the following instructions to create one: Browse to the AWS IAM console. Click Users on the left navigation and then click Add User . Enter a User Name , check the checkbox for AWS Management Console access , enter a Custom Password , and click Next:Permissions . Click Attach existing policies directly , click the checkbox next to the AdministratorAccess policy, and click Next:Review . Click Create User Click Dashboard on the left navigation and use the IAM users sign-in link to login as the admin user you just created.","title":"Create an IAM user (with admin permissions)"},{"location":"modules/#add-credits-optional","text":"If you are doing these workshop as part of an AWS sponsored event that doesn't provide AWS accounts, you will receive credits to cover the costs. Below are the instructions for entering the credits: Browse to the AWS Account Settings console. Enter the Promo Code you received (these will be handed out at the beginning of the workshop). Enter the Security Check and click Redeem.","title":"Add credits (optional)"},{"location":"modules/#additional-software-needed-for-labs","text":"The templates and scripts setting up the lab environment install the following software in the lab environment for the purposes of deploying and running the labs: mysql-client package. PostgreSQL open source software is provided under the GPL License. sysbench available using the GPL License. test_db available using the Creative Commons Attribution-Share Alike 3.0 Unported License. Percona's sysbench-tpcc available using the Apache License 2.0.","title":"Additional software needed for labs"},{"location":"modules/create/","text":"Creating a New Aurora Cluster \u00b6 Note If you are familiar with the basic concepts of Amazon Aurora MySQL, and have created a cluster in the past, you may skip this module, by using provisioning the lab environment using the lab-with-cluster.yml CloudFormation template, so the DB cluster is provisioned for you. Skip to Connecting, Loading Data and Auto Scaling . This lab will walk you through the steps of creating an Amazon Aurora database cluster manually, and configuring app the parameters required for the cluster components. At the end of this lab you will have a database cluster ready to be used in subsequent labs. This lab contains the following tasks: Creating the DB cluster Retrieving the DB cluster endpoints Assigning an IAM role to the DB cluster Creating a replica auto scaling policy This lab requires the following lab modules to be completed first: Prerequisites (using lab-no-cluster-pgsql.yml template is sufficient) 1. Creating the DB cluster \u00b6 Open the Amazon RDS service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create database to start the configuration process Note The RDS console database creation workflow has been simplified recently. Depending on your previous usage of the RDS console UI, you may see the old workflow or the new one, you may also be presented with a prompt to toggle between them. In this lab we are using the new workflow for reference, although the steps will work similarly in the old console workflow as well, if you are more familiar with it. In the first configuration section of the Create database page, called Database settings ensure the Easy create toggle button is turned OFF (grey color). Next, in the Engine options section, choose the Amazon Aurora engine type, the Amazon Aurora with PostgreSQL compatibility edition, the Aurora (PostgreSQL 10.7) version. In the Database features section, select One writer and multiple readers , and in the Templates section, select Production . The selections so far will instruct AWS to create an Aurora PostgreSQL database cluster with the most recent version of the PostgreSQL 10.7 compatible engine in a highly available configuration with one writer and one reader database instance in the cluster. In the Settings section give your database cluster a recognizable identifier, such as labstack-cluster . Configure the name and password of the master database user, with the most elevated permissions in the database. We recommend to use the user name masteruser for consistency with subsequent labs and a password of your choosing. For simplicity ensure the check box Auto generate a password is not checked . In the Connectivity section, expand the sub-section called Additional connectivity configuration . This section allows you to specify where the database cluster will be deployed within your defined network configuration. To simplify the labs, the CloudFormation stack you deployed in the preceding Prerequisites module, has configured a VPC that includes all resources needed for an Aurora database cluster. This includes the VPC itself, subnets, DB subnet groups, security groups and several other networking constructs. All you need to do is select the appropriate existing connectivity controls in this section. Pick the Virtual Private Cloud (VPC) named after the CloudFormation stack name, such as labstack-vpc . Similarly make sure the selected Subnet Group also matches the stack name (e.g. labstack-dbsubnets-[hash] ). Make sure the cluster Publicly accessible option is set to No . The lab environment also configured a VPC security group that allows your lab workspace EC2 instance to connect to the database. Make sure the Choose existing security group option is selected and from the dropdown pick the security group with a name ending in -pgsql-internal (eg. labstack-pgsql-internal ). Please remove any other security groups, such as default from the selection. Next, expand the Additional configuration section. Set the Initial database name to mylab . For the DB cluster parameter group and DB parameter group selectors, choose the groups with the stack name in their name (e.g. labstack-[...] ). Choose a 7 days Backup retention period . Check the box to Enable encryption and select the [default] aws/rds for the Master key . Check the box to Enable Performance Insights with a Retention period of Default (7 days) and use the [default] aws/rds Master key for monitoring data encryption. Next, check the Enable Enhanced Monitoring box, and select a Granularity of 1 second . For Log exports check the Postgresql log** boxes. Also in the Advanced configuration section, de-select the check box Enable delete protection . In a production use case, you will want to leave that option checked, but for testing purposes, un-checking this option will make it easier to clean up the resources once you have completed the labs. Before continuing, let's summarize the configuration options selected. You will create a database cluster with the following characteristics: Aurora PostgreSQL 10.7 compatible (latest stable engine version) Cluster composed of a writer and a reader DB instance in different availability zones (highly available) Deployed in the VPC and using the network configuration of the lab environment Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption With Enhanced Monitoring and Performance Insights enabled Click Create database to provision the DB cluster. 2. Retrieving the DB cluster endpoints \u00b6 The database cluster may take several minutes to provision, including the DB instances making up the cluster. In order to connect to the DB cluster and start using it in subsequent labs, you need to retrieve the DB cluster endpoints. There are two endpoints created by default. The Cluster Endpoint will always point to the writer DB instance of the cluster, and should be used for both writes and reads. The Reader Endpoint will always resolve to one of the reader DB instances and should be used to offload read operations to read replicas. In the RDS console, go to the DB cluster detail view by clicking on the cluster DB identifier. The Endpoints section in the Connectivity and security tab of the details page displays the endpoints. Note these values down, as you will use them later. 3. Assigning an IAM role to the DB cluster \u00b6 Once created, you should assign an IAM role to the DB cluster, in order to allow the cluster access to Amazon S3 for importing and exporting data. The IAM role has already been created using CloudFormation when you created the lab environment. On the same DB cluster detail page as before, in the Manage IAM roles section, choose the IAM role named after the stack name, ending in -integrate-[region] (e.g. labstack-integrate-[region] ). Then click Add role . Once the operation completes the Status of the role will change from Pending to Active . 4. Creating a replica auto scaling policy \u00b6 Finally, you will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load. In the top right corner of the details page, click on Actions and then on Add replica auto scaling . Provide a Policy name based on the stack name, such as labstack-autoscale-readers . For the Target metric choose Average CPU utilization of Aurora Replicas . Enter a Target value of 20 percent. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes. Next, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs. In the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2 . In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost of associated with the labs we limit the number of readers to two. Next click Add policy .","title":"Creating a New Aurora Cluster"},{"location":"modules/create/#creating-a-new-aurora-cluster","text":"Note If you are familiar with the basic concepts of Amazon Aurora MySQL, and have created a cluster in the past, you may skip this module, by using provisioning the lab environment using the lab-with-cluster.yml CloudFormation template, so the DB cluster is provisioned for you. Skip to Connecting, Loading Data and Auto Scaling . This lab will walk you through the steps of creating an Amazon Aurora database cluster manually, and configuring app the parameters required for the cluster components. At the end of this lab you will have a database cluster ready to be used in subsequent labs. This lab contains the following tasks: Creating the DB cluster Retrieving the DB cluster endpoints Assigning an IAM role to the DB cluster Creating a replica auto scaling policy This lab requires the following lab modules to be completed first: Prerequisites (using lab-no-cluster-pgsql.yml template is sufficient)","title":"Creating a New Aurora Cluster"},{"location":"modules/create/#1-creating-the-db-cluster","text":"Open the Amazon RDS service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create database to start the configuration process Note The RDS console database creation workflow has been simplified recently. Depending on your previous usage of the RDS console UI, you may see the old workflow or the new one, you may also be presented with a prompt to toggle between them. In this lab we are using the new workflow for reference, although the steps will work similarly in the old console workflow as well, if you are more familiar with it. In the first configuration section of the Create database page, called Database settings ensure the Easy create toggle button is turned OFF (grey color). Next, in the Engine options section, choose the Amazon Aurora engine type, the Amazon Aurora with PostgreSQL compatibility edition, the Aurora (PostgreSQL 10.7) version. In the Database features section, select One writer and multiple readers , and in the Templates section, select Production . The selections so far will instruct AWS to create an Aurora PostgreSQL database cluster with the most recent version of the PostgreSQL 10.7 compatible engine in a highly available configuration with one writer and one reader database instance in the cluster. In the Settings section give your database cluster a recognizable identifier, such as labstack-cluster . Configure the name and password of the master database user, with the most elevated permissions in the database. We recommend to use the user name masteruser for consistency with subsequent labs and a password of your choosing. For simplicity ensure the check box Auto generate a password is not checked . In the Connectivity section, expand the sub-section called Additional connectivity configuration . This section allows you to specify where the database cluster will be deployed within your defined network configuration. To simplify the labs, the CloudFormation stack you deployed in the preceding Prerequisites module, has configured a VPC that includes all resources needed for an Aurora database cluster. This includes the VPC itself, subnets, DB subnet groups, security groups and several other networking constructs. All you need to do is select the appropriate existing connectivity controls in this section. Pick the Virtual Private Cloud (VPC) named after the CloudFormation stack name, such as labstack-vpc . Similarly make sure the selected Subnet Group also matches the stack name (e.g. labstack-dbsubnets-[hash] ). Make sure the cluster Publicly accessible option is set to No . The lab environment also configured a VPC security group that allows your lab workspace EC2 instance to connect to the database. Make sure the Choose existing security group option is selected and from the dropdown pick the security group with a name ending in -pgsql-internal (eg. labstack-pgsql-internal ). Please remove any other security groups, such as default from the selection. Next, expand the Additional configuration section. Set the Initial database name to mylab . For the DB cluster parameter group and DB parameter group selectors, choose the groups with the stack name in their name (e.g. labstack-[...] ). Choose a 7 days Backup retention period . Check the box to Enable encryption and select the [default] aws/rds for the Master key . Check the box to Enable Performance Insights with a Retention period of Default (7 days) and use the [default] aws/rds Master key for monitoring data encryption. Next, check the Enable Enhanced Monitoring box, and select a Granularity of 1 second . For Log exports check the Postgresql log** boxes. Also in the Advanced configuration section, de-select the check box Enable delete protection . In a production use case, you will want to leave that option checked, but for testing purposes, un-checking this option will make it easier to clean up the resources once you have completed the labs. Before continuing, let's summarize the configuration options selected. You will create a database cluster with the following characteristics: Aurora PostgreSQL 10.7 compatible (latest stable engine version) Cluster composed of a writer and a reader DB instance in different availability zones (highly available) Deployed in the VPC and using the network configuration of the lab environment Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption With Enhanced Monitoring and Performance Insights enabled Click Create database to provision the DB cluster.","title":"1. Creating the DB cluster"},{"location":"modules/create/#2-retrieving-the-db-cluster-endpoints","text":"The database cluster may take several minutes to provision, including the DB instances making up the cluster. In order to connect to the DB cluster and start using it in subsequent labs, you need to retrieve the DB cluster endpoints. There are two endpoints created by default. The Cluster Endpoint will always point to the writer DB instance of the cluster, and should be used for both writes and reads. The Reader Endpoint will always resolve to one of the reader DB instances and should be used to offload read operations to read replicas. In the RDS console, go to the DB cluster detail view by clicking on the cluster DB identifier. The Endpoints section in the Connectivity and security tab of the details page displays the endpoints. Note these values down, as you will use them later.","title":"2. Retrieving the DB cluster endpoints"},{"location":"modules/create/#3-assigning-an-iam-role-to-the-db-cluster","text":"Once created, you should assign an IAM role to the DB cluster, in order to allow the cluster access to Amazon S3 for importing and exporting data. The IAM role has already been created using CloudFormation when you created the lab environment. On the same DB cluster detail page as before, in the Manage IAM roles section, choose the IAM role named after the stack name, ending in -integrate-[region] (e.g. labstack-integrate-[region] ). Then click Add role . Once the operation completes the Status of the role will change from Pending to Active .","title":"3. Assigning an IAM role to the DB cluster"},{"location":"modules/create/#4-creating-a-replica-auto-scaling-policy","text":"Finally, you will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load. In the top right corner of the details page, click on Actions and then on Add replica auto scaling . Provide a Policy name based on the stack name, such as labstack-autoscale-readers . For the Target metric choose Average CPU utilization of Aurora Replicas . Enter a Target value of 20 percent. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes. Next, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs. In the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2 . In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost of associated with the labs we limit the number of readers to two. Next click Add policy .","title":"4. Creating a replica auto scaling policy"},{"location":"modules/perf-insights/","text":"Using Performance Insights \u00b6 This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance. This lab contains the following tasks: Generating load on your DB cluster Understanding the Performance Insights interface Examining the performance of your DB instance This lab requires the following lab modules to be completed first: Prerequisites Creating a New Aurora Cluster (conditional, if creating a cluster manually) Connecting, Loading Data and Auto Scaling (connectivity section only) 1. Generating load on your DB cluster \u00b6 You will use Percona's TPCC-like benchmark script based on sysbench to generate load. For simplicity we have packaged the correct set of commands in an AWS Systems Manager Command Document . You will use AWS Systems Manager Run Command to execute the test. On the Session Manager workstation command line see the Connecting, Loading Data and Auto Scaling lab , enter one of the following commands. If you have completed the Creating a New Aurora Cluster lab, and created the Aurora DB cluster manually execute this command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] \\ --parameters \\ clusterEndpoint=[clusterEndpoint],\\ dbUser=$DBUSER,\\ dbPassword=\"$DBPASS\" If AWS CloudFormation has provisioned the DB cluster on your behalf, and you skipped the Creating a New Aurora Cluster lab, you can run this simplified command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] Command parameter values at a glance: Parameter Parameter Placeholder Value DB cluster provisioned by CloudFormation Value DB cluster configured manually Description --document-name [loadTestRunDoc] See CloudFormation stack output See CloudFormation stack output The name of the command document to run on your behalf. --instance-ids [bastionInstance] See CloudFormation stack output See CloudFormation stack output The EC2 instance to execute this command on. --parameters clusterEndpoint=[clusterEndpoint],dbUser=$DBUSER,dbPassword=\"$DBPASS\" N/A Substitute the DB cluster endpoint with the values configured manually Additional command parameters. The command will be sent to the workstation EC2 instance which will prepare the test data set and run the load test. It may take up to a minute for CloudWatch to reflect the additional load in the metrics. You will see a confirmation that the command has been initiated. 2. Understanding the Performance Insights interface \u00b6 While the command is running, open the Amazon RDS service console in a new tab, if not already open. Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. In the menu on the left hand side, click on the Performance Insights menu option. Next, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. As the different Db instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab we are generating load on the Writer (master) DB instance only. Select the DB instance where the name either ends in -node-01 or -instance-1 Once a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into 3 sections, allowing you to drill down from high level performance indicator metrics down to individual queries, waits, users and hosts generating the load. The performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the buttons across the top right of the interface ( 5m , 1h , 5h , 24h , 1w , all ). You can also zoom into a specific period of time by dragging across the graphs. Note All dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom. Section Filters Description Counter Metrics Click cog icon in top right corner to select additional counters This section plots internal database counter metrics over time, such as number of rows read or written, buffer pool hit ratio, etc. These counters are useful to correlate with other metrics, including the database load metrics, to identify causes of abnormal behavior. Database load Load can be sliced by waits (default), SQL commands, users and hosts This metric is design to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits , SQL (default), Users and Hosts Drill down capability that allows you to get detailed performance data down to the individual commands. 3. Examining the performance of your DB instance \u00b6 After running the load generator workload above, you will see a performance profile similar to the example below in the Performance Insights dashboard. The load generator command will first create an initial data set using sysbench prepare . And then will run an OLTP workload for the duration of 5 minutes, consisting of concurrent transactional reads and writes using 4 parallel threads. Amazon Aurora MySQL specific wait events are documented in the Amazon Aurora MySQL Reference guide . Use the Performance Insights dashboard and the reference guide documentation to evaluate the workload profile of your load test, and answer the following questions: Is the database server overloaded at any point during the load test? Can you identify any resource bottlenecks during the load test? If so how can they be mitigated? What are the most common wait events during the load test? Why are the load patterns different between the first and second phase of the load test?","title":"Using Performance Insights"},{"location":"modules/perf-insights/#using-performance-insights","text":"This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance. This lab contains the following tasks: Generating load on your DB cluster Understanding the Performance Insights interface Examining the performance of your DB instance This lab requires the following lab modules to be completed first: Prerequisites Creating a New Aurora Cluster (conditional, if creating a cluster manually) Connecting, Loading Data and Auto Scaling (connectivity section only)","title":"Using Performance Insights"},{"location":"modules/perf-insights/#1-generating-load-on-your-db-cluster","text":"You will use Percona's TPCC-like benchmark script based on sysbench to generate load. For simplicity we have packaged the correct set of commands in an AWS Systems Manager Command Document . You will use AWS Systems Manager Run Command to execute the test. On the Session Manager workstation command line see the Connecting, Loading Data and Auto Scaling lab , enter one of the following commands. If you have completed the Creating a New Aurora Cluster lab, and created the Aurora DB cluster manually execute this command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] \\ --parameters \\ clusterEndpoint=[clusterEndpoint],\\ dbUser=$DBUSER,\\ dbPassword=\"$DBPASS\" If AWS CloudFormation has provisioned the DB cluster on your behalf, and you skipped the Creating a New Aurora Cluster lab, you can run this simplified command: aws ssm send-command \\ --document-name [loadTestRunDoc] \\ --instance-ids [bastionInstance] Command parameter values at a glance: Parameter Parameter Placeholder Value DB cluster provisioned by CloudFormation Value DB cluster configured manually Description --document-name [loadTestRunDoc] See CloudFormation stack output See CloudFormation stack output The name of the command document to run on your behalf. --instance-ids [bastionInstance] See CloudFormation stack output See CloudFormation stack output The EC2 instance to execute this command on. --parameters clusterEndpoint=[clusterEndpoint],dbUser=$DBUSER,dbPassword=\"$DBPASS\" N/A Substitute the DB cluster endpoint with the values configured manually Additional command parameters. The command will be sent to the workstation EC2 instance which will prepare the test data set and run the load test. It may take up to a minute for CloudWatch to reflect the additional load in the metrics. You will see a confirmation that the command has been initiated.","title":"1. Generating load on your DB cluster"},{"location":"modules/perf-insights/#2-understanding-the-performance-insights-interface","text":"While the command is running, open the Amazon RDS service console in a new tab, if not already open. Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. In the menu on the left hand side, click on the Performance Insights menu option. Next, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. As the different Db instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab we are generating load on the Writer (master) DB instance only. Select the DB instance where the name either ends in -node-01 or -instance-1 Once a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into 3 sections, allowing you to drill down from high level performance indicator metrics down to individual queries, waits, users and hosts generating the load. The performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the buttons across the top right of the interface ( 5m , 1h , 5h , 24h , 1w , all ). You can also zoom into a specific period of time by dragging across the graphs. Note All dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom. Section Filters Description Counter Metrics Click cog icon in top right corner to select additional counters This section plots internal database counter metrics over time, such as number of rows read or written, buffer pool hit ratio, etc. These counters are useful to correlate with other metrics, including the database load metrics, to identify causes of abnormal behavior. Database load Load can be sliced by waits (default), SQL commands, users and hosts This metric is design to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits , SQL (default), Users and Hosts Drill down capability that allows you to get detailed performance data down to the individual commands.","title":"2. Understanding the Performance Insights interface"},{"location":"modules/perf-insights/#3-examining-the-performance-of-your-db-instance","text":"After running the load generator workload above, you will see a performance profile similar to the example below in the Performance Insights dashboard. The load generator command will first create an initial data set using sysbench prepare . And then will run an OLTP workload for the duration of 5 minutes, consisting of concurrent transactional reads and writes using 4 parallel threads. Amazon Aurora MySQL specific wait events are documented in the Amazon Aurora MySQL Reference guide . Use the Performance Insights dashboard and the reference guide documentation to evaluate the workload profile of your load test, and answer the following questions: Is the database server overloaded at any point during the load test? Can you identify any resource bottlenecks during the load test? If so how can they be mitigated? What are the most common wait events during the load test? Why are the load patterns different between the first and second phase of the load test?","title":"3. Examining the performance of your DB instance"},{"location":"modules/prerequisites/","text":"Prerequisites \u00b6 The following steps should be completed before getting started with any of the labs in this repository. Not all steps may apply to all students or environments. This lab contains the following tasks: Signing in to the AWS Management Console Creating a lab environment using AWS CloudFormation 1. Signing in to the AWS Management Console \u00b6 If you are running these labs in a formal, instructional setting, please use the Console URL, and credentials provided to you to access and log into the AWS Management Console. Otherwise, please use your own credentials. You can access the console at: https://console.aws.amazon.com/ or through the Single Sign-On (SSO) mechanism provided by your organization. If you are running these labs in a formal, instructional setting, please use the AWS region provided. Ensure the correct AWS region is selected in the top right corner, if not use that dropdown to choose the correct region. The labs are designed to work in any of the regions where Amazon Aurora PostgreSQL compatible is available. However, not all features and capabilities of Amazon Aurora may be available in all supported regions at this time. 2. Creating a lab environment using AWS CloudFormation \u00b6 To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. Formal Event If you are running these labs in a formal instructional event, the lab environment may have been initialized on your behalf. If unsure, please verify with the event support staff. Please download the most appropriate CloudFormation template based on the labs you want to run: Option Download Template One-Click Launch Description I will create the DB cluster manually lab-no-cluster-pgsql.yml Use when you wish to provision the initial cluster manually by following Lab 1. - Creating a New Aurora Cluster Provision the DB cluster for me lab-with-cluster-pgsql.yml Use when you wish to skip the initial cluster creation lab, and have the DB cluster provisioned for you, so you can continue from Lab 2. - Cluster Endpoints and Read Replica Auto Scaling If you downloaded the template, save it in a memorable location such as your desktop, you will need to reference it later. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack . For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options page, leave the defaults as they are, scroll to the bottom and click Next . On the Review labstack page, scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create . The stack will take approximatively 20 minutes to provision, you can monitor the status on the Stack detail page. You can monitor the progress of the stack creation process by refreshing the Events tab. The latest event in the list will indicate CREATE_COMPLETE for the stack resource. Once the status of the stack is CREATE_COMPLETE , click on the Outputs tab. The values here will be critical to the completion of the remainder of the lab. Please take a moment to save these values somewhere that you will have easy access to them during the remainder of the lab. The names that appear in the Key column are referenced directly in the instructions in subsequent steps, using the parameter format: [outputKey]","title":"Prerequisites"},{"location":"modules/prerequisites/#prerequisites","text":"The following steps should be completed before getting started with any of the labs in this repository. Not all steps may apply to all students or environments. This lab contains the following tasks: Signing in to the AWS Management Console Creating a lab environment using AWS CloudFormation","title":"Prerequisites"},{"location":"modules/prerequisites/#1-signing-in-to-the-aws-management-console","text":"If you are running these labs in a formal, instructional setting, please use the Console URL, and credentials provided to you to access and log into the AWS Management Console. Otherwise, please use your own credentials. You can access the console at: https://console.aws.amazon.com/ or through the Single Sign-On (SSO) mechanism provided by your organization. If you are running these labs in a formal, instructional setting, please use the AWS region provided. Ensure the correct AWS region is selected in the top right corner, if not use that dropdown to choose the correct region. The labs are designed to work in any of the regions where Amazon Aurora PostgreSQL compatible is available. However, not all features and capabilities of Amazon Aurora may be available in all supported regions at this time.","title":"1. Signing in to the AWS Management Console"},{"location":"modules/prerequisites/#2-creating-a-lab-environment-using-aws-cloudformation","text":"To simplify the getting started experience with the labs, we have created foundational templates for AWS CloudFormation that provision the resources needed for the lab environment. These templates are designed to deploy a consistent networking infrastructure, and client-side experience of software packages and components used in the lab. Formal Event If you are running these labs in a formal instructional event, the lab environment may have been initialized on your behalf. If unsure, please verify with the event support staff. Please download the most appropriate CloudFormation template based on the labs you want to run: Option Download Template One-Click Launch Description I will create the DB cluster manually lab-no-cluster-pgsql.yml Use when you wish to provision the initial cluster manually by following Lab 1. - Creating a New Aurora Cluster Provision the DB cluster for me lab-with-cluster-pgsql.yml Use when you wish to skip the initial cluster creation lab, and have the DB cluster provisioned for you, so you can continue from Lab 2. - Cluster Endpoints and Read Replica Auto Scaling If you downloaded the template, save it in a memorable location such as your desktop, you will need to reference it later. Open the CloudFormation service console . Region Check Ensure you are still working in the correct region, especially if you are following the links above to open the service console at the right screen. Click Create Stack . Note The CloudFormation console has been upgraded recently. Depending on your previous usage of the CloudFormation console UI, you may see the old design or the new design, you may also be presented with a prompt to toggle between them. In this lab we are using the new design for reference, although the steps will work similarly in the old console design as well, if you are more familiar with it. Select the radio button named Upload a template , then Choose file and select the template file you downloaded previously named and then click Next . In the field named Stack Name , enter the value labstack . For the vpcAZs parameter select 3 availability zones (AZs) from the dropdown. If your desired region only supports 2 AZs, please select just the two AZs available. Click Next . On the Configure stack options page, leave the defaults as they are, scroll to the bottom and click Next . On the Review labstack page, scroll to the bottom, check the box that reads: I acknowledge that AWS CloudFormation might create IAM resources with custom names and then click Create . The stack will take approximatively 20 minutes to provision, you can monitor the status on the Stack detail page. You can monitor the progress of the stack creation process by refreshing the Events tab. The latest event in the list will indicate CREATE_COMPLETE for the stack resource. Once the status of the stack is CREATE_COMPLETE , click on the Outputs tab. The values here will be critical to the completion of the remainder of the lab. Please take a moment to save these values somewhere that you will have easy access to them during the remainder of the lab. The names that appear in the Key column are referenced directly in the instructions in subsequent steps, using the parameter format: [outputKey]","title":"2. Creating a lab environment using AWS CloudFormation"},{"location":"modules/qpm/","text":"Query Plan Management \u00b6 1. Prerequisite \u00b6 Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file 2. Creating Aurora PostgreSQL cluster with Cloudformation \u00b6 Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013 2.1 Retrieving Database credentials from Secret Manager \u00b6 Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database. 2.2 Cloudformation Resource chart \u00b6 Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d 2.3 Connecting to the EC2 bastion Instance \u00b6 We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here . 3. Quick start guide on using QPM with automatic capture \u00b6 Query plan management is available with Amazon Aurora PostgreSQL version 10.5-compatible (Aurora 2.1.0) and later, or Amazon Aurora PostgreSQL version 9.6.11-compatible (Aurora 1.4.0) and later. The quickest way to enable QPM is to use the automatic plan capture, which enables the plan capture for all SQL statements that run at least two times. With query plan management, you can control execution plans for a set of statements that you want to manage. You can do the following: Improve plan stability by forcing the optimizer to choose from a small number of known, good plans. Optimize plans centrally and then distribute the best plans globally. Identify indexes that aren't used and assess the impact of creating or dropping an index. Automatically detect a new minimum-cost plan discovered by the optimizer. Try new optimizer features with less risk, because you can choose to approve only the plan changes that improve performance. For additional details on the Query Plan Management please refer official documentation Managing Query Execution Plans for Aurora PostgreSQL . Here are the steps to configure and enable the use of QPM on your Aurora PostgreSQL cluster for automatic capture and using managed plans with QPM: 1. Modify the Amazon Aurora DB Cluster Parameters related to the CCM. \u00b6 Screenshots of some of the steps are shown below a. Open the Amazon RDS service console . b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format <stack-name-apgcustom-clusterparamgroup-nnnn> . In this case the stack name is labstack and hence the DB cluster parameter group file is labstack- apgcustom-clusterparamgroup-nnnn . The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group. d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of rds.enable_plan_management parameter to 1 and click on \u201cSave changes\u201d f. Open your database level parameter group and click on \u201cEdit Parameters\u201d g. Modify the value for apg_plan_mgmt.capture_plan_baselines parameter to \u201cautomatic\u201d and apg_plan_mgmt.use_plan_baselines to \u201cautomatic\u201d. Modify the value for apg_plan_mgmt.use_plan_baselines to \u201ctrue\u201d. For more information, see Modifying Parameters in a DB Parameter Group. Please note that these parameters can be set at either the cluster level or at the database level. The default recommendation would be to set it at the Aurora cluster level. h. Click on the \u201cPreview changes\u201d to verify the changes and click save changes. i. Wait for the status of the instance to change to available . Restart your DB instance to enable this new setting. j. Connect to your DB instance with a SQL client such as psql. For more information, see Using psql to Connect to a PostgreSQL DB Instance . The cluster endpoint for the Aurora PostgreSQL can be found from the key value from the output\u2019s sections under \u201cclusterEndpoint\u201d key. The username and the password need to be extracted from the secrets Manager as shown above ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> select aurora_version(),version(); aurora_version | version ----------------+----------------------------------------------------------------------------- 2.3.5 | PostgreSQL 10.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.9.3, 64-bit (1 row) 2. Create and verify the apg_plan_mgmt extension for your DB instance. \u00b6 a. Create the apg_plan_mgmt extension for your DB instance. mylab=> CREATE EXTENSION apg_plan_mgmt; CREATE EXTENSION mylab=> select extname,extversion from pg_extension where extname='apg_plan_mgmt'; extname | extversion ---------------+------------ apg_plan_mgmt | 1.0.1 b. Query to make sure that all QPM-related parameters are modified to the appropriate value. mylab=> show rds.enable_plan_management; rds.enable_plan_management ---------------------------- 1 mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- automatic mylab=> show apg_plan_mgmt.use_plan_baselines; apg_plan_mgmt.use_plan_baselines ---------------------------------- on 3. Run synthetic workload with automatic capture. \u00b6 The Cloud Formation template used for this workshop creates an EC2 bastion host bootstrapped with PostgreSQL tools (Pgbench, psql and sysbench etc.). The CF will also initialize the Database with pgbench (scale=100) data. a. From another EC2 Instance terminal use pgbench (a PostgreSQL benchmarking tool) to generate a simulated workload, which runs same queries for a specified period. With automatic capture enabled, QPM captures plans for each query that runs at least twice. Below is the example. export PATH =/ home / ec2 - user / postgresql - 10.7 / src / bin / pgbench : $ PATH cd / home / ec2 - user / postgresql - 10.7 / src / bin / pgbench . / pgbench -- progress - timestamp - M prepared - n - T 100 - P 1 - c 500 - j 500 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab b. Query apg_plan_mgmt.dba_plans table to view the managed statements and the execution plans for the SQL statements started with the pgbench tool. cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | enabled | sql_text -1677381765 -225188843 Approved t UPDATE pgbench_branches SET bbalance = bbalance + $1 WHERE bid = $2; -60114982 300482084 Approved t INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP); 1319555216 30042398 Approved t select count(*) from pgbench_branches; -2033469270 -1987991358 Approved t UPDATE pgbench_tellers SET tbalance = tbalance + $1 WHERE tid = $2; c. Turn off automatic capture. Capturing all plans with automatic capture has little runtime overhead and can be enabled in production. We are turning off the automatic capture to make sure that we don\u2019t capture SQL statements outside the pgbench workload. This can be turned off by setting the apg_plan_mgmt.capture_plan_baselines parameter to off from the DB instance-level parameter group. mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- Off d. Verify that the execution plan of the managed statement is the plan captured by QPM. We have manually executed the explain plan on one of the managed statements (highlighted in the yellow above). The explain plan output does show the SQL hash and the plan hash matches with the QPM approved plan for that statement. mylab=> explain (hashes true) UPDATE pgbench_tellers SET tbalance = tbalance + 100 WHERE tid = 200; QUERY PLAN ---------------------------------------------------------------------- Update on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) -> Index Scan using pgbench_tellers_pkey on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) Index Cond: (tid = 200) SQL Hash: -2033469270, Plan Hash: -1987991358 In addition to automatic plan capture, QPM also offers manual capture, which offers a mechanism to capture execution plans for known problematic queries. Capturing the plans automatically is recommended generally. However, there are situations where capturing plans manually would be the best option, such as: You don't want to enable plan management at the Database level, but you do want to control a few critical SQL statements only. You want to save the plan for a specific set of literals or parameter values that are causing a performance problem. 4. QPM Plan adaptability with plan evolution mechanism \u00b6 If the optimizer's generated plan is not a stored plan, the optimizer captures and stores it as a new unapproved plan to preserve stability for the QPM-managed SQL statements. Query plan management provides techniques and functions to add, maintain, and improve execution plans and thus provides Plan adaptability. Users can on demand or periodically instruct QPM to evolve all the stored plans to see if there is a better minimum cost plan available than any of the approved plans. QPM provides apg_plan_mgmt.evolve_plan_baselines function to compare plans based on their actual performance. Depending on the outcome of your performance experiments, you can change a plan's status from unapproved to either approved or rejected. You can instead decide to use the apg_plan_mgmt.evolve_plan_baselines function to temporarily disable a plan if it does not meet your requirements. For additional details about the QPM Plan evolution, see Evaluating Plan Performance. For the first use case, we walk through an example on how QPM helps ensure plan stability, these changes can result in plan regression. In most cases, you set up QPM to use automatic plan capture so that plans are captured for all statements that run two or more times. However, you can also capture plans for a specific set of statements that you specify manually. To do this, you set apg_plan_mgmt.capture_plan_baselines = off by default. At the session level, apg_plan_mgmt.capture_plan_baselines = manual at the session level. How to do it is described later. a. Enable manual plan capture to instruct QPM to capture the execution plan of the desired SQL statements manually. mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET b. Run an explain plan for the query so that QPM can capture the plan of the query (the following output for the explain plan is truncated for brevity). mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 c. Disable manual capture of the plan after you capture the execution plan for the desired SQL statement. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET d. View the captured query plan for the query that ran previously. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline isn't shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); e. To instruct the query optimizer to use the approved or preferred captured plans for your managed statements, set the parameter apg_plan_mgmt.use_plan_baselines to true. mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET f. View the explain plan output to see that the QPM approved plan is used by the query optimizer. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 g. Create a new index on the pgbench_history table column \u201cmtime\u201d to change the planner configuration and force the query optimizer to generate a new plan. mylab=> create index pgbench_hist_mtime on pgbench_history(mtime); CREATE INDEX h. View the explain plan output to see that QPM detects a new plan but still uses the approved plan and maintains the plan stability. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. ***Note: For this example, an approved plan was used instead of the minimum cost plan. SQL Hash: 1561242727, Plan Hash: -1990695905, Minimum Cost Plan Hash: -794604077*** i. Run the following SQL query to view the new plan and status of the plan. To ensure plan stability, QPM stores all the newly generated plans for a managed query in QPM as unapproved plans. The following output shows that there are two different execution plans stored for the same managed statement, as shown by the two different plan_hash values. Although the new execution plan has the minimum cost (lower than the approved plan), QPM continues to ignore the unapproved plans to maintain plan stability. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For the sake of brevity, the plan_outline is not shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+---------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 UnApproved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); The following is an example of plan adaptability with QPM. This example evaluates the unapproved plan based on the minimum speedup factor. It approves any captured unapproved plan that is faster by at least 10 percent than the best approved plan for the statement. For additional details, see Evaluating Plan Performance in the Aurora documentation . mylab=> SELECT apg_plan_mgmt.Evolve_plan_baselines (sql_hash, plan_hash, 1.1,'approve') FROM apg_plan_mgmt.dba_plans WHERE status = 'Unapproved'; NOTICE: [Unapproved] SQL Hash: 1561242727, Plan Hash: 1944377599, SELECT sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where ... NOTICE: Baseline [Planning time 0.693 ms, Execution time 316.644 ms] NOTICE: Baseline+1 [Planning time 0.695 ms, Execution time 213.919 ms] NOTICE: Total time benefit: 102.723 ms, Execution time benefit: 102.725 ms, Avg Log Cardinality Error: 3.53418, Cost = 111.16..111.17 NOTICE: Unapproved -> Approved After QPM evaluates the plan based on the speed factor, the plan status changes to approved. At this point, the optimizer can choose that plan for that managed statement now. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 Approved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); j. View the explain plan output to see whether the query is using the newly approved minimum cost plan. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ----------------------------------------------------------------------------- Aggregate (cost=111.16..111.17 rows=1 width=16) InitPlan 2 (returns $1) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 1 (returns $0) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan using pgbench_hist_mtime on pgbench_history mn (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) InitPlan 4 (returns $3) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 3 (returns $2) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan Backward using pgbench_hist_mtime on pgbench_history mx (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) -> Hash Join (cost=14.60..107.06 rows=632 width=8) Hash Cond: (h.bid = b.bid) -> Index Scan using pgbench_hist_mtime on pgbench_history h (cost=0.42..85.01 rows=2107 width=8) Index Cond: ((mtime >= $1) AND (mtime <= $3)) -> Hash (cost=14.14..14.14 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) SQL Hash: 1561242727, Plan Hash: -794604077 5. Fixing plans with QPM using pg_hint_plan \u00b6 In some cases, the query optimizer doesn\u2019t generate the best execution plan for the query. One approach to fixing this problem is to put query hints into your application code, but this approach is widely discouraged because it makes applications more brittle and harder to maintain, and in some cases, you can\u2019t hint the SQL because it is generated by a 3rd party application. What we will show is how to use hints to control the query optimizer, but then to remove the hints and allow QPM to enforce the desired plan, without adding hints to the application code. For this purpose, PostgreSQL users can use the pg_hint_plan extension to provide directives such as \u201cscan method,\u201d \u201cjoin method,\u201d \u201cjoin order,\u201d, or \u201crow number correction,\u201d to the optimizer. The resulting plan will be saved by QPM, along with any GUC parameters you choose to override (such as work_mem). QPM remembers any GUC parameter overrides and uses them when it needs to recreate the plan. To install and learn more about how to use the pg_hint_plan extension, see the pg_hint_plan documentation. QPM steps to fix the plan generated by using hints Working with pg_hint_plan is incredibly useful for cases where the query can\u2019t be modified to add hints. In this example, use a sample query to generate the execution plan that you want by adding hints to the managed statement. Then associate this execution plan with the original unmodified statement. Here are the detailed steps: a. Check if the plan capture is disabled mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- off (1 row) b. Run the query with the hint to use. In the following example, use the \u201cHashJoin\u201d hint, which is a directive for the optimizer to choose the join method as HashJoin. The original plan of the query without hints is as follows. mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Nested Loop (cost=0.42..181906.82 rows=1000000 width=465) Join Filter: (b.bid = a.bid) -> Index Scan using pgbench_accounts_pkey on pgbench_accounts a (cost=0.42..44165.43 rows=1000000 width=97) -> Materialize (cost=0.00..14.15 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: -451962956 c. Enable pg_hint_plan and manual plan capture: Mylab=> SET pg_hint_plan.enable_hint = true; SET mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET d. EXPLAIN the query with the hints you want to use. In the following example, use the HashJoin (a, b) hint, which is a directive for the optimizer to use a hash join algorithm to join from table a to table b: The plan that you want with a hash join is as follows. mylab=> /*+ HashJoin(a b) */ EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..250138.04 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: 1139293728 e. Verify that plan 1139293728 was captured, and note the status of the plan View the captured plan and the status of the plan. mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; Where plan_hash=1139293728; sql_hash | plan_hash | status | enabled | sql_text -----------+------------+----------+---------+--------------------------- 356104612 | 1139293728 | Approved | t | SELECT + | | | | * + | | | | FROM + | | | | pgbench_branches b + | | | | JOIN + | | | | pgbench_accounts a + | | | | ON b.bid = a.bid + | | | | ORDER BY + | | | | a.aid; f. If necessary, Approve the plan In this case this is the first and only plan saved for statement 356104612, so it was saved as an Approved plan. If this statement already had a baseline of approved plans, then this plan would have been saved as an Unapproved plan. In general, to Reject all existing plans for a statement and then Approve one specific plan, you could call apg_plan_mgmt.set_plan_status twice, like this: mylab=> SELECT apg_plan_mgmt.set_plan_status (sql_hash, plan_hash, 'Rejected') from apg_plan_mgmt.dba_plans where sql_hash = 356104612; SET mylab=> SELECT apg_plan_mgmt.set_plan_status (356104612, 1139293728, 'Approved'); SET g. Remove the hint, turn off manual capture, turn on use_plan_baselines, and also verify that the desired plan is in use without the hint. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..337638.11 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) Note: An Approved plan was used instead of the minimum cost plan. SQL Hash: 356104612, Plan Hash: 1139293728, Minimum Cost Plan Hash: -451962956 6. Deploying QPM-managed plans globally using export and import (no lab) \u00b6 Large enterprise customers often have applications and databases deployed globally. They also often maintain several environments (Dev, QA, Staging, UAT, Preprod, and Prod) for each application database. However, managing the execution plans manually in each of the databases in specific AWS Regions and each of the database environments can be cumbersome and time-consuming. QPM provides an option to export and import QPM-managed plans from one database to another database. With this option, you can manage the query execution centrally and deploy databases globally. This feature is useful for the scenarios where you investigate a set of plans on a preprod database, verify that they perform well, and then load them into a production database. Here are the steps to migrate QPM-managed plans from one database to another. For additional details, see Exporting and Importing Plans in the Aurora documentation. Export the QPM-managed plan from the source system. To do this, from the source database with the preferred execution plan, an authorized DB user can copy any subset of the apg_plan_mgmt.plans table to another table. That user can then save it using the pg_dump command. For additional details on the pg_dump, see pg_dump in the PostgreSQL documentation. Import the QPM-managed plan on the target system. On the target system, copy any subset of the apg_plan_mgmt.plans table to another table, and then save it using the pg_dump command. This is an optional step to preserve any existing managed plans on the target system before importing new plans for the source system. We have assumed that you have used pg_dump tar-format archive in step 1. Use the pg_restore command to copy the .tar file into a new table (plan_copy). For additional details about the pg_restore, see pg_restore in the PostgreSQL documentation. Merge the new table with the apg_plan_mgmt.plans table. Reload the managed plans into shared memory and remove the temporary plans table. SELECT apg_plan_mgmt.reload(); -- Refresh shared memory with new plans. DROP TABLE plans_copy; -- Drop the temporary plan table. 7. Disabling QPM and deleting plans manually \u00b6 Disable the QPM Open your cluster-level parameter group and set the rds.enable_plan_management parameter to 0 Delete all the plans captured SELECT SUM(apg_plan_mgmt.delete_plan(sql_hash, plan_hash)) FROM apg_plan_mgmt.dba_plans; 3.Helpful SQL statements SELECT sql_hash,plan_hash,status,enabled,sql_text FROM apg_plan_mgmt.dba_plans; SELECT sql_hash,plan_hash,status,estimated_total_cost \"cost\",sql_text FROM apg_plan_mgmt.dba_plans;","title":"Query Plan Management"},{"location":"modules/qpm/#query-plan-management","text":"","title":"Query Plan Management"},{"location":"modules/qpm/#1-prerequisite","text":"Create an EC2 Key Pair : You will need an EC2 key pair in order to log into the EC2 bastion instance To create a new key pair: Open the EC2 service console In the left-hand gutter under \u201cNetwork & Security\u201d click \u201cKey Pairs\u201d Supply a name and click \u201cCreate\u201d Download or otherwise save the .pem file","title":"1. Prerequisite"},{"location":"modules/qpm/#2-creating-aurora-postgresql-cluster-with-cloudformation","text":"Log in to your AWS console and go to the CloudFormation landing page Click create stack, select \u2018Specify an Amazon S3 template URL\u2019 and launch the CloudFormation stack from this template Download and save this locally and use upload a template to S3 option and click on \u201cChoose File\u201d option to point to the location where you have saved the template. Here is a screenshot of the first page \u2013","title":"2. Creating Aurora PostgreSQL cluster with Cloudformation"},{"location":"modules/qpm/#21-retrieving-database-credentials-from-secret-manager","text":"Search for the secret name as shown in the output of the stack and select the secret name. Click on the Retrieve secret value to get the Database user and the password to connect to the Aurora Database.","title":"2.1 Retrieving Database credentials from Secret Manager"},{"location":"modules/qpm/#22-cloudformation-resource-chart","text":"Please note that the Database names and the Custom Cluster and Database Parameter groups shown below are only for illustrative purpose. Participants are required to use the appropriate resources created from the Cloudformation template. Please refer the below table for the list of resources and the value Resource name Value Cluster Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomclusterparamgroup\u201d Database Parameter Group refer CloudFormation template output section and refer the key value \u201capgcustomdbparamgroup\u201d Cluster Endpoint refer CloudFormation template output section and refer the key value \u201cclusterEndpoint\u201d Reader Endpoint refer CloudFormation template output section and refer the key value \u201creaderEndpoint\u201d DB name mylab DB username masteruser DB password extract from the secrets Manager as shown above bastionEndpoint refer CloudFormation template output section and refer the key value \u201cbastionEndpoint\u201d","title":"2.2 Cloudformation Resource chart"},{"location":"modules/qpm/#23-connecting-to-the-ec2-bastion-instance","text":"We are creating EC2 instance (Amazon Linux AMI-ID ami-0f2176987ee50226e) and bootstrapping the EC2 Instance to have pgbench and sysbench benchmarking tools to be installed. ssh -i <keypair.pem> ec2-user@<bastionEndpoint> Replace the [ keypair.pem ] with the keypair file name input provided to the Cloud formation template. Replace the \u201cbastionEndpoint\u201d with the key value from the output section of the Cloud formation template. If you need to open an access for your laptop IP specifically, then whitelist your IP, by specifying your IP in the format x.x.x.x/32 ( Lookup your IP ). If there are any issues in accessing the instance, you can always modify the security group to populate your IP address as My IP as mentioned here .","title":"2.3 Connecting to the EC2 bastion Instance"},{"location":"modules/qpm/#3-quick-start-guide-on-using-qpm-with-automatic-capture","text":"Query plan management is available with Amazon Aurora PostgreSQL version 10.5-compatible (Aurora 2.1.0) and later, or Amazon Aurora PostgreSQL version 9.6.11-compatible (Aurora 1.4.0) and later. The quickest way to enable QPM is to use the automatic plan capture, which enables the plan capture for all SQL statements that run at least two times. With query plan management, you can control execution plans for a set of statements that you want to manage. You can do the following: Improve plan stability by forcing the optimizer to choose from a small number of known, good plans. Optimize plans centrally and then distribute the best plans globally. Identify indexes that aren't used and assess the impact of creating or dropping an index. Automatically detect a new minimum-cost plan discovered by the optimizer. Try new optimizer features with less risk, because you can choose to approve only the plan changes that improve performance. For additional details on the Query Plan Management please refer official documentation Managing Query Execution Plans for Aurora PostgreSQL . Here are the steps to configure and enable the use of QPM on your Aurora PostgreSQL cluster for automatic capture and using managed plans with QPM:","title":"3. Quick start guide on using QPM with automatic capture"},{"location":"modules/qpm/#1-modify-the-amazon-aurora-db-cluster-parameters-related-to-the-ccm","text":"Screenshots of some of the steps are shown below a. Open the Amazon RDS service console . b. In the navigation pane, choose Parameter groups. c. In the list, choose the parameter group for your Aurora PostgreSQL DB cluster. Please refer the name of the DB cluster parameter group file created from the output section of the CloudFormation Stack. It is in the format <stack-name-apgcustom-clusterparamgroup-nnnn> . In this case the stack name is labstack and hence the DB cluster parameter group file is labstack- apgcustom-clusterparamgroup-nnnn . The DB cluster must use a parameter group other than the default, because you can't change values in a default parameter group. For more information , see Creating a DB Cluster Parameter Group. d. Click on the DB cluster parameter group selected above and then click on \u201cEdit Parameters\u201d e. Set the value of rds.enable_plan_management parameter to 1 and click on \u201cSave changes\u201d f. Open your database level parameter group and click on \u201cEdit Parameters\u201d g. Modify the value for apg_plan_mgmt.capture_plan_baselines parameter to \u201cautomatic\u201d and apg_plan_mgmt.use_plan_baselines to \u201cautomatic\u201d. Modify the value for apg_plan_mgmt.use_plan_baselines to \u201ctrue\u201d. For more information, see Modifying Parameters in a DB Parameter Group. Please note that these parameters can be set at either the cluster level or at the database level. The default recommendation would be to set it at the Aurora cluster level. h. Click on the \u201cPreview changes\u201d to verify the changes and click save changes. i. Wait for the status of the instance to change to available . Restart your DB instance to enable this new setting. j. Connect to your DB instance with a SQL client such as psql. For more information, see Using psql to Connect to a PostgreSQL DB Instance . The cluster endpoint for the Aurora PostgreSQL can be found from the key value from the output\u2019s sections under \u201cclusterEndpoint\u201d key. The username and the password need to be extracted from the secrets Manager as shown above ./psql -h <cluster-endpoint> -p <port>- U <username> -d <dbname> cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> select aurora_version(),version(); aurora_version | version ----------------+----------------------------------------------------------------------------- 2.3.5 | PostgreSQL 10.7 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.9.3, 64-bit (1 row)","title":"1. Modify the Amazon Aurora DB Cluster Parameters related to the CCM."},{"location":"modules/qpm/#2-create-and-verify-the-apg_plan_mgmt-extension-for-your-db-instance","text":"a. Create the apg_plan_mgmt extension for your DB instance. mylab=> CREATE EXTENSION apg_plan_mgmt; CREATE EXTENSION mylab=> select extname,extversion from pg_extension where extname='apg_plan_mgmt'; extname | extversion ---------------+------------ apg_plan_mgmt | 1.0.1 b. Query to make sure that all QPM-related parameters are modified to the appropriate value. mylab=> show rds.enable_plan_management; rds.enable_plan_management ---------------------------- 1 mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- automatic mylab=> show apg_plan_mgmt.use_plan_baselines; apg_plan_mgmt.use_plan_baselines ---------------------------------- on","title":"2. Create and verify the apg_plan_mgmt extension for your DB instance."},{"location":"modules/qpm/#3-run-synthetic-workload-with-automatic-capture","text":"The Cloud Formation template used for this workshop creates an EC2 bastion host bootstrapped with PostgreSQL tools (Pgbench, psql and sysbench etc.). The CF will also initialize the Database with pgbench (scale=100) data. a. From another EC2 Instance terminal use pgbench (a PostgreSQL benchmarking tool) to generate a simulated workload, which runs same queries for a specified period. With automatic capture enabled, QPM captures plans for each query that runs at least twice. Below is the example. export PATH =/ home / ec2 - user / postgresql - 10.7 / src / bin / pgbench : $ PATH cd / home / ec2 - user / postgresql - 10.7 / src / bin / pgbench . / pgbench -- progress - timestamp - M prepared - n - T 100 - P 1 - c 500 - j 500 -- host = labstack - cluster . cluster - xxxxxxxxx . us - west - 2. rds . amazonaws . com - b tpcb - like @1 - b select - only @20 -- username = masteruser mylab b. Query apg_plan_mgmt.dba_plans table to view the managed statements and the execution plans for the SQL statements started with the pgbench tool. cd /home/ec2-user/postgresql-10.7/src/bin/psql export PATH=/home/ec2-user/postgresql-10.7/src/bin/:$PATH ./psql -h labstack-cluster.cluster-xxxxxxxxx.us-west-2.rds.amazonaws.com -p 5432 -U masteruser -d mylab mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | enabled | sql_text -1677381765 -225188843 Approved t UPDATE pgbench_branches SET bbalance = bbalance + $1 WHERE bid = $2; -60114982 300482084 Approved t INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP); 1319555216 30042398 Approved t select count(*) from pgbench_branches; -2033469270 -1987991358 Approved t UPDATE pgbench_tellers SET tbalance = tbalance + $1 WHERE tid = $2; c. Turn off automatic capture. Capturing all plans with automatic capture has little runtime overhead and can be enabled in production. We are turning off the automatic capture to make sure that we don\u2019t capture SQL statements outside the pgbench workload. This can be turned off by setting the apg_plan_mgmt.capture_plan_baselines parameter to off from the DB instance-level parameter group. mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- Off d. Verify that the execution plan of the managed statement is the plan captured by QPM. We have manually executed the explain plan on one of the managed statements (highlighted in the yellow above). The explain plan output does show the SQL hash and the plan hash matches with the QPM approved plan for that statement. mylab=> explain (hashes true) UPDATE pgbench_tellers SET tbalance = tbalance + 100 WHERE tid = 200; QUERY PLAN ---------------------------------------------------------------------- Update on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) -> Index Scan using pgbench_tellers_pkey on pgbench_tellers (cost=0.14..8.16 rows=1 width=358) Index Cond: (tid = 200) SQL Hash: -2033469270, Plan Hash: -1987991358 In addition to automatic plan capture, QPM also offers manual capture, which offers a mechanism to capture execution plans for known problematic queries. Capturing the plans automatically is recommended generally. However, there are situations where capturing plans manually would be the best option, such as: You don't want to enable plan management at the Database level, but you do want to control a few critical SQL statements only. You want to save the plan for a specific set of literals or parameter values that are causing a performance problem.","title":"3. Run synthetic workload with automatic capture."},{"location":"modules/qpm/#4-qpm-plan-adaptability-with-plan-evolution-mechanism","text":"If the optimizer's generated plan is not a stored plan, the optimizer captures and stores it as a new unapproved plan to preserve stability for the QPM-managed SQL statements. Query plan management provides techniques and functions to add, maintain, and improve execution plans and thus provides Plan adaptability. Users can on demand or periodically instruct QPM to evolve all the stored plans to see if there is a better minimum cost plan available than any of the approved plans. QPM provides apg_plan_mgmt.evolve_plan_baselines function to compare plans based on their actual performance. Depending on the outcome of your performance experiments, you can change a plan's status from unapproved to either approved or rejected. You can instead decide to use the apg_plan_mgmt.evolve_plan_baselines function to temporarily disable a plan if it does not meet your requirements. For additional details about the QPM Plan evolution, see Evaluating Plan Performance. For the first use case, we walk through an example on how QPM helps ensure plan stability, these changes can result in plan regression. In most cases, you set up QPM to use automatic plan capture so that plans are captured for all statements that run two or more times. However, you can also capture plans for a specific set of statements that you specify manually. To do this, you set apg_plan_mgmt.capture_plan_baselines = off by default. At the session level, apg_plan_mgmt.capture_plan_baselines = manual at the session level. How to do it is described later. a. Enable manual plan capture to instruct QPM to capture the execution plan of the desired SQL statements manually. mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET b. Run an explain plan for the query so that QPM can capture the plan of the query (the following output for the explain plan is truncated for brevity). mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 c. Disable manual capture of the plan after you capture the execution plan for the desired SQL statement. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET d. View the captured query plan for the query that ran previously. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline isn't shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); e. To instruct the query optimizer to use the approved or preferred captured plans for your managed statements, set the parameter apg_plan_mgmt.use_plan_baselines to true. mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET f. View the explain plan output to see that the QPM approved plan is used by the query optimizer. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ---------------------------------------------------------------------- Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. SQL Hash: 1561242727, Plan Hash: -1990695905 g. Create a new index on the pgbench_history table column \u201cmtime\u201d to change the planner configuration and force the query optimizer to generate a new plan. mylab=> create index pgbench_hist_mtime on pgbench_history(mtime); CREATE INDEX h. View the explain plan output to see that QPM detects a new plan but still uses the approved plan and maintains the plan stability. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN Aggregate (cost=23228.13..23228.14 rows=1 width=16) InitPlan 1 (returns $1) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mn (cost=0.00..5346.11 rows=247911 width=8) InitPlan 2 (returns $3) -> Finalize Aggregate (cost=6966.00..6966.01 rows=1 width=8) -> Gather (cost=6965.89..6966.00 rows=1 width=8) Workers Planned: 1 -> Partial Aggregate (cost=5965.89..5965.90 rows=1 width=8) -> Parallel Seq Scan on pgbench_history mx (cost=0.00..5346.11 rows=247911 width=8) -> Nested Loop (cost=0.00..9292.95 rows=632 width=8) Join Filter: (h.bid = b.bid) -> Seq Scan on pgbench_history h (cost=0.00..9188.74 rows=2107 width=8) Filter: ((mtime >= $1) AND (mtime <= $3)) -> Materialize (cost=0.00..14.15 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. ***Note: For this example, an approved plan was used instead of the minimum cost plan. SQL Hash: 1561242727, Plan Hash: -1990695905, Minimum Cost Plan Hash: -794604077*** i. Run the following SQL query to view the new plan and status of the plan. To ensure plan stability, QPM stores all the newly generated plans for a managed query in QPM as unapproved plans. The following output shows that there are two different execution plans stored for the same managed statement, as shown by the two different plan_hash values. Although the new execution plan has the minimum cost (lower than the approved plan), QPM continues to ignore the unapproved plans to maintain plan stability. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For the sake of brevity, the plan_outline is not shown here. Instead, plan_hash_value from the explain plan preceding is compared with plan_hash from the output of the apg_plan_mgmt.dba_plans query. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+---------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 UnApproved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); The following is an example of plan adaptability with QPM. This example evaluates the unapproved plan based on the minimum speedup factor. It approves any captured unapproved plan that is faster by at least 10 percent than the best approved plan for the statement. For additional details, see Evaluating Plan Performance in the Aurora documentation . mylab=> SELECT apg_plan_mgmt.Evolve_plan_baselines (sql_hash, plan_hash, 1.1,'approve') FROM apg_plan_mgmt.dba_plans WHERE status = 'Unapproved'; NOTICE: [Unapproved] SQL Hash: 1561242727, Plan Hash: 1944377599, SELECT sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where ... NOTICE: Baseline [Planning time 0.693 ms, Execution time 316.644 ms] NOTICE: Baseline+1 [Planning time 0.695 ms, Execution time 213.919 ms] NOTICE: Total time benefit: 102.723 ms, Execution time benefit: 102.725 ms, Avg Log Cardinality Error: 3.53418, Cost = 111.16..111.17 NOTICE: Unapproved -> Approved After QPM evaluates the plan based on the speed factor, the plan status changes to approved. At this point, the optimizer can choose that plan for that managed statement now. mylab=> SELECT sql_hash, plan_hash, status, estimated_total_cost \"cost\", sql_text FROM apg_plan_mgmt.dba_plans; sql_hash | plan_hash | status | cost | sql_text ------------+-------------+----------+---------+----------------------------------------------------------- 1561242727 -1990695905 Approved 23228.14 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); 1561242727 -794604077 Approved 111.17 select sum(delta),sum(bbalance) from pgbench_history h, pgbench_branches b where b.bid=h.bid and b.bid in (1,2,3) and mtime between (select min(mtime) from pgbench_history mn) and (select max(mtime) from pgbench_history mx); j. View the explain plan output to see whether the query is using the newly approved minimum cost plan. mylab=> explain (hashes true) SELECT Sum(delta), Sum(bbalance) FROM pgbench_history h, pgbench_branches b WHERE b.bid = h.bid AND b.bid IN ( 1, 2, 3 ) AND mtime BETWEEN (SELECT Min(mtime) FROM pgbench_history mn) AND (SELECT Max(mtime) FROM pgbench_history mx); QUERY PLAN ----------------------------------------------------------------------------- Aggregate (cost=111.16..111.17 rows=1 width=16) InitPlan 2 (returns $1) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 1 (returns $0) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan using pgbench_hist_mtime on pgbench_history mn (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) InitPlan 4 (returns $3) -> Result (cost=0.46..0.47 rows=1 width=8) InitPlan 3 (returns $2) -> Limit (cost=0.42..0.46 rows=1 width=8) -> Index Only Scan Backward using pgbench_hist_mtime on pgbench_history mx (cost=0.42..14882.41 rows=421449 width=8) Index Cond: (mtime IS NOT NULL) -> Hash Join (cost=14.60..107.06 rows=632 width=8) Hash Cond: (h.bid = b.bid) -> Index Scan using pgbench_hist_mtime on pgbench_history h (cost=0.42..85.01 rows=2107 width=8) Index Cond: ((mtime >= $1) AND (mtime <= $3)) -> Hash (cost=14.14..14.14 rows=3 width=8) -> Seq Scan on pgbench_branches b (cost=0.00..14.14 rows=3 width=8) Filter: (bid = ANY ('{1,2,3}'::integer[])) SQL Hash: 1561242727, Plan Hash: -794604077","title":"4. QPM Plan adaptability with plan evolution mechanism"},{"location":"modules/qpm/#5-fixing-plans-with-qpm-using-pg_hint_plan","text":"In some cases, the query optimizer doesn\u2019t generate the best execution plan for the query. One approach to fixing this problem is to put query hints into your application code, but this approach is widely discouraged because it makes applications more brittle and harder to maintain, and in some cases, you can\u2019t hint the SQL because it is generated by a 3rd party application. What we will show is how to use hints to control the query optimizer, but then to remove the hints and allow QPM to enforce the desired plan, without adding hints to the application code. For this purpose, PostgreSQL users can use the pg_hint_plan extension to provide directives such as \u201cscan method,\u201d \u201cjoin method,\u201d \u201cjoin order,\u201d, or \u201crow number correction,\u201d to the optimizer. The resulting plan will be saved by QPM, along with any GUC parameters you choose to override (such as work_mem). QPM remembers any GUC parameter overrides and uses them when it needs to recreate the plan. To install and learn more about how to use the pg_hint_plan extension, see the pg_hint_plan documentation. QPM steps to fix the plan generated by using hints Working with pg_hint_plan is incredibly useful for cases where the query can\u2019t be modified to add hints. In this example, use a sample query to generate the execution plan that you want by adding hints to the managed statement. Then associate this execution plan with the original unmodified statement. Here are the detailed steps: a. Check if the plan capture is disabled mylab=> show apg_plan_mgmt.capture_plan_baselines; apg_plan_mgmt.capture_plan_baselines -------------------------------------- off (1 row) b. Run the query with the hint to use. In the following example, use the \u201cHashJoin\u201d hint, which is a directive for the optimizer to choose the join method as HashJoin. The original plan of the query without hints is as follows. mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Nested Loop (cost=0.42..181906.82 rows=1000000 width=465) Join Filter: (b.bid = a.bid) -> Index Scan using pgbench_accounts_pkey on pgbench_accounts a (cost=0.42..44165.43 rows=1000000 width=97) -> Materialize (cost=0.00..14.15 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: -451962956 c. Enable pg_hint_plan and manual plan capture: Mylab=> SET pg_hint_plan.enable_hint = true; SET mylab=> SET apg_plan_mgmt.capture_plan_baselines = manual; SET d. EXPLAIN the query with the hints you want to use. In the following example, use the HashJoin (a, b) hint, which is a directive for the optimizer to use a hash join algorithm to join from table a to table b: The plan that you want with a hash join is as follows. mylab=> /*+ HashJoin(a b) */ EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..250138.04 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) SQL Hash: 356104612, Plan Hash: 1139293728 e. Verify that plan 1139293728 was captured, and note the status of the plan View the captured plan and the status of the plan. mylab=> SELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; Where plan_hash=1139293728; sql_hash | plan_hash | status | enabled | sql_text -----------+------------+----------+---------+--------------------------- 356104612 | 1139293728 | Approved | t | SELECT + | | | | * + | | | | FROM + | | | | pgbench_branches b + | | | | JOIN + | | | | pgbench_accounts a + | | | | ON b.bid = a.bid + | | | | ORDER BY + | | | | a.aid; f. If necessary, Approve the plan In this case this is the first and only plan saved for statement 356104612, so it was saved as an Approved plan. If this statement already had a baseline of approved plans, then this plan would have been saved as an Unapproved plan. In general, to Reject all existing plans for a statement and then Approve one specific plan, you could call apg_plan_mgmt.set_plan_status twice, like this: mylab=> SELECT apg_plan_mgmt.set_plan_status (sql_hash, plan_hash, 'Rejected') from apg_plan_mgmt.dba_plans where sql_hash = 356104612; SET mylab=> SELECT apg_plan_mgmt.set_plan_status (356104612, 1139293728, 'Approved'); SET g. Remove the hint, turn off manual capture, turn on use_plan_baselines, and also verify that the desired plan is in use without the hint. mylab=> SET apg_plan_mgmt.capture_plan_baselines = off; SET mylab=> SET apg_plan_mgmt.use_plan_baselines = true; SET mylab=> EXPLAIN (hashes true) SELECT * FROM pgbench_branches b JOIN pgbench_accounts a ON b.bid = a.bid ORDER BY a.aid; QUERY PLAN ---------------------------------------------------------------------- Gather Merge (cost=240409.02..337638.11 rows=833334 width=465) Workers Planned: 2 -> Sort (cost=239409.00..240450.67 rows=416667 width=465) Sort Key: a.aid -> Hash Join (cost=14.22..23920.19 rows=416667 width=465) Hash Cond: (a.bid = b.bid) -> Parallel Seq Scan on pgbench_accounts a (cost=0.00..22348.67 rows=416667 width=97) -> Hash (cost=14.10..14.10 rows=10 width=364) -> Seq Scan on pgbench_branches b (cost=0.00..14.10 rows=10 width=364) Note: An Approved plan was used instead of the minimum cost plan. SQL Hash: 356104612, Plan Hash: 1139293728, Minimum Cost Plan Hash: -451962956","title":"5. Fixing plans with QPM using pg_hint_plan"},{"location":"modules/qpm/#6-deploying-qpm-managed-plans-globally-using-export-and-import-no-lab","text":"Large enterprise customers often have applications and databases deployed globally. They also often maintain several environments (Dev, QA, Staging, UAT, Preprod, and Prod) for each application database. However, managing the execution plans manually in each of the databases in specific AWS Regions and each of the database environments can be cumbersome and time-consuming. QPM provides an option to export and import QPM-managed plans from one database to another database. With this option, you can manage the query execution centrally and deploy databases globally. This feature is useful for the scenarios where you investigate a set of plans on a preprod database, verify that they perform well, and then load them into a production database. Here are the steps to migrate QPM-managed plans from one database to another. For additional details, see Exporting and Importing Plans in the Aurora documentation. Export the QPM-managed plan from the source system. To do this, from the source database with the preferred execution plan, an authorized DB user can copy any subset of the apg_plan_mgmt.plans table to another table. That user can then save it using the pg_dump command. For additional details on the pg_dump, see pg_dump in the PostgreSQL documentation. Import the QPM-managed plan on the target system. On the target system, copy any subset of the apg_plan_mgmt.plans table to another table, and then save it using the pg_dump command. This is an optional step to preserve any existing managed plans on the target system before importing new plans for the source system. We have assumed that you have used pg_dump tar-format archive in step 1. Use the pg_restore command to copy the .tar file into a new table (plan_copy). For additional details about the pg_restore, see pg_restore in the PostgreSQL documentation. Merge the new table with the apg_plan_mgmt.plans table. Reload the managed plans into shared memory and remove the temporary plans table. SELECT apg_plan_mgmt.reload(); -- Refresh shared memory with new plans. DROP TABLE plans_copy; -- Drop the temporary plan table.","title":"6. Deploying QPM-managed plans globally using export and import (no lab)"},{"location":"modules/qpm/#7-disabling-qpm-and-deleting-plans-manually","text":"Disable the QPM Open your cluster-level parameter group and set the rds.enable_plan_management parameter to 0 Delete all the plans captured SELECT SUM(apg_plan_mgmt.delete_plan(sql_hash, plan_hash)) FROM apg_plan_mgmt.dba_plans; 3.Helpful SQL statements SELECT sql_hash,plan_hash,status,enabled,sql_text FROM apg_plan_mgmt.dba_plans; SELECT sql_hash,plan_hash,status,estimated_total_cost \"cost\",sql_text FROM apg_plan_mgmt.dba_plans;","title":"7. Disabling QPM and deleting plans manually"}]}